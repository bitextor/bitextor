from os.path import join

#Output dirs
permanentDir = config["permanentDir"]
modelDir = permanentDir + "/model"
#print("permanentDir", permanentDir)

marian=config["marianDir"]
moses=config["mosesDir"]
subword_nmt=config["subwordNmtDir"]
vocabSize=config["nmtVocabSize"]
detokenizer=config["LANG2Detokenizer"]

LANG1=config["lang1"]
LANG2=config["lang2"]

#NMT commands
trainCmd = "{0}/build/marian -d {1} ".format(marian, config["gpuId"]) + " ".join(config["marianArgs"])

translateCmd = "{0}/build/marian-decoder --max-length-crop -d {1} --quiet-translation ".format(marian, config["gpuId"])

#Tokenization
tokenizer_l1= config["LANG1Tokenizer"]
tokenizer_l2= config["LANG2Tokenizer"]

#Input data prefixes
trainPrefixes=config["initCorpusTrainPrefix"]
devPrefix=config["initCorpusDevPrefix"]
testPrefixes=config["initCorpusTestPrefix"]
assert(len(devPrefix) == 1)

############################################# EVALUATION #############################################################

def allTestNames(dataset):
    names = []
    for f in dataset:
        names.append(os.path.basename(f))
    return names

rule report:
    input:
        expand("evaluation/{name}.bleu", name=allTestNames(testPrefixes))
    output:
        "evaluation/report"
    run:
        with open(output[0], "wt") as outHandle:
            for file in input:
                with open(file, "rt") as inHandle:
                    str = inHandle.read()
                    outHandle.write(os.path.basename(file).replace(".bleu",""))
                    outHandle.write("\t")
                    outHandle.write(str)

rule multibleu:
    input:
        trans="evaluation/{name}.output.detokenized"
        ,
        ref="processed_corpus/test/{name}."+"{lang}".format(lang=LANG2)
    output:
        "evaluation/{name}.bleu"
    shell:
        "cat {input.trans} | sacrebleu {input.ref} > {output}"

rule translate_only:
    input:
        trans="evaluation/in.output.detokenized"

    output:
        "evaluation/output"
    shell:
      	#"hostname;"
        "cp {input} {output}"

##########################################  RUNNING MT ENGINE ##########################################


rule translate_test:
    input:
        modelPrevious="{dir}/marian/model.npz.decoder.yml".format(dir=modelDir)
        ,
        test="processed_corpus/test/{name}.bpe." + "{lang}".format(lang=LANG1)
    output:
        "evaluation/{name}.output"
    params:
        model="{dir}/marian/{modelFile}".format(dir=modelDir, modelFile=config["marianModelFile"])

    shell:
        "cat {input.test} | {translateCmd} -c {params.model} > {output}"

rule train_nmt:
    input:
        vocab="{dir}/vocab.yml".format(dir=modelDir)
        ,
        train=["processed_corpus/train.clean-bpe.{lang}".format(lang=LANG1),
               "processed_corpus/train.clean-bpe.{lang}".format(lang=LANG2)]
        ,
        valid=["processed_corpus/dev.bpe.{lang}".format(lang=LANG1),
               "processed_corpus/dev.bpe.{lang}".format(lang=LANG2)]

    output:
        "{dir}/marian/model.npz.decoder.yml".format(dir=modelDir)
    shell:
        "{trainCmd} -t {input.train} --valid-sets {input.valid} --vocabs {input.vocab} {input.vocab} -m {modelDir}/marian/model.npz"

################################################## MARIAN VOCAB ################################################################

rule make_vocab_yml:
    input:
        "processed_corpus/train.clean-bpe."+"{lang}".format(lang=LANG1)
        ,
        "processed_corpus/train.clean-bpe."+"{lang}".format(lang=LANG2)
    output:
        '{pref}'.format(pref=modelDir)+'/vocab.yml'
    shell:
        "cat {input} | {marian}/build/marian-vocab --max-size {vocabSize} > {output}"

####################################################### TRUECASE ###########################################################

rule apply_truecaser:
    input:
        file='{name}.tok.{lang}'
        ,
        model="{dir}/truecaser/".format(dir=modelDir)+"truecase-model.{lang}"
    output:
        '{name}.tc.{lang}'
    shell:
        "cat {input.file} | {moses}/scripts/recaser/truecase.perl -model {input.model} > {output}"

rule apply_truecaser_train:
    input:
        file='{name}.clean.{lang}'
        ,
        model="{dir}/truecaser/".format(dir=modelDir)+"truecase-model.{lang}"
    output:
        '{name}.clean-tc.{lang}'
    shell:
        "cat {input.file} | {moses}/scripts/recaser/truecase.perl -model {input.model} > {output}"

rule learn_truecaser:
    input:
        "processed_corpus/train.clean.{lang}"
    output:
        "{dir}/truecaser/".format(dir=modelDir)+"truecase-model.{lang}"
    shell:
        "mkdir -p {modelDir}/truecaser;"
        "{moses}/scripts/recaser/train-truecaser.perl -corpus {input} -model {output}"

####################################################### CLEAN ###########################################################

rule clean:
    input:
        "{pref}.tok."+"{lang1}".format(lang1=LANG1)
        ,
        "{pref}.tok."+"{lang2}".format(lang2=LANG2)
    output:
        "{pref}.clean."+"{lang1}".format(lang1=LANG1)
        ,
        "{pref}.clean."+"{lang2}".format(lang2=LANG2)
    shell:
        "{moses}/scripts/training/clean-corpus-n.perl {wildcards.pref}.tok {LANG1} {LANG2} {wildcards.pref}.clean 1 80 {wildcards.pref}.lines-retained"

####################################################### TOKENIZE ###########################################################

rule tokenize_file_l1:
    input: 
        "{pref}."+"{lang}".format(lang=LANG1)
    output:
        "{pref}.tok."+"{lang}".format(lang=LANG1)
    shell:
        "cat {input} | {tokenizer_l1} > {output}"

rule tokenize_file_l2:
    input:
        "{pref}."+"{lang}".format(lang=LANG2)
    output:
        "{pref}.tok."+"{lang}".format(lang=LANG2)
    shell:
        "cat {input} | {tokenizer_l2} > {output}"


####################################################### POSTPROCESSING ###########################################################

rule detok:
    input:
        "{pref}.output.detruecased"
    output:
        "{pref}.output.detokenized"
    shell:
        "cat {input} | {detokenizer} > {output}"

rule detruecase:
    input:
        "{pref}.output.debpe"
    output:
        "{pref}.output.detruecased"
    shell:
        "cat {input} | {moses}/scripts/recaser/detruecase.perl > {output}"

rule debpe:
    input:
        "{pref}.output"
    output:
        "{pref}.output.debpe"
    shell:
        "cat {input} | sed -r 's/(@@ )|(@@ ?$)//g' > {output}"

############################################## BPE ##############################################

rule apply_bpe:
    input:
        file="{pref}.tc.{lang}"
        ,
        vocab="{dir}/".format(dir=modelDir)+"vocab.{lang1}{lang2}".format(lang1=LANG1, lang2=LANG2)
    output:
        "{pref}.bpe.{lang}"
    shell:
        "{subword_nmt}/subword_nmt/apply_bpe.py -c {input.vocab} < {input.file} > {output}"

rule apply_bpe_train:
    input:
        file="{pref}.clean-tc.{lang}"
        ,
        vocab="{dir}/".format(dir=modelDir)+"vocab.{lang1}{lang2}".format(lang1=LANG1, lang2=LANG2)
    output:
        "{pref}.clean-bpe.{lang}"
    shell:
        "{subword_nmt}/subword_nmt/apply_bpe.py -c {input.vocab} < {input.file} > {output}"

rule learn_bpe:
    input:
        "processed_corpus/train.clean."+"{lang}".format(lang=LANG1)
        ,
        "processed_corpus/train.clean."+"{lang}".format(lang=LANG2)
    output:
        '{dir}'.format(dir=modelDir)+'/vocab.'+'{lang1}{lang2}'.format(lang1=LANG1, lang2=LANG2)
    shell:
        "cat {input} | {subword_nmt}/subword_nmt/learn_bpe.py -s {vocabSize}  > {output}"

###################################### PREPARE DATA ##############################################

rule prepare_traindata:
    input:
         l1=expand("{dataset}.{lang}", dataset=trainPrefixes, lang=LANG1)
         ,
         l2=expand("{dataset}.{lang}", dataset=trainPrefixes, lang=LANG2)
    output:
         l1="processed_corpus/train.{lang}".format(lang=LANG1)
         ,
         l2="processed_corpus/train.{lang}".format(lang=LANG2)
    shell:
         "mkdir -p processed_corpus; cat {input.l1} > {output.l1} && cat {input.l2} > {output.l2}"

rule prepare_devdata:
    input: 
         l1=expand("{dataset}.{lang}", dataset=devPrefix, lang=LANG1)
         ,
         l2=expand("{dataset}.{lang}", dataset=devPrefix, lang=LANG2)
    output: 
         l1="processed_corpus/dev.{lang}".format(lang=LANG1)
         ,
         l2="processed_corpus/dev.{lang}".format(lang=LANG2)
    shell:
         "mkdir -p processed_corpus; cat {input.l1} > {output.l1} && cat {input.l2} > {output.l2}"

rule prepare_test:
    input:
         l1=expand("{dataset}.{lang}", dataset=testPrefixes, lang=LANG1)
         ,
         l2=expand("{dataset}.{lang}", dataset=testPrefixes, lang=LANG2)
    output:
         expand("processed_corpus/test/{name}.{lang}", name=allTestNames(testPrefixes), lang=LANG1)
         ,
         expand("processed_corpus/test/{name}.{lang}", name=allTestNames(testPrefixes), lang=LANG2)
    shell:
         "mkdir -p processed_corpus/test; cp -r {input.l1} {input.l2} processed_corpus/test"

rule prepare_translate_only:
    input:
         l1="in"
    output:
         "processed_corpus/test/in.{lang}".format(lang=LANG1)
    shell:
         "cp {input} {output}"

rule decompressGZ:
    input:
         "{pref}.{lang}.gz"
    output:
         temp("{pref}.{lang}")
    wildcard_constraints:
         lang="({l1}|{l2})".format(l1=LANG1, l2=LANG2)
    shell:
         "zcat {input} > {output}"

rule decompressXZ:
    input:
         "{pref}.{lang}.xz"
    output:
         temp("{pref}.{lang}")
    wildcard_constraints:
         lang="({l1}|{l2})".format(l1=LANG1, l2=LANG2)
    shell:
         "xzcat -T 0 {input} > {output}"
