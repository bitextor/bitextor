import os
import sys
import subprocess
import pprint

from bitextor.utils.args import validate_args
from bitextor.utils.common import (
    open_xz_or_gzip_or_plain,
    duration_to_seconds,
    return_dict_value_if_key,
    path_exists,
    initialize_persistent_dict,
    set_up_logging_logger,
    flush_logger,
)
import bitextor.utils.check_nltk_model as check_nltk_model
from bitextor.utils.allocate_cuda_gpus import (
    allocate_cuda_visible_device,
    allocate_cuda_devices_teardown,
)

from pytools.persistent_dict import PersistentDict

valid, config = validate_args(config)

if not valid:
    raise Exception("provided configuration is not valid")

#################################################################
include: "rules/common.smk"


#################################################################
# BASIC PARAMETERS

WORKFLOW = workflow.basedir
DATADIR = config["dataDir"]
TRANSIENT = config["transientDir"]
PERMANENT = config["permanentDir"]
TMPDIR = config["tempDir"]

# Create directories
for f in (DATADIR, TRANSIENT, PERMANENT, TMPDIR):
    try:
        os.makedirs(f)
    except FileExistsError:
        pass

LANGS = set()

# the only scenario where config won't have LANG1 and LANG2 is when the workflow is monolingual,
# and in that case LANG1/LANG2 and SRC_LANG/TRG_LANG won't be used anywhere, so setting them to 'l1' and 'l2' is fine
# (if LANG1/LANG2 and SRC_LANG/TRG_LANG are unset or both empty string, Snakemake will give an error, even if they are not used)
LANG1 = return_dict_value_if_key(config, "lang1", "l1", only_check_key=True)
LANG2 = return_dict_value_if_key(config, "lang2", "l2", only_check_key=True)

if "langs" in config:
    LANGS = set(config["langs"])
if "lang1" in config:
    LANGS.add(LANG1)
if "lang2" in config:
    LANGS.add(LANG2)

PROFILING = return_dict_value_if_key(config, "profiling", "", pos_value="\\time -v")
VERBOSE = config["verbose"]
LOGGER = set_up_logging_logger(logging.getLogger("bitextor"), level=logging.DEBUG if VERBOSE else logging.INFO)

#################################################################
# CRAWLING
CRAWLTARGET = return_dict_value_if_key(config, "crawler", "")
TLD_CRAWL = return_dict_value_if_key(config, "crawlTLD", [])
CRAWLSIZELIMIT = return_dict_value_if_key(config, "crawlSizeLimit", "", apply_function=str)
CRAWLTIMELIMIT = return_dict_value_if_key(config, "crawlTimeLimit", "", apply_function=str)
CRAWLWAIT = return_dict_value_if_key(config, "crawlWait", "", apply_function=str)
CRAWLFILETYPES = return_dict_value_if_key(config, "crawlFileTypes", [])
CRAWLJOBS = return_dict_value_if_key(config, "crawlerNumThreads", 2, only_check_key=True, apply_function=str)
CRAWLTIMEOUT = return_dict_value_if_key(config, "crawlerConnectionTimeout", 10, only_check_key=True, apply_function=str)
CRAWLDUMPARGS = return_dict_value_if_key(config, "dumpCurrentCrawl", "")
CONTINUECRAWL = return_dict_value_if_key(config, "resumePreviousCrawl", False)
HERITRIXPATH = return_dict_value_if_key(config, "heritrixPath", "")
HERITRIXURL = return_dict_value_if_key(config, "heritrixUrl", "https://localhost:8443", only_check_key=True)
HERITRIXUSER = return_dict_value_if_key(config, "heritrixUser", "admin:admin", only_check_key=True)
CRAWLMAXFOLDERTREEDEPTH = return_dict_value_if_key(config, "crawlMaxFolderTreeDepth", 20, only_check_key=True, apply_function=str)
CRAWLSCOUTSTEPS = return_dict_value_if_key(config, "crawlScoutSteps", 200, only_check_key=True, apply_function=str)

if "crawler" in config:
    if CRAWLTARGET == "linguacrawl":
        # TODO should we change this default value, as we have done with wget, as well?
        CRAWLFILETYPES = ["text/html", "application/pdf"]

USERAGENT = "Mozilla/5.0 (compatible; Bitextor/8 +https://github.com/bitextor/bitextor)"
CRAWLBLACKLISTURL = ['wordpress','blogspot','facebook','google','wikipedia','youtube','perehodi','twitter','instagram']
CRAWLPREFIXFILTER = ['mailto:']

if "crawlerUserAgent" in config:
    USERAGENT = config["crawlerUserAgent"]
if "crawlBlackListURL" in config:
    CRAWLBLACKLISTURL = config["crawlBlackListURL"]
if "crawlPrefixFilter" in config:
    CRAWLPREFIXFILTER = config["crawlPrefixFilter"]

#################################################################
# PREPROCESS
PPROC = "warc2text"
PPROC_FILES = ["text.gz", "url.gz", "mime.gz"]
TEXT_FILE = "text.gz"
HTML_FILE = ""

if "writeHTML" in config and config["writeHTML"]:
    HTML_FILE = "html.gz"
    PPROC_FILES.append("html.gz")
if "preprocessor" in config:
    if config["preprocessor"] == "warc2preprocess":
        PPROC = "w2p"
        PPROC_FILES = ["plain_text.gz", "url.gz", "mime.gz", "normalized_html.gz", "deboilerplate_html.gz"]
        TEXT_FILE = "plain_text.gz"
        HTML_FILE = "deboilerplate_html.gz"
    else:
        PPROC = config["preprocessor"]

SHARDS = config["shards"]
BATCHES = config["batches"]

BOILERPLATE_CLEANING = config["boilerplateCleaning"]
PARAGRAPH_IDENTIFICATION = config["paragraphIdentification"]
ADDITIONAL_METADATA = config["additionalMetadata"]

CLEANHTML = return_dict_value_if_key(config, "cleanHTML", "", pos_value="--cleanhtml")
FTFY = return_dict_value_if_key(config, "ftfy", "", pos_value="--ftfy")
LANGID = return_dict_value_if_key(config, "langID", "cld2", only_check_key=True)
HTML5LIB = return_dict_value_if_key(config, "html5lib", "", pos_value="--html5lib")
BOILERPIPE_MAX_HEAP_SIZE = return_dict_value_if_key(config, "boilerpipeMaxHeapSize", -1, only_check_key=True)

PARSER = ""
PDFEXTRACT = ""
if "parser" in config:
    PARSER = f"--parser {config['parser']}"
if "PDFextract" in config and config["PDFextract"]:
    PDFEXTRACT_CF = ""
    PDFEXTRACT_SJ = ""
    PDFEXTRACT_KL = ""

    if "PDFextract_configfile" in config and config["PDFextract_configfile"]:
        PDFEXTRACT_CF = f" --pe_configfile {config['PDFextract_configfile']}"
    if "PDFextract_sentence_join_path" in config and config["PDFextract_sentence_join_path"]:
        PDFEXTRACT_SJ = f" --sentence_join_path {config['PDFextract_sentence_join_path']}"
    if "PDFextract_kenlm_path" in config and config["PDFextract_kenlm_path"]:
        PDFEXTRACT_KL = f" --kenlm_path {config['PDFextract_kenlm_path']}"

    PDFEXTRACT = f"--pdfextract {PDFEXTRACT_CF} {PDFEXTRACT_SJ} {PDFEXTRACT_KL}"

# sentence splitting and tokenisation
SENTTOKS = return_dict_value_if_key(config, "sentenceSplitters", {})
CUSTOMNBPS = return_dict_value_if_key(config, "customNBPs", {})
WORDTOKS = return_dict_value_if_key(config, "wordTokenizers", {})
MORPHTOKS = return_dict_value_if_key(config, "morphologicalAnalysers", {})

WORDTOK1 = f"{WORKFLOW}/data/moses/tokenizer/tokenizer.perl -q -b -a -l {LANG1}"
WORDTOK2 = f"{WORKFLOW}/data/moses/tokenizer/tokenizer.perl -q -b -a -l {LANG2}"

if WORDTOKS:
    WORDTOK1 = get_lang_or_default(WORDTOKS, LANG1)
    WORDTOK2 = get_lang_or_default(WORDTOKS, LANG2)

PRUNE_THRESHOLD = f"--prune {config['pruneThreshold']}"
PRUNE_TYPE = f"--prune-type {config['pruneType']}"

# Sharding
EMPTY_SHARD_CHECK = f"{TMPDIR}/empty_shard"
EMPTY_SHARD_BATCH_DIR = "EMPTY/1"

#################################################################
# NEURAL
NEURAL_TOOLS_BATCH_SIZE = {
    "NDA": 32,
    "vecalign": 32,
    "bicleaner": None,
}

if "neuralToolsBatchSize" in config:
    for k in config["neuralToolsBatchSize"]:
        NEURAL_TOOLS_BATCH_SIZE[k] = config["neuralToolsBatchSize"][k]

        if NEURAL_TOOLS_BATCH_SIZE[k] < 1:
            LOGGER.warning("Zero or negative value in parallelWorkers['%s']: value resets to 1", k)

            NEURAL_TOOLS_BATCH_SIZE[k] = 1

EMBEDDINGS_MODEL = return_dict_value_if_key(config, "embeddingsModel", "", only_check_key=True)

#################################################################
# DOCALIGN
DOCALIGN = config["documentAligner"]
DOC_THRESHOLD = return_dict_value_if_key(config, "documentAlignerThreshold", 0.1, only_check_key=True)

# mt
MT_COMMAND = config["alignerCmd"] if DOCALIGN == "externalMT" else None
SRC_LANG = LANG1
TRG_LANG = LANG2

if "translationDirection" in config and config["translationDirection"] == f"{LANG2}2{LANG1}":
    # Swap necessary variables
    SRC_LANG, TRG_LANG = LANG2, LANG1
    WORDTOK1, WORDTOK2 = WORDTOK2, WORDTOK1

# dic-based docalign
# snakemake/include/dic-docsegalign
#################################################################
# dic
# snakemake/include/dic-generation

DIC = return_dict_value_if_key(config, "dic", None)
DIC_GENERATE = return_dict_value_if_key(config, "generateDic", False)

#################################################################
# SEGALIGN
SEGALIGN = config["sentenceAligner"]
# bleualign
SEGALIGN_THRESHOLD = return_dict_value_if_key(config, "sentenceAlignerThreshold", 0.0)

# hunalign
# snakemake/include/dic-docsegalign
#################################################################
# CLEANING
DEFERRED = return_dict_value_if_key(config, "deferred", False, pos_value=True)
DEFERRED_CMD = "mmhsum" if DEFERRED else '' # Do NOT use double quote in the command

BIFIXER = return_dict_value_if_key(config, "bifixer", False, pos_value=True)
BIFIXER_DEFERRED_COLS = "--sdeferredcol src_deferred_hash --tdeferredcol trg_deferred_hash" if DEFERRED else ""
BIFIXER_PARAGRAPHS_COLS = "--sparagraphid src_paragraph_id --tparagraphid trg_paragraph_id" if PARAGRAPH_IDENTIFICATION else ""
# Enabled by default if not provided
BIFIXER_AGGRESSIVE_DEDUP = "--aggressive_dedup" if "bifixerAggressiveDedup" not in config else \
    return_dict_value_if_key(config, "bifixerAggressiveDedup", "", pos_value="--aggressive_dedup")
# Enabled by default if not provided
BIFIXER_IGNORE_SEGMENTATION = "--ignore_segmentation" if "bifixerIgnoreSegmentation" not in config else \
    return_dict_value_if_key(config, "bifixerIgnoreSegmentation", "", pos_value="--ignore_segmentation")

BICLEANER = return_dict_value_if_key(config, "bicleaner", False, pos_value=True)
BICLEANER_FLAVOUR = return_dict_value_if_key(config, "bicleanerFlavour", "classic", only_check_key=True)
BICLEANER_MODEL = config["bicleanerModel"] if BICLEANER else ""
BICLEANER_THRESHOLD = return_dict_value_if_key(config, "bicleanerThreshold", 0.0)
BICLEANER_GENERATE_MODEL = return_dict_value_if_key(config, "bicleanerGenerateModel", False)
BICLEANER_TRAINING_CORPUS_PREFIX = return_dict_value_if_key(config, "bicleanerParallelCorpusTrainingPrefix", [])
BICLEANER_AI_MONO_CORPUS_PREFIX = return_dict_value_if_key(config, "bicleanerMonoCorpusPrefix", [])
BICLEANER_AI_DEV_CORPUS_PREFIX = return_dict_value_if_key(config, "bicleanerParallelCorpusDevPrefix", [])
BICLEANER_BIN = "bicleaner-classify" if BICLEANER_FLAVOUR == "classic" else "bicleaner-ai-classify"

if BICLEANER:
    is_flavour_ai = BICLEANER_FLAVOUR == "ai"
    path_exists_f=os.path.isdir if is_flavour_ai else os.path.isfile

    # Create soft links based on the flavour if necessary
    if is_flavour_ai:
        # Get directory path from bicleaner model path
        BICLEANER_MODEL = '/'.join(BICLEANER_MODEL.split('/')[:-1])

    # BICLEANER_MODEL will be a directory if is_flavour_ai else will be a file

ELRC = return_dict_value_if_key(config, "elrc", False, pos_value=True)
TMX = return_dict_value_if_key(config, "tmx", False, pos_value=True)
DEDUPED = return_dict_value_if_key(config, "deduped", False, pos_value=True)

BIROAMER = return_dict_value_if_key(config, "biroamer", False, pos_value=True)
BIROAMER_MIX_FILES = "/dev/null"
BIROAMER_ALIGNMENT_CORPUS = ""
BIROAMER_OMIT = return_dict_value_if_key(config, "biroamerOmitRandomSentences", "", pos_value="-o")

OUTPUT_FILES = ["sent", "raw"]
if "granularity" in config:
    if  "documents" in config["granularity"]:
        OUTPUT_FILES.append("documents")
STATS_FILES = ["sent", "raw"]

if TMX:
    OUTPUT_FILES.append("not-deduped.tmx")
if DEDUPED:
    OUTPUT_FILES.append("deduped.tmx")
    OUTPUT_FILES.append("deduped.txt")
    STATS_FILES.append("deduped")
if BIROAMER:
    if TMX:
        OUTPUT_FILES.append("not-deduped.roamed.tmx")

    if DEDUPED:
        OUTPUT_FILES.append("deduped.roamed.tmx")

    if "biroamerMixFiles" in config and len(config["biroamerMixFiles"]) > 0:
        BIROAMER_MIX_FILES = " ".join(config["biroamerMixFiles"])

    if "biroamerImproveAlignmentCorpus" in config:
        BIROAMER_ALIGNMENT_CORPUS = f"-a {config['biroamerImproveAlignmentCorpus']}"
if not BICLEANER_GENERATE_MODEL:
    BICLEANER_TRAINING_CORPUS_PREFIX = []
    BICLEANER_AI_MONO_CORPUS_PREFIX = []
    BICLEANER_AI_DEV_CORPUS_PREFIX = []


#################################################################
# DATASOURCES
HOSTS = set()
WARCS = set()
PREVERTICALS = set()

if "warcs" in config:
    WARCS = WARCS.union(config["warcs"])
if "hosts" in config:
    HOSTS = HOSTS.union(config["hosts"])
if "preverticals" in config:
    PREVERTICALS = PREVERTICALS.union(config["preverticals"])
if "hostsFile" in config:
    with open_xz_or_gzip_or_plain(config["hostsFile"]) as f:
        for line in f:
            HOSTS.add(line.strip())
if "warcsFile" in config:
    with open_xz_or_gzip_or_plain(config["warcsFile"]) as f:
        for line in f:
            WARCS.add(line.strip())
if "preverticalsFile" in config:
    with open_xz_or_gzip_or_plain(config["preverticalsFile"]) as f:
        for line in f:
            PREVERTICALS.add(line.strip())

# sort in order to avoid different results across executions
HOSTS = sorted(list(HOSTS))
WARCS = sorted(list(WARCS))
PREVERTICALS = sorted(list(PREVERTICALS))

# group hosts by domain and check their validity
DOMAIN_2_HOSTS = create_domain_key_2_host_map(HOSTS)

# assign an ID to each WARC and check that all WARCs exist
TARGET_2_PROVIDED_WARCS = create_id_key_2_file_map(WARCS, file_desc="WARCs")

# assign an ID to each prevertical and check that all preverticals exist
TARGET_2_PROVIDED_PREVERTICALS = create_id_key_2_file_map(PREVERTICALS, id_offset=len(TARGET_2_PROVIDED_WARCS), file_desc="preverticals")

# group crawled WARCs by domains
TARGET_2_CRAWLED_WARCS = dict([
    (domain, [f"{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz" for host in hosts])
    for (domain, hosts) in DOMAIN_2_HOSTS.items()
])

# group all files (e.g. WARCs, prevertical) by target (either domain if crawled, or ID if provided by user)
TARGET_2_WARCS = {**TARGET_2_CRAWLED_WARCS, **TARGET_2_PROVIDED_WARCS}
TARGET_2_PREVERTICALS = {**TARGET_2_PROVIDED_PREVERTICALS}

#################################################################
### WORKFLOW EXECUTION ##########################################
THREADS = {
    "split": 1,
    "translate": 1,
    "tokenise": 1,
    "docalign": 1,
    "segalign": 1,
    "bifixer": 1,
    "bicleaner": 1,
    "filter": 1,
    "sents": 1,
    "mgiza": 1,
}
JOB_THREADS = {
    "split": 0,
    "translate": 0,
    "tokenise": 0,
    "docalign": 0,
    "segalign": 0,
    "bifixer": 0,
    "bicleaner": 0,
}

if "parallelWorkers" in config:
    for k in config["parallelWorkers"]:
        THREADS[k] = config["parallelWorkers"][k]

        if THREADS[k] < 1:
            LOGGER.warning("Zero or negative value in parallelWorkers['%s']: value resets to 1", k)

            THREADS[k] = 1

if "parallelJobs" in config:
    for k in config["parallelJobs"]:
        JOB_THREADS[k] = config["parallelJobs"][k]

        if JOB_THREADS[k] < 0:
            LOGGER.warning("Negative value in parallelJobs['%s']: value resets to 0", k)

            JOB_THREADS[k] = 0

        # Modify the snakemake resources if they were not provided
        job_resource_name = f"job_{k}"

        if job_resource_name not in workflow.global_resources:
            # Since we want to run as parallel jobs as configured, the resources
            #  are squared in order to leave available the exact number of resources
            #  for each job.
            # E.g., if parallelJobs["bicleaner"] = 4, we want 4 jobs at max. to run
            #  in parallel, so we need 4 "job_bicleaner" resources to be free for each
            #  "bicleaner" job, so 4 * 4 = 16 necessary "job_bicleaner" resources
            workflow.global_resources[job_resource_name] = JOB_THREADS[k] ** 2

OUTPUT = []
UNTIL = return_dict_value_if_key(config, "until", "")
UNTIL_RULES_ALL_LANGS = ("crawl", "preprocess", "shard", "split", "tokenise") # All languages from LANGS

if UNTIL not in UNTIL_RULES_ALL_LANGS:
    # At some point, not all languages from LANGS will be taken into account but only LANG1 and LANG2

    if len(LANGS) > 2:
        # There are different languages to process, not only LANG1 and LANG2

        # Warn about the fact that not all languages will be processed at some point
        if not UNTIL:
            LOGGER.warning("Not all languages will be processed until the end of the pipeline")
        else:
            LOGGER.warning("Not all languages will be processed until '%s'", UNTIL)

if not UNTIL: # config["until"] not provided -> run the whole pipeline
    OUTPUT = expand(
        "{permanent}/{lang1}-{lang2}.{output_file}.gz",
        permanent=PERMANENT,
        lang1=LANG1,
        lang2=LANG2,
        output_file=OUTPUT_FILES,
    )
    OUTPUT.extend(
        expand(
            "{permanent}/{lang1}-{lang2}.stats.{stats_file}",
            permanent=PERMANENT,
            lang1=LANG1,
            lang2=LANG2,
            stats_file=STATS_FILES,
        )
    )

    # We need to add these output files because tokenisation rules are the last rules that don't
    # need both shards. Otherwise snakemake waites for both shards to be completed to continue
    # TODO if we don't have the MT docalign set, then tokenization is not the last step but sentence aligner -> are we doing extra work?
    OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{TRG_LANG}")

    if DOCALIGN == "externalMT":
        OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}")
    elif DOCALIGN in ("DIC", "NDA"):
        OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{SRC_LANG}")
elif UNTIL == "crawl":
    if CRAWLTARGET == "linguacrawl":
        OUTPUT.append(f"{DATADIR}/warc/linguacrawl.finished")
    else:
        for domain, hosts in DOMAIN_2_HOSTS.items():
            for host in hosts:
                OUTPUT.append(f"{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz")
elif UNTIL == "preprocess":
    OUTPUT = expand(
        "{datadir}/preprocess/{target}/{pproc}/{lang}/{pproc_file}",
        datadir=DATADIR,
        target=TARGET_2_WARCS,
        pproc=PPROC,
        lang=LANGS,
        pproc_file=PPROC_FILES,
    )
    OUTPUT.extend(expand(
        "{datadir}/preprocess/{target}/prevertical2text/{lang}/{pproc_file}",
        datadir=DATADIR,
        target=TARGET_2_PREVERTICALS,
        lang=LANGS,
        pproc_file=PPROC_FILES,
    ))
elif UNTIL == "shard":
    OUTPUT = expand("{datadir}/shards/02.batches.{lang}", datadir=DATADIR, lang=LANGS)
elif UNTIL == "split":
    OUTPUT = expand("{datadir}/shards/03.split.{lang}", datadir=DATADIR, lang=LANGS)
elif UNTIL == "tokenise":
    OUTPUT = expand("{datadir}/shards/05.tokenise.{lang}", datadir=DATADIR, lang=LANGS)

# TODO should we implement all languages provided in LANGS? They're not from this point forward
# SRC_LANG and TRG_LANG dependent
elif UNTIL == "translate":
    OUTPUT = f"{DATADIR}/shards/04.translate.{SRC_LANG}2{TRG_LANG}"
elif UNTIL == "tokenise_trg":
    OUTPUT = f"{DATADIR}/shards/05.tokenise.{TRG_LANG}"
elif UNTIL == "tokenise_src":
    if DOCALIGN == "externalMT":
        OUTPUT = f"{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}"
    elif DOCALIGN in ("DIC", "NDA"):
        OUTPUT = f"{DATADIR}/shards/05.tokenise.{SRC_LANG}"
else:
    # We need to add these output files because tokenisation rules are the last rules that don't
    # need both shards. Otherwise snakemake waites for both shards to be completed to continue
    # TODO if we don't have the MT docalign set, then tokenization is not the last step but sentence aligner -> are we doing extra work?
    OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{TRG_LANG}")

    if DOCALIGN == "externalMT":
        OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}")
    elif DOCALIGN in ("DIC", "NDA"):
        OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{SRC_LANG}")

    if UNTIL == "docalign":
        OUTPUT.append(f"{TRANSIENT}/06_01.docalign.{LANG1}_{LANG2}")
    elif UNTIL == "segalign":
        OUTPUT.append(f"{TRANSIENT}/06_02.segalign.{LANG1}_{LANG2}")
    elif UNTIL == "bifixer":
        OUTPUT.append(f"{TRANSIENT}/07_01.bifixer.{LANG1}_{LANG2}")
    elif UNTIL == "bicleaner":
        OUTPUT.append(f"{TRANSIENT}/07_02.bicleaner.{LANG1}_{LANG2}")
    elif UNTIL == "filter":
        OUTPUT.append(f"{TRANSIENT}/07_03.filter.{LANG1}_{LANG2}")

#################################################################
# OTHER
GPU_TOKENS_DIR = f"{TRANSIENT}/gpu_tokens" # We use TRANSIENT instead of TMPDIR since TRANSIENT have to be unique,
                                           #  but TMPDIR might be the same directory for different instances
                                           #  (taking into account the semantic of the variable names of the directories)
PERSISTENT_STORAGE = PersistentDict("bitextor") # Global dictionary (shared in all processes)
                                                # thread-safe -> it handles concurrency

# Check NLTK models (they need to be downloaded now because the download is not thread-safe)
if BIFIXER:
    check_nltk_model.check("tokenizers/punkt", "punkt")
if BIROAMER:
    check_nltk_model.check("misc/perluniprops", "perluniprops")

# Metadata from text
ADDITIONAL_METADATA = ADDITIONAL_METADATA and (len(PREVERTICALS) != 0) # Update with additional conditions if needed
PROPAGATE_METADATA_FROM_TEXT = PARAGRAPH_IDENTIFICATION or ADDITIONAL_METADATA
PROPAGATE_METADATA_HEADERS = []
METADATA_HEADERS_NUMBER_FIELDS = 0

if PROPAGATE_METADATA_FROM_TEXT:
    if PARAGRAPH_IDENTIFICATION:
        PROPAGATE_METADATA_HEADERS.append("paragraph_id")
        METADATA_HEADERS_NUMBER_FIELDS += 1
    if ADDITIONAL_METADATA:
        # Different conditions which adds metadata from preprocessors
        if len(PREVERTICALS) != 0:
            PROPAGATE_METADATA_HEADERS.extend(["doc_title", "crawl_date", "file_type", "boilerplate", "heading_html_tag"])
            METADATA_HEADERS_NUMBER_FIELDS += 5

PROPAGATE_METADATA_HEADERS = ','.join(PROPAGATE_METADATA_HEADERS)

shell.prefix("set -euo pipefail;")

#################################################################
### HANDLERS ####################################################

onstart:
    for line in pprint.pformat(config, indent=2, width=100).strip().split('\n'):
        LOGGER.debug("Config: %s", line)

    LOGGER.debug("-----------------------------------------------")

    for line in subprocess.run(["locale"], stdout=subprocess.PIPE).stdout.decode("utf-8").strip().split('\n'):
        LOGGER.debug("Locale: %s", line)

    LOGGER.debug("-----------------------------------------------")

    for line in subprocess.run(["printenv"], stdout=subprocess.PIPE).stdout.decode("utf-8").strip().split('\n'):
        LOGGER.debug("Envvar: %s", line)

    LOGGER.debug("-----------------------------------------------")

    for line in subprocess.run(["df", "-h"], stdout=subprocess.PIPE).stdout.decode("utf-8").strip().split('\n'):
        LOGGER.debug("Storage: %s", line)

    LOGGER.debug("-----------------------------------------------")

    # TODO log variables which might not be defined in the config variable if VERBOSE? (e.g. BIFIXER_AGGRESSIVE_DEDUP)

    # Initialize persistent dict
    initialize_persistent_dict(PERSISTENT_STORAGE)

onerror:
    LOGGER.info("Executing teardown...")
    flush_logger(LOGGER) # Teardown might take a while, so at least inform about it

    allocate_cuda_devices_teardown(PERSISTENT_STORAGE, f"{GPU_TOKENS_DIR}/cuda_teardown_onerror")

    LOGGER.error("The execution finished with errors...")

onsuccess:
    LOGGER.info("Executing teardown...")
    flush_logger(LOGGER) # Teardown might take a while, so at least inform about it

    allocate_cuda_devices_teardown(PERSISTENT_STORAGE, f"{GPU_TOKENS_DIR}/cuda_teardown_onsuccess")

    LOGGER.info("The execution finished without errors!")

#################################################################
### FINAL OUTPUTS ###############################################
rule all:
    input:
        OUTPUT,


#################################################################
### INCLUDE #####################################################
include: "rules/dic_generation.smk"
include: "rules/dic_doc_seg_align.smk"


#################################################################
### CRAWLING ####################################################

rule wget_download:
    """
    Download {target} with wget
    """
    params:
        url="http://{target}",
        folder=lambda wildcards, output: os.path.dirname(output[0]),
        time_limit=apply_format(CRAWLTIMELIMIT, "-t {}s"),
        user_agent=apply_format(USERAGENT, "-a '{}'"),
        wait=apply_format(CRAWLWAIT, "--wait {}"),
        file_types=apply_format(",".join(CRAWLFILETYPES), "-f {}")
    output:
        f"{DATADIR}/warc/{{target}}/wget.warc.gz",
    shell:
        """
        mkdir -p {params.folder} {TMPDIR}
        DIRNAME=$(mktemp -d "{TMPDIR}/downloaded.{wildcards.target}.XXXXXX")
        {PROFILING} python3 {WORKFLOW}/bitextor_wget.py --url {params.url} --output-path $DIRNAME {params.time_limit} {params.user_agent} {params.file_types} {params.wait} --warc {output}
        rm -rf $DIRNAME
        """


rule heritrix_download:
    """
    Download {target} with heritrix
    """
    params:
        url="http://{target}",
        folder=lambda wildcards, output: os.path.dirname(output[0]),
        time_limit=CRAWLTIMELIMIT
    output:
        f"{DATADIR}/warc/{{target}}/heritrix.warc.gz",
    shell:
        """
        URL=$(python3 -c "from bitextor.utils.common import check_connection; \
            e, url = check_connection('{params.url}'); \
            print(url) ; \
            exit(e)")
        if [ $? -ne 0 ]; then
            touch {DATADIR}/warc/{wildcards.target}/heritrix.warc
            gzip {DATADIR}/warc/{wildcards.target}/heritrix.warc
        else
            mkdir -p {params.folder} {TMPDIR}
            if [ "$(ps aux | grep -i Heritrix | grep -v grep)" == "" ]
                then {HERITRIXPATH}/bin/heritrix -a {HERITRIXUSER}
            fi
            curl -v -d "action=teardown" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
            curl -v -d "createpath={wildcards.target}&action=create" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine
            DIRNAME=$(mktemp -d "{TMPDIR}/downloaded.{wildcards.target}.XXXXXX")
            cat {WORKFLOW}/data/crawler-beans.cxml | sed "s@http://example.example/example@${{URL}}@g" > $DIRNAME/my-crawler-beans.cxml
            curl -v -T $DIRNAME/my-crawler-beans.cxml -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}/jobdir/crawler-beans.cxml
            curl -v -d "action=build" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
            curl -v -d "action=launch&checkpoint=latest" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
            sleep 2
            curl -v -d "action=unpause" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
            RUNTIME=0
            sleep 15
            while [ -f {HERITRIXPATH}/jobs/{wildcards.target}/latest/warcs/*warc.gz.open ]
            do
                sleep 5
                RUNTIME=$((RUNTIME+5))
                if [ "{params.time_limit}" != "" ]
                then
                    if [ $RUNTIME -gt "{params.time_limit}" ]
                    then
                        echo "Crawling time limit reached"
                        curl -v -d "action=pause" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                        curl -v -d "action=checkpoint" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                        curl -v -d "action=terminate" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                    fi
                fi
            done
            echo "Job {wildcards.target} finished!"
            cat {HERITRIXPATH}/jobs/{wildcards.target}/*/warcs/*warc.gz > {output}
        fi
        """


rule linguacrawl_config:
    """
    Create a linguacrawl yaml config file from Bitextor's parameters
    """
    output:
        yaml_file=temp(f"{TMPDIR}/linguacrawl.yaml"),
    params:
        user_agent=apply_format(USERAGENT, "user_agent: '{}'\n"),
        output_dir=apply_format(f"{DATADIR}/warc/linguacrawl", "output_dir: '{}'\n"),
        wait=apply_format(CRAWLWAIT, "crawl_delay: {}\n"),
        time_limit=apply_format(CRAWLTIMELIMIT, "max_time_per_site: {}\n"),
        size_limit=apply_format(CRAWLSIZELIMIT, "max_size_per_site: {}\n"),
        timeout=apply_format(CRAWLTIMEOUT, "connection_timeout: {}\n"),
        num_threads=apply_format(CRAWLJOBS, "max_jobs: {}\n"),
        prefix_filter=apply_format(str(CRAWLPREFIXFILTER), "prefix_filter: {}\n"),
        resume_crawling=apply_format(str(CONTINUECRAWL), "resume_crawling: {}\n"),
        dump_args=apply_format(str(CRAWLDUMPARGS), "verbose: {}\n"),
        file_types=apply_format(f"({'|'.join(CRAWLFILETYPES)})", "accepted_content: '{}'\n"),
        max_folder_depth=apply_format(CRAWLMAXFOLDERTREEDEPTH, "max_folder_tree_depth: {}\n"),
        scout_steps=apply_format(CRAWLSCOUTSTEPS, "scout_steps: {}\n"),
        blacklist=apply_format(str(CRAWLBLACKLISTURL), "url_blacklist: {}\n"),
    run:
        yaml_content = ""

        langs = "','".join(LANGS)
        hosts = "','".join(HOSTS)

        # Mandatory
        yaml_content += params.user_agent
        yaml_content += f"langs_of_interest: ['{langs}']\n"
        yaml_content += params.output_dir
        # Optional (the argument parser will not complain, but the crawler will crash in some cases)
        yaml_content += params.wait
        yaml_content += params.time_limit
        yaml_content += params.size_limit
        yaml_content += params.timeout
        yaml_content += params.num_threads
        yaml_content += params.resume_crawling
        yaml_content += f"seed_urls: ['{hosts}']\n"
        # yaml_content += f"seed_urls_from_file: \n" # HOSTS contain all hosts if hosts defined either with or without file
        yaml_content += params.prefix_filter
        yaml_content += params.dump_args
        yaml_content += params.file_types
        yaml_content += params.max_folder_depth
        yaml_content += f"max_attempts: 3\n"
        yaml_content += params.scout_steps
        yaml_content += f"min_langs_in_site: 2\n"
        yaml_content += params.blacklist

        # TODO use specific argument? If not, use SRC_LANG/TRG_LANG?
        if LANG1 or LANG2:
            yaml_content += f"mandatory_lang: '{LANG1}'\n" if LANG1 else f"mandatory_lang: '{LANG2}'\n"
            yaml_content += f"min_percent_mandatory_lang: 10\n"

        # tld = LANGS.union([params.url.split('.')[-1]]).union(TLD_CRAWL)
        tld = LANGS.union([url.split(".")[-1] for url in HOSTS]).union(TLD_CRAWL)
        tld = "','".join(tld)

        yaml_content += f"accepted_tlds: ['{tld}']\n"

        fdescriptor = open(output.yaml_file, "w")
        fdescriptor.write(yaml_content)
        fdescriptor.flush()
        fdescriptor.close()


# This must be a checkpoint because we do not know the resulted warcs since we let the user decide if they want to
#  either join all the warcs (it loses important information like the source of the warcs) or not
# Manual stopping: kill -s sigint `ps aux | grep bin/linguacrawl | grep -v grep | awk '{print $2}'`
# TODO move code to a single python script and call it
checkpoint linguacrawl_download:
    """
    Download HOSTS with linguacrawl
    """
    input:
        f"{TMPDIR}/linguacrawl.yaml"
    params:
        folder=f"{DATADIR}/warc/linguacrawl",
        crawl_cat=config["crawlCat"] if "crawlCat" in config else False,
        crawl_cat_max=config["crawlCatMaxSize"]*1024*1024 if "crawlCatMaxSize" in config else 0
    output:
        f"{DATADIR}/warc/linguacrawl.finished",
    run:
        shell(
            """
            mkdir -p {params.folder} {TMPDIR}
            {PROFILING} linguacrawl {input}
            """
        )

        all_files = subprocess.Popen(("ls", params.folder), stdout=subprocess.PIPE)
        try:
            # Process the resulted warcs
            warcs = subprocess.check_output(
                ("grep", "[.]warc[.]gz$"), stdin=all_files.stdout
            )  # It will throw an exception if no warcs were downloaded
            all_files.wait()

            warcs = warcs.decode("utf-8").split("\n")
            warcs = list(filter(lambda warc: len(warc) != 0, warcs))
            warcs = list(map(lambda warc: f"{params.folder}/{warc}", warcs))

            if not params.crawl_cat:
                warcs = "\n".join(warcs)
                shell(f'echo -e "{warcs}" > {{output[0]}}')
            else:
                if params.crawl_cat_max <= 0:
                    warcs = " ".join(warcs)
                    shell(
                        f"""
                        cat {warcs} > {params.folder}/linguacrawl.warc.gz
                        echo "{params.folder}/linguacrawl.warc.gz" > {{output[0]}}
                        """
                    )
                else:
                    # Improve the number of preprocess rules that are executed
                    current = 0
                    blocks = 0
                    files = [f"{params.folder}/linguacrawl{blocks}.warc.gz"]

                    for warc in warcs:
                        if current > {params.crawl_cat_max}:
                            current = 0
                            blocks += 1
                            files.append(f"{params.folder}/linguacrawl{blocks}.warc.gz")

                        shell(f"cat {warc} >> {files[-1]}")

                        du_command = subprocess.Popen(("du", "-b", warc), stdout=subprocess.PIPE)

                        try:
                            du_warc = subprocess.check_output(("awk", "{print $1}"), stdin=du_command.stdout)
                            du_command.wait()

                            current += int(du_warc)
                        except:
                            LOGGER.warning("Could not retrieve the size of %s", warc)

                    files = "\n".join(files)
                    shell(f'echo -e "{files}" > {{output[0]}}')
        except:
            # No warcs were downloaded: error or warning
            if len(WARCS) == 0:
                LOGGER.error("Could not find any file after crawling")
                sys.exit(1)
            LOGGER.warning("Could not find any file after crawling")
            shell(f"touch {output}")


#################################################################
### PREPROCESS ##################################################
rule warc2preprocess:
    """
    Process a list of WARCs (or a single WARC)
        and produce {plain_text,mime,url,normalized_html,deboilerplate_html}.gz
    """
    input:
        get_pproc_input,
    output:
        expand("{data}/preprocess/{{target}}/w2p/{lang}/{pproc_file}", data=DATADIR, lang=LANGS, pproc_file=PPROC_FILES),
    threads: 2
    params:
        folder=lambda wildcards, output: os.path.dirname(os.path.dirname(output[0])),  # remove "{lang}/{pproc_file}"
        pproclangs=','.join(LANGS),
        boilerplate="--boilerpipe" if BOILERPLATE_CLEANING else '',
        heap_size=apply_format(str(BOILERPIPE_MAX_HEAP_SIZE) if BOILERPIPE_MAX_HEAP_SIZE >= 0 else '', "--boilerpipe-max-heap-size {}"),
        paragraphs="--paragraph-identification" if PARAGRAPH_IDENTIFICATION else '',
    shell:
        """
        mkdir -p {params.folder}
        cat {input} \\
            | {PROFILING} python3 {WORKFLOW}/bitextor_warc2htmlwarc.py {CLEANHTML} {FTFY} {PDFEXTRACT} --disable-output-gzip \\
            | {PROFILING} python3 {WORKFLOW}/bitextor_warc2preprocess.py --input - --langs {params.pproclangs} \\
                --compression gz --langid {LANGID} {params.boilerplate} {params.heap_size} {HTML5LIB} {PARSER} \\
                --output-dir {params.folder} {params.paragraphs}
        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/plain_text.gz ]; then
                >&2 echo "WARNING: no '$lang' data found in {wildcards.target}: creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{plain_text,mime,url,normalized_html,deboilerplate_html}}
                gzip {params.folder}/$lang/{{plain_text,mime,url,normalized_html,deboilerplate_html}}

                # Generate empty file in order to be able to check if is needed to generate empty shards later
                touch "{EMPTY_SHARD_CHECK}_$lang"
            fi
        done
        """


rule warc2text:
    """
    Process a list of WARCs (or a single WARC)
        and produce {text,mime,url}.gz and optionally html.gz
    """
    input:
        get_pproc_input,
    output:
        expand(
            "{data}/preprocess/{{target}}/warc2text/{lang}/{pproc_file}",
            data=DATADIR,
            lang=LANGS,
            pproc_file=PPROC_FILES,
        ),
    params:
        folder=lambda wildcards, output: os.path.dirname(os.path.dirname(output[0])),  # remove "{lang}/{pproc_file}"
        f=",".join([f.strip(".gz") for f in PPROC_FILES]),
        paragraphs='--paragraph-identification' if PARAGRAPH_IDENTIFICATION else '',
    shell:
        """
        mkdir -p {params.folder}
        {PROFILING} warc2text -o {params.folder} -s -f {params.f} {params.paragraphs} {input}
        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/text.gz ]; then
                >&2 echo "WARNING: no '$lang' data found in {wildcards.target}: creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{{params.f}}}
                gzip {params.folder}/$lang/{{{params.f}}}

                # Generate empty file in order to be able to check if is needed to generate empty shards later
                touch "{EMPTY_SHARD_CHECK}_$lang"
            fi
        done
        """


rule prevertical2text:
    """
    Process a list of prevertical format files (or a single prevertical format file)
        and produce {text,mime,url}.gz and optionally html.gz
    """
    input:
        lambda wildcards: TARGET_2_PREVERTICALS[wildcards.target],
    output:
        expand(
            "{data}/preprocess/{{target}}/prevertical2text/{lang}/{pproc_file}",
            data=DATADIR,
            lang=LANGS,
            pproc_file=PPROC_FILES,
        ),
    params:
        folder=lambda wildcards, output: os.path.dirname(os.path.dirname(output[0])),  # remove "{lang}/{pproc_file}"
        f=",".join([f.strip(".gz") for f in PPROC_FILES]),
        boilerplate='-b' if BOILERPLATE_CLEANING else '',
        paragraphs='-p' if PARAGRAPH_IDENTIFICATION else '',
    shell:
        """
        CAT=$([[ {input} == *.gz ]] && echo "zcat" || echo "cat")

        mkdir -p {params.folder}
        $CAT {input} \\
            | python3 {WORKFLOW}/bitextor_prevertical_lang_iso639_1.py \\
            | {PROFILING} prevertical2text -o {params.folder} -s -f {params.f} {params.boilerplate} {params.paragraphs} -

        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/text.gz ]; then
                >&2 echo "WARNING: no '$lang' data found in {wildcards.target}: creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{{params.f}}}
                gzip {params.folder}/$lang/{{{params.f}}}

                # Generate empty file in order to be able to check if is needed to generate empty shards later
                touch "{EMPTY_SHARD_CHECK}_$lang"
            fi
        done
        """


# DAG will be re-evaluated after completing shard rule (because number of batches is dynamic and unknown)
checkpoint shard:
    """
    Use giashard to shard the output of warc2text/warc2preprocess to balance jobs
    :input: output of the preprocessing rules
    :output: a plain text files that contains the list of every batch generated
        (i.e. each line has a path to a batch folder)
    """
    input:
        # this is separated in two functions to make sure that
        # the preprocessing of the provided files and the crawling
        # may be executed in parallel
        # (otherwise the linguacrawl checkpoint might prevent that)
        get_shard_input_files,
        get_shard_input_crawled,
    output:
        f"{DATADIR}/shards/02.batches.{{lang}}",  # list of batches created for lang
    params:
        n=SHARDS,
        b=BATCHES,
        o_no_lang=lambda wildcards, output: os.path.dirname(output[0]),
        o=f"{DATADIR}/shards/{{lang}}",
        f=",".join([f.strip(".gz") for f in PPROC_FILES]),
        l=temp(f"{TMPDIR}/shard_list_files.{{lang}}"),
    shell:
        """
        ulimit -n 2048
        mkdir -p {params.o}
        rm -rf {params.o}/* # remove anything that might be left after a previous run
        rm -f {params.l}
        find {DATADIR}/preprocess/*/prevertical2text/{wildcards.lang} -type d >> {params.l} 2> /dev/null || true
        find {DATADIR}/preprocess/*/{PPROC}/{wildcards.lang} -type d >> {params.l} 2> /dev/null || true

        giashard_bin=$([[ "$(command -v giashard)" == "" ]] && echo "$HOME/go/bin/giashard" || echo "giashard")

        {PROFILING} $giashard_bin -n {params.n} -b {params.b} -o {params.o} -f {params.f} -l {params.l}

        nofiles=$(ls {params.o} | wc -l)

        if [[ "$nofiles" == "0" ]] && [[ -f "{EMPTY_SHARD_CHECK}_{wildcards.lang}" ]]; then
            # No files generated
            >&2 echo "WARNING: no files generated after running giashard for lang '{wildcards.lang}': creating empty files instead"

            # Generate empty shards if needed in order to avoid the pipeline to break
            mkdir -p {params.o}/{EMPTY_SHARD_BATCH_DIR}

            touch {params.o}/{EMPTY_SHARD_BATCH_DIR}/empty
            touch {params.o}/{EMPTY_SHARD_BATCH_DIR}/{{{params.f}}}
            gzip {params.o}/{EMPTY_SHARD_BATCH_DIR}/{{{params.f}}}
        fi

        ls -d {params.o}/*/* > {output}
        """


rule split:
    """
    Use sentence splitter to obtain sentences from the plain text file
    :input: gz-compressed file with a base64-encoded document per line
        document is the plain text extracted by the preprocess
    :output: gz-compressed file with a base64-encoded document per line
        output must have the same number of lines as the input (i.e. same number of docs)
    """
    input:
        f"{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/{TEXT_FILE}",
    output:
        sentences=f"{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/sentences.gz",
        metadata=expand(
            "{data}/shards/{{lang}}/{{shard}}/{{batch}}/{f}.gz",
            data=DATADIR,
            f=["metadata"] if PROPAGATE_METADATA_FROM_TEXT else []
        ),
    params:
        splitter=lambda wildcards: apply_format(get_lang_or_default(SENTTOKS, wildcards.lang), '--sentence-splitter "{}"'),
        customnbp=lambda wildcards: apply_format(get_customnbp(CUSTOMNBPS, wildcards.lang), '--customnbp "{}"'),
        paragraphs='--process-paragraphs' if PARAGRAPH_IDENTIFICATION else '',
        metadata=lambda wildcards, output: "--propagate-metadata" if ADDITIONAL_METADATA else "",
        metadata_output=lambda wildcards, output: apply_format('Â¿?'.join(output.metadata), '--metadata-output "{}"'),
        metadata_aux_output=temp(f"{TMPDIR}/split_rule_metadata/aux_metadata_output.{{lang}}.{{shard}}_{{batch}}.gz"),
    threads: THREADS["split"]
    resources:
        job_split=JOB_THREADS["split"],
    shell:
        """
        parallel_cmd=$([[ {threads} -gt 1 ]] && echo "parallel --gnu --halt 2 --pipe --j {threads} -k" || echo "")

        zcat {input} \\
            | {PROFILING} ${{parallel_cmd}} python3 {WORKFLOW}/bitextor_split.py \\
                {params.splitter} {params.customnbp} --langcode {wildcards.lang} {PRUNE_THRESHOLD} \\
                {PRUNE_TYPE} {params.paragraphs} {params.metadata} {params.metadata_output} \\
            | pigz -c > {output.sentences}

        # Check metadata
        if [[ ! -z "{output.metadata}" ]]; then
            if [[ ! -f "{output.metadata}" ]]; then
                >&2 echo "WARNING: metadata output file should exist but doesn't: creating fake metadata file"

                zcat "{output.sentences}" \\
                    | b64filter awk '{{for (i=1; i<'"{METADATA_HEADERS_NUMBER_FIELDS}"'; i++) printf "\\t"}} {{printf "\\n"}}' \\
                    | pigz -c > "{output.metadata}"
            else
                header_number_fields=$(zcat "{output.metadata}" | tail -n1 | base64 -d | tail -n1 | tr -cd $'\\t' | wc -c)
                header_number_fields=$(( $header_number_fields + 1 )) # We want the number of fields, and we have the number of <tab>

                if [[ "$header_number_fields" -ne "{METADATA_HEADERS_NUMBER_FIELDS}" ]]; then
                    if [[ "$header_number_fields" -gt "{METADATA_HEADERS_NUMBER_FIELDS}" ]]; then
                        # There are more metadata fields than expected: warn about it
                        >&2 echo "WARNING: more metadata fields than expected: $header_number_fields vs" \\
                                 "{METADATA_HEADERS_NUMBER_FIELDS}"
                    else
                        # TODO this is a workaround for allowing to run multiple preprocessor in one instance for preprocessors which
                        #  doesn't provide any metadata but, optionally, the paragraph identification. This might not work in the
                        #  moment that different preprocessor may provide different metadata, and in that case, all preprocessor
                        #  will have to provide the same metadata (for those fields which doesn't have sense in a preprocessor, the
                        #  solution might be to write empty metadata fields)

                        # We need to fake the missing header fields
                        mkdir -p "$(dirname {params.metadata_aux_output})"

                        missing_header_fields=$(( "{METADATA_HEADERS_NUMBER_FIELDS}" - "$header_number_fields" ))

                        >&2 echo "WARNING: metadata fields are not enough: creating fake metadata for the missing fields" \\
                                 "($header_number_fields vs {METADATA_HEADERS_NUMBER_FIELDS})"

                        zcat "{output.sentences}" \\
                            | b64filter awk '{{for (i=1; i<'"$missing_header_fields"'; i++) printf "\\t"}} {{printf "\\n"}}' \\
                            | pigz -c > "{params.metadata_aux_output}"
                        mv -n "{output.metadata}" "{output.metadata}.aux.gz"
                        paste <(zcat "{output.metadata}.aux.gz") <(zcat "{params.metadata_aux_output}") \\
                            | python3 {WORKFLOW}/utils/join_b64_docs.py \\
                            | pigz -c > "{output.metadata}"
                    fi
                fi
            fi
        fi
        """


rule aggregate_split:
    """
    Helper rule to implement until=split config
    :input: the result of every split rule
    :output: a file that contains the path to the output of every split rule per lang
    """
    input:
        lambda wildcards: [f"{batch}/sentences.gz" for batch in get_batches(wildcards.lang)],
    output:
        f"{DATADIR}/shards/03.split.{{lang}}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


#################################################################
### DOCALIGN ####################################################
rule aggregate_matches:
    """
    Helper rule to implement until=docalign config
    :input: the result of every docalign result
    :output: a file that contains the path to the output of every matches file
    """
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.{DOCALIGN}.06_01.matches"
            for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)
        ],
    output:
        f"{TRANSIENT}/06_01.docalign.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


# MT ############################################################
rule custom_translate:
    """
    Translate source documents into target language (according to translationDirection)
    :input: gz-compressed file with a base64-encoded document per line
        the documents are the output of sentence splitting
    :output: gz-compressed file with a base64-encoded translated document per line
        output must have the same number of lines as the input (i.e. same number of docs)
        each translated document must have the same number of lines as the source
    """
    input:
        source=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz",
    output:
        f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences_{TRG_LANG}.gz",
    threads: THREADS["translate"]
    resources:
        job_translate=JOB_THREADS["translate"],
    shell:
        """
        mkdir -p {TMPDIR}
        initial_nolines=$(zcat "{input.source}" | base64 -d | wc -l)

        parallel_cmd=$([[ {threads} -gt 1 ]] && echo "parallel --gnu --halt 2 --pipe --j {threads} -k" || echo "")

        zcat {input.source} \\
            | {PROFILING} b64filter cache ${{parallel_cmd}} {MT_COMMAND} \\
            | pigz -c > "{output}"

        n_after=$(zcat "{output}" | base64 -d | wc -l)

        if [ $initial_nolines -ne $n_after ]; then
            >&2 echo "ERROR: lines count differs: source $initial_nolines, target $n_after"
            false # Trigger exit
        fi
        """


rule aggregate_translate:
    """
    Helper rule to implement until=translate config
    :input: the result of every translate rule
    :output: a file that contains the path to the output of every translate rule per lang
    """
    input:
        lambda wildcards: [f"{batch}/sentences_{TRG_LANG}.gz" for batch in get_batches(SRC_LANG)],
    output:
        f"{DATADIR}/shards/04.translate.{SRC_LANG}2{TRG_LANG}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


rule tokenise_translated:
    """
    Tokenise the output of translation rule to feed into mt-docalign
    :input: gz-compressed file with a base64-encoded document per line
        the documents are the output of translation
    :output: gz-compressed file with a base64-encoded translated and tokenised document per line
        output must have the same number of lines as the input (i.e. same number of docs)
        each tokenised document must have the same number of lines as the source
    """
    input:
        rules.custom_translate.output,
    output:
        f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/tokenised_{TRG_LANG}.gz",
    params:
        tokeniser=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, TRG_LANG), '--word-tokenizer "{}"'),
        lemmatizer=lambda wildcards: apply_format(get_lang_or_default(MORPHTOKS, TRG_LANG), '--morph-analyser "{}"'),
    threads: THREADS["tokenise"]
    resources:
        job_tokenise=JOB_THREADS["tokenise"],
    shell:
        """
        parallel_cmd=$([[ {threads} -gt 1 ]] && echo "parallel --gnu --halt 2 --pipe --j {threads} -k" || echo "")

        zcat {input} \\
            | {PROFILING} ${{parallel_cmd}} python3 {WORKFLOW}/bitextor_tokenize.py \\
                {params.tokeniser} {params.lemmatizer} \\
                --langcode {TRG_LANG} \\
            | pigz -c > {output}
        """


rule tokenise:
    """
    Tokenise the target side documents to feed into mt-docalign or dic-docalign
    :input: gz-compressed file with a base64-encoded document per line
        the documents are the output of sentence splitting
    :output: gz-compressed file with a base64-encoded tokenised document per line
        output must have the same number of lines as the input (i.e. same number of docs)
        each tokenised document must have the same number of lines as the source
    """
    input:
        f"{DATADIR}/shards/{{lang}}/{{shard}}/{{trg_batch}}/sentences.gz",
    output:
        f"{DATADIR}/shards/{{lang}}/{{shard}}/{{trg_batch}}/tokenised.gz",
    params:
        tokeniser=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, wildcards.lang), '--word-tokenizer "{}"'),
        lemmatizer=lambda wildcards: apply_format(get_lang_or_default(MORPHTOKS, wildcards.lang), '--morph-analyser "{}"'),
    threads: THREADS["tokenise"]
    resources:
        job_tokenise=JOB_THREADS["tokenise"],
    shell:
        """
        parallel_cmd=$([[ {threads} -gt 1 ]] && echo "parallel --gnu --halt 2 --pipe --j {threads} -k" || echo "")

        zcat {input} \\
            | {PROFILING} ${{parallel_cmd}} python3 {WORKFLOW}/bitextor_tokenize.py \\
                {params.tokeniser} {params.lemmatizer} \\
                --langcode {wildcards.lang} \\
            | pigz -c > {output}
        """

rule aggregate_tokenise:
    """
    Helper rule to implement until=tokenise_trg config
    :input: the result of every tokenise rule
    :output: a file that contains the path to the output of every tokenise rule
    """
    input:
        lambda wildcards: [f"{batch}/tokenised.gz" for batch in get_batches(wildcards.lang)],
    output:
        f"{DATADIR}/shards/05.tokenise.{{lang}}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


rule aggregate_translate_tokenise_source:
    """
    Helper rule to implement until=tokenise_src config
    :input: the result of every tokenise_translated rule
    :output: a file that contains the path to the output of every tokenise_translated rule
    """
    input:
        lambda wildcards: [f"{batch}/tokenised_{TRG_LANG}.gz" for batch in get_batches(SRC_LANG)],
    output:
        f"{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


rule mt_matches:
    """
    Align documents using mt document aligner
    :input.l1: gz-compressed file with a base64-encoded document per line
        source language document translated into target language document, and tokenised
    :input.l2: gz-compressed file with a base64-encoded document per line
        tokenised target language documents
    :output: indices file
        plain text file with 3 tab separated columns: mt_doc_aligner_score, idx_translated, idx_trg
            idx_translated is the number of the aligned document in source language
            idx_trg is the number of the aligned document in target language
    """
    input:
        l1=rules.tokenise_translated.output,
        l2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/tokenised.gz",
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.externalMT.06_01.matches",
    params:
        folder=lambda wildcards, output: os.path.dirname(output[0]),
    threads: THREADS["docalign"]
    resources:
        job_docalign=JOB_THREADS["docalign"],
    shell:
        """
        mkdir -p {params.folder}

        {PROFILING} docalign {input.l1} {input.l2} --threshold {DOC_THRESHOLD} -j {threads} > {output}
        """


doc_emb_matches_gpu_token = f"{GPU_TOKENS_DIR}/doc_emb_matches_{{shard}}_{{src_batch}}_{{trg_batch}}"

rule doc_emb_matches:
    """
    Align documents using neural-document-aligner
    :input.plain1: gz-compressed file with a base64-encoded document per line
        sentence-split source documents
    :input.plain2: gz-compressed file with a base64-encoded document per line
        sentence-split target documents
    :output: indices file
        plain text file with 3 tab separated columns: src_idx, trg_idx, nda_score
            src_idx is the number of the aligned document in source language
            trg_idx is the number of the aligned document in target language
    """
    input:
        plain1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz",
        plain2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/sentences.gz",
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.NDA.06_01.matches",
    params:
        folder=f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{{shard}}/nda/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}",
        src_emb_name="emb.src",
        trg_emb_name="emb.trg",
        model=apply_format(EMBEDDINGS_MODEL, "--model {}"),
        gpu_ofile_token=temp(doc_emb_matches_gpu_token),
        batch_size=apply_format(NEURAL_TOOLS_BATCH_SIZE["NDA"], "--embeddings-batch-size {}"),
    threads: THREADS["docalign"]
    resources:
        job_docalign=JOB_THREADS["docalign"],
    run:
        gpu_cuda_device = allocate_cuda_visible_device(PERSISTENT_STORAGE, params.gpu_ofile_token, max_devices=JOB_THREADS["docalign"])

        shell(
            """
            mkdir -p {params.folder}
            mkdir -p "$(dirname {params.gpu_ofile_token})"

            [[ "{gpu_cuda_device}" -ge "0" ]] && \\
            export CUDA_VISIBLE_DEVICES="{gpu_cuda_device}"

            cat <(zcat {input.plain1} | sed "s:$:	-	src:") \\
                <(zcat {input.plain2} | sed "s:$:	-	trg:") \\
                | {PROFILING} neural-document-aligner - {params.folder}/{params.src_emb_name} \\
                    {params.folder}/{params.trg_emb_name} {params.model} \\
                    --docalign-strategy 'faiss' --weights-strategy 0 \\
                    --merging-strategy 3 --results-strategy 0 \\
                    --emb-optimization-strategy 2 --gen-emb-optimization-strategy 2 \\
                    --output-with-idxs --paths-to-docs-are-base64-values \\
                    --threshold {DOC_THRESHOLD} --logging-level 30 {params.batch_size} > {output}

            touch "{params.gpu_ofile_token}"
            """
        )


### DOCUMENTS OUTPUT ####################################################
rule make_documents_output_file:
    input:
        indices=f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.{DOCALIGN}.06_01.matches",
        plain1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/{TEXT_FILE}",
        plain2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/{TEXT_FILE}",
        url1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/url.gz",
        url2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/url.gz",
    output:
        document_pairs=temp(f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}/tempDocumentsOutput.gz"),
    params:
        c1="src_index" if DOCALIGN == "DIC" else "src_idx" if DOCALIGN == "NDA" else "idx_translated",
        c2="trg_index" if DOCALIGN == "DIC" else "trg_idx" if DOCALIGN == "NDA" else "idx_trg",
    shell:
        """
        header="src_url\ttrg_url\tsrc_doc\ttrg_doc"

        python3 {WORKFLOW}/utils/cut_header.py -f {params.c1},{params.c2} --input {input.indices} \
            | tail -n +2 \
            | docjoin \
                -l {input.url1} -r {input.url2} \
                -l {input.plain1} -r {input.plain2} \
            | cat <(echo "$header") - \
            | pigz -c > {output}
        """

### SEGALIGN ####################################################
rule aggregate_segalign:
    """
    Helper rule to implement until=segalign config
    :input: the result of every segalin result
    :output: a file that contains the path to every segalign result
    """
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.{SEGALIGN}.06_02.segalign.gz"
            for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)
        ],
    output:
        f"{TRANSIENT}/06_02.segalign.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


# BLEUALIGN #####################################################
rule bleualign:
    """
    Use bleualign to align sentences withing the matched documents
    :input.indices: output of mt docalign (columns are "mt_doc_aligner_score idx_translated idx_trg")
    :input.plain1: gz-compressed file with a base64-encoded document per line
        sentence-split source documents
    :input.plain2: gz-compressed file with a base64-encoded document per line
        sentence-split target documents
    :input.url1: gz-compressed file with a URL per line for source (source of the corresponding documents)
    :input.url2: gz-compressed file with a URL per line for target (source of the corresponding documents)
    :input.translated1: gz-compressed file with a base64-encoded document per line
        source documents translated into target language
    :output: aligned sentences
        gz-compressed file with 5 tab-separated columns: url1,url2,sentence1,sentence2,bleualign_score
    """
    input:
        indices=rules.mt_matches.output,
        plain1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz",
        plain2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/sentences.gz",
        url1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/url.gz",
        url2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/url.gz",
        translated1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences_{TRG_LANG}.gz",
    params:
        folder=lambda wildcards, output: os.path.dirname(output[0]),
        deferred="--print-sent-hash" if DEFERRED else '',
        metadata=apply_format(PROPAGATE_METADATA_HEADERS, "--metadata-header-fields {}"),
        src_metadata=f"-l {DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/metadata.gz" if PROPAGATE_METADATA_FROM_TEXT else '',
        trg_metadata=f"-r {DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/metadata.gz" if PROPAGATE_METADATA_FROM_TEXT else '',
    # in segalign rule output columns are reordered (or not) in accordance with translationDirection
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.bleualign.06_02.segalign.gz",
    threads: THREADS["segalign"]
    resources:
        job_segalign=JOB_THREADS["segalign"],
    shell:
        """
        mkdir -p {params.folder}

        parallel_cmd=$([[ {threads} -gt 1 ]] && echo "parallel --gnu --halt 2 --pipe --j {threads} -l 1 --group --header 1" || echo "")
        header="src_url\ttrg_url\tsrc_text\ttrg_text\tsrc_translated"

        [[ ! -z "{params.metadata}" ]] && header=$(echo "${{header}}\tsrc_metadata\ttrg_metadata")

        python3 {WORKFLOW}/utils/cut_header.py -f idx_translated,idx_trg --input {input.indices} \\
            | tail -n +2 \\
            | docjoin \\
                -l {input.url1} -r {input.url2} \\
                -l {input.plain1} -r {input.plain2} \\
                -l {input.translated1} {params.src_metadata} {params.trg_metadata} \\
            | cat <(echo "$header") - \\
            | {PROFILING} ${{parallel_cmd}} bleualign_cpp {params.deferred} \\
                --bleu-threshold {SEGALIGN_THRESHOLD} {params.metadata} \\
            | pigz -c > {output}
        """


# VECALIGN #####################################################
vecalign_gpu_token = f"{GPU_TOKENS_DIR}/vecalign_{{shard}}_{{src_batch}}_{{trg_batch}}"

rule vecalign:
    """
    Use vecalign to align sentences withing the matched documents
    :input.indices: output of embeddings docalign (columns are "emb_doc_aligner_score src_index trg_index")
    :input.plain1: gz-compressed file with a base64-encoded document per line
        sentence-split source documents
    :input.plain2: gz-compressed file with a base64-encoded document per line
        sentence-split target documents
    :input.url1: gz-compressed file with a URL per line for source (source of the corresponding documents)
    :input.url2: gz-compressed file with a URL per line for target (source of the corresponding documents)
    :output: aligned sentences
        gz-compressed file with 5 tab-separated columns: url1,url2,sentence1,sentence2,vecalign_score
    """
    input:
        indices=rules.doc_emb_matches.output,
        plain1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz",
        plain2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/sentences.gz",
        url1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/url.gz",
        url2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/url.gz",
    params:
        folder=f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{{shard}}/vecalign/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}",
        src_emb_storage=f"{rules.doc_emb_matches.params.folder}/{rules.doc_emb_matches.params.src_emb_name}",
        trg_emb_storage=f"{rules.doc_emb_matches.params.folder}/{rules.doc_emb_matches.params.trg_emb_name}",
        deferred=f"--print-sent-hash \"{DEFERRED_CMD}\"" if DEFERRED else '',
        model=apply_format(EMBEDDINGS_MODEL, "--model {}"),
        gpu_ofile_token=temp(vecalign_gpu_token),
        metadata=apply_format(PROPAGATE_METADATA_HEADERS, "--metadata-header-fields {}"),
        src_metadata=f"-l {DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/metadata.gz" if PROPAGATE_METADATA_FROM_TEXT else '',
        trg_metadata=f"-r {DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/metadata.gz" if PROPAGATE_METADATA_FROM_TEXT else '',
        c1="src_index" if DOCALIGN == "DIC" else "src_idx" if DOCALIGN == "NDA" else "idx_translated",
        c2="trg_index" if DOCALIGN == "DIC" else "trg_idx" if DOCALIGN == "NDA" else "idx_trg",
        batch_size=apply_format(NEURAL_TOOLS_BATCH_SIZE["vecalign"], "--embeddings-batch-size {}"),
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.vecalign.06_02.segalign.gz",
    threads: THREADS["segalign"]
    resources:
        job_segalign=JOB_THREADS["segalign"],
    run:
        gpu_cuda_device = allocate_cuda_visible_device(PERSISTENT_STORAGE, params.gpu_ofile_token, max_devices=JOB_THREADS["segalign"])

        shell(
            """
            mkdir -p "{params.folder}"
            mkdir -p "$(dirname {params.gpu_ofile_token})"

            [[ "{gpu_cuda_device}" -ge "0" ]] && \\
            export CUDA_VISIBLE_DEVICES="{gpu_cuda_device}"

            header="src_url\ttrg_url\tsrc_text\ttrg_text"

            [[ ! -z "{params.metadata}" ]] && header=$(echo "${{header}}\tsrc_metadata\ttrg_metadata")

            python3 {WORKFLOW}/utils/cut_header.py -f {params.c1},{params.c2} --input {input.indices} \\
                | tail -n +2 \\
                | docjoin \\
                    -l {input.url1} -r {input.url2} \\
                    -l {input.plain1} -r {input.plain2} {params.src_metadata} {params.trg_metadata} \\
                | cat <(echo "$header") - \\
                | {PROFILING} python3 "{WORKFLOW}/bitextor_nda_vecalign.py" - \\
                    --threshold {SEGALIGN_THRESHOLD} {params.deferred} {params.model} {params.metadata} \\
                    --tmp-dir "{params.folder}" {params.batch_size} \\
                    --embedding-src-storage-input "{input.plain1}" --embedding-trg-storage-input "{input.plain2}" \\
                    --embedding-src-storage-path "{params.src_emb_storage}" --embedding-trg-storage-path "{params.trg_emb_storage}" \\
                    --src-embeddings-optimization-strategy 2 --trg-embeddings-optimization-strategy 2 \\
                    --src-storage-embeddings-optimization-strategy 2 --trg-storage-embeddings-optimization-strategy 2 \\
                    --embedding-src-storage-not-uniq --embedding-trg-storage-not-uniq \\
                    --vecalign-num-overlaps 2 --vecalign-alignment-max-size 2 \\
                | pigz -c > "{output}"

            touch "{params.gpu_ofile_token}"
            """
        )


### FILTERING AND CLEANING ######################################

split_input_filename = "bleualign.06_02.segalign"

if SEGALIGN == "hunalign":
    split_input_filename = "hunalign.06_02.segalign"
elif SEGALIGN == "vecalign":
    split_input_filename = "vecalign.06_02.segalign"

# split segalign results into balanced chunks
checkpoint split_segalign:
    """
    Join all of the output of segalign and split them again into even chunks this time
        each batch is a gz-compressed file that has the same format as segalign output
        if translatationDirection is different from lang1->lang2, in this rule the columns are switched
    :input: the result of every segalin result
    :output: a plain text files that contains the list of every postprocessing batch generated
        (i.e. each line has a path to a postprocessing batch file without the extension)
    """
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.{split_input_filename}.gz"
            for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)
        ],
    output:
        batches=f"{TRANSIENT}/{LANG1}_{LANG2}/{LANG1}_{LANG2}.postprocessing_batches",
    params:
        size=BATCHES,  # use same parameter as for shards
        folder=f"{TRANSIENT}/{LANG1}_{LANG2}/{split_input_filename}",
    run:
        # We need to make this check in order to avoid the pipeline to break in the case that no input was received (e.g. no common shards)
        # It is needed to run this piece of code directly in Python and not in bash because Snakemake attempts to replace {input[0]} before
        #  running the script, so it fails even before we are able to check if the input is empty in bash script
        if len(input) == 0:
            raise Exception("Didn't get any input file")
        else:
            shell(
                """
                mkdir -p {params.folder}
                rm -f {params.folder}/* # remove anything that might be left after a previous run
                header=$(head -1 <(zcat {input[0]}) | tr -d '\n')

                echo {input} \\
                    | tr ' ' '\n' \\
                    | xargs -I[] bash -c 'zcat "[]" | tail -n +2' \\
                    | cat <(echo "$header") - \\
                    | python3 {WORKFLOW}/bitextor_split_segalign.py -f src_text,trg_text -s {params.size} --gzip -o "{params.folder}/"
                if [ -z "$(ls -A {params.folder})" ]; then
                    cat < /dev/null > {output.batches}
                else
                    ls {params.folder}/* | sed 's/[.]gz$//g' > {output.batches}
                fi
                """
            )


rule bifixer:
    """
    Apply bifixer to the segalign output
    :input: a single chunk of segalign output
        gz-compressed, columns are: url1 url2 sent1 sent2 score deferred1 deferred2
        (deferred is optional)
    :output: plain text, marked as temp, same columns as input with two new columns: hash and score
    """
    input:
        segalign=f"{TRANSIENT}/{LANG1}_{LANG2}/{split_input_filename}/{{batch}}.gz",
    output:
        temp(f"{TRANSIENT}/{LANG1}_{LANG2}/07_01.bifixer/{{batch}}.gz"),
    threads: THREADS["bifixer"]
    resources:
        job_bifixer=JOB_THREADS["bifixer"],
    shell:
        """
        parallel_cmd=$([[ {threads} -gt 1 ]] && echo "parallel --gnu --halt 2 --pipe --j {threads} -k --header 1" || echo "")

        zcat {input.segalign} \\
            | {PROFILING} ${{parallel_cmd}} bifixer -q {BIFIXER_AGGRESSIVE_DEDUP} {BIFIXER_IGNORE_SEGMENTATION} \\
                {BIFIXER_DEFERRED_COLS} {BIFIXER_PARAGRAPHS_COLS} --header - - {SRC_LANG} {TRG_LANG} \\
            | pigz -c > {output}
        """


rule aggregate_bifixer:
    """
    Helper rule to implement until=bifixer config
    :input: the result of every bifixer result
    :output: a file that contains the path to every bifixer result
    """
    input:
        lambda wildcards: [f"{TRANSIENT}/{LANG1}_{LANG2}/07_01.bifixer/{batch}.gz" for batch in get_postproc_batches()],
    output:
        f"{TRANSIENT}/07_01.bifixer.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


if BICLEANER_GENERATE_MODEL and BICLEANER_FLAVOUR == "classic":
    rule bicleaner_train_model:
        """
        Train bicleaner model (flavour: classic)
        """
        input:
            corpus_l1=expand("{dataset}.{lang}.gz", dataset=BICLEANER_TRAINING_CORPUS_PREFIX, lang=SRC_LANG),
            corpus_l2=expand("{dataset}.{lang}.gz", dataset=BICLEANER_TRAINING_CORPUS_PREFIX, lang=TRG_LANG),
            # 'optional_input_file' is needed because if 'config["generateDic"] == True and not BICLEANER_GENERATE_MODEL' then
            #   will trigger this rule ("reason: Input files updated by another job") even if the output already exists
            e2f=optional_input_file(f"{DIC}.lex.e2f.gz", BICLEANER_GENERATE_MODEL),
            f2e=optional_input_file(f"{DIC}.lex.f2e.gz", BICLEANER_GENERATE_MODEL),
            vcb1=optional_input_file(f"{mgizaModelDir}/corpus.{SRC_LANG}.filtered.vcb.gz", BICLEANER_GENERATE_MODEL),
            vcb2=optional_input_file(f"{mgizaModelDir}/corpus.{TRG_LANG}.filtered.vcb.gz", BICLEANER_GENERATE_MODEL),
        output:
            model=BICLEANER_MODEL,
        params:
            training_corpus=temp(f"{TMPDIR}/bicleaner_train/training_parallel_corpus.{SRC_LANG}_{TRG_LANG}"),
            classifier=f"{'/'.join(BICLEANER_MODEL.split('/')[:-1])}/{SRC_LANG}-{TRG_LANG}.classifier",
            tokenizer_l1=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, SRC_LANG), '-S "{}"'),
            tokenizer_l2=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, TRG_LANG), '-T "{}"'),
        shell:
            """
            echo "Training corpus: {params.training_corpus}"
            mkdir -p "{TMPDIR}/bicleaner_train"
            paste <(zcat {input.corpus_l1}) <(zcat {input.corpus_l2}) > {params.training_corpus}

            {PROFILING} bicleaner-train {params.training_corpus} {params.tokenizer_l1} {params.tokenizer_l2} --treat_oovs \\
                --normalize_by_length -s {SRC_LANG} -t {TRG_LANG} -d {input.e2f} -D {input.f2e} -f {input.vcb1} \\
                -F {input.vcb2} -c {params.classifier} -m {output.model} --classifier_type random_forest \\
                --seed 71213
            """

if BICLEANER_GENERATE_MODEL and BICLEANER_FLAVOUR == "ai":
    rule bicleaner_train_model:
        """
        Train bicleaner model (flavour: ai)
        """
        input:
            corpus_l1=expand("{dataset}.{lang}.gz", dataset=BICLEANER_TRAINING_CORPUS_PREFIX, lang=SRC_LANG),
            corpus_l2=expand("{dataset}.{lang}.gz", dataset=BICLEANER_TRAINING_CORPUS_PREFIX, lang=TRG_LANG),
            mono_l1=' '.join(expand("{dataset}.{lang}.gz", dataset=BICLEANER_AI_MONO_CORPUS_PREFIX, lang=SRC_LANG)),
            mono_l2=' '.join(expand("{dataset}.{lang}.gz", dataset=BICLEANER_AI_MONO_CORPUS_PREFIX, lang=TRG_LANG)),
            dev_corpus_l1=' '.join(expand("{dataset}.{lang}.gz", dataset=BICLEANER_AI_DEV_CORPUS_PREFIX, lang=SRC_LANG)),
            dev_corpus_l2=' '.join(expand("{dataset}.{lang}.gz", dataset=BICLEANER_AI_DEV_CORPUS_PREFIX, lang=TRG_LANG)),
        output:
            model=BICLEANER_MODEL,
        params:
            training_corpus=temp(f"{TMPDIR}/bicleaner_train/training_parallel_corpus.{SRC_LANG}_{TRG_LANG}"),
            tfreq=temp(f"{TMPDIR}/bicleaner_train/bicleaner-ai/wordfreq_{TRG_LANG}.gz"),
            dev_corpus=temp(f"{TMPDIR}/bicleaner_train/bicleaner-ai/dev_parallel_corpus.{SRC_LANG}_{TRG_LANG}"),
            mono_corpus=temp(f"{TMPDIR}/bicleaner_train/bicleaner-ai/mono_corpus.{SRC_LANG}_{TRG_LANG}"),
            metadata_dir=BICLEANER_MODEL,
            ai_metadata=f"{BICLEANER_MODEL}/metadata.yaml",
            model=BICLEANER_MODEL,
            tokenizer_l1=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, SRC_LANG), '-S "{}"'),
            tokenizer_l2=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, TRG_LANG), '-T "{}"'),
        shell:
            """
            echo "Training corpus: {params.training_corpus}"
            mkdir -p "{TMPDIR}/bicleaner_train/bicleaner-ai"
            paste <(zcat {input.corpus_l1}) <(zcat {input.corpus_l2}) > {params.training_corpus}

            # Target lang frequency
            echo "Frequency file (lang={TRG_LANG}): {params.tfreq}"
            zcat {input.mono_l2} \\
                | {WORDTOK2} \\
                | awk '{{print tolower($0)}}' \\
                | tr ' ' '\n' \\
                | LC_ALL=C sort -T {TMPDIR} | uniq -c \\
                | LC_ALL=C sort -T {TMPDIR} -nr \\
                | grep -v "[[:space:]]*1" \\
                | pigz -c > {params.tfreq}

            # Dev corpus
            echo "Dev corpus: {params.dev_corpus}"
            paste <(zcat {input.dev_corpus_l1}) <(zcat {input.dev_corpus_l2}) > {params.dev_corpus}

            # Mono
            echo "Mono corpus: {params.mono_corpus}"

            mono_l1_nolines=$(zcat {input.mono_l1} | wc -l)
            mono_l2_nolines=$(zcat {input.mono_l2} | wc -l)
            mono_min_nolines=$([[ $mono_l1_nolines -gt $mono_l2_nolines ]] && echo "$mono_l2_nolines" || echo "$mono_l1_nolines")

            head -n $mono_min_nolines <(zcat {input.mono_l1}) > {params.mono_corpus}
            head -n $mono_min_nolines <(zcat {input.mono_l2}) >> {params.mono_corpus}

            # Train
            {PROFILING} bicleaner-ai-train {params.tokenizer_l1} {params.tokenizer_l2} -s {SRC_LANG} -t {TRG_LANG} \\
                -m {params.metadata_dir} --classifier_type dec_attention --parallel_train {params.training_corpus} \\
                -F {params.tfreq} --parallel_valid {params.dev_corpus} --mono_train {params.mono_corpus} --seed 71213 && \\
                    mv {params.metadata_dir} {output.model}
            """


bicleaner_gpu_token = f"{GPU_TOKENS_DIR}/bicleaner_{{batch}}"

# TODO we should split bicleaner and bicleaner-hardrules in different rules in order to improve the GPU usage
#  because the block processed by bicleaner will discard all the sentences discarded by bicleaner-hardrules
rule bicleaner:
    """
    Compute bicleaner scores of the aligned sentence pairs
    :input.bifixer: either the output of bifixer rule, or a single chunk of segalign output if bifixer is disabled
    :input.model: bicleaner model, either provided by the user or generated by train_bicleaer
    :output: gz-compressed, same columns as input with one new column: score
    """
    input:
        bifixer=rules.bifixer.output if BIFIXER else rules.bifixer.input.segalign,
        model=BICLEANER_MODEL,
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/07_02.bicleaner/{{batch}}.gz",
    params:
        metadata_file=f"{BICLEANER_MODEL}/metadata.yaml" if BICLEANER_FLAVOUR == "ai" else BICLEANER_MODEL,
        tokenizer_l1=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, SRC_LANG), '-S "{}"'),
        tokenizer_l2=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, TRG_LANG), '-T "{}"'),
        gpu_ofile_token=temp(bicleaner_gpu_token),
        batch_size=apply_format(NEURAL_TOOLS_BATCH_SIZE["bicleaner"] if BICLEANER_FLAVOUR == "ai" else None, "--batch_size {}"),
    threads: THREADS["bicleaner"]
    resources:
        job_bicleaner=JOB_THREADS["bicleaner"],
    run:
        gpu_cuda_device = allocate_cuda_visible_device(PERSISTENT_STORAGE, params.gpu_ofile_token, max_devices=JOB_THREADS["bicleaner"])

        shell(
            """
            mkdir -p "$(dirname {params.gpu_ofile_token})"

            [[ "{gpu_cuda_device}" -ge "0" ]] && \\
            export CUDA_VISIBLE_DEVICES="{gpu_cuda_device}"

            slang=$(egrep "source_lang" {params.metadata_file} | cut -d " " -f 2)
            text_fields=$(head -1 <(zcat {input.bifixer}) \\
                | python3 {WORKFLOW}/utils/cut_header.py -f src_text,trg_text --only-print-idxs --respect-original-sorting \\
                | tr -d '\n')

            bicleaner_src_lang=$(cat "{params.metadata_file}" | grep "^source_lang: " | awk '{{print $2}}')
            bicleaner_trg_lang=$(cat "{params.metadata_file}" | grep "^target_lang: " | awk '{{print $2}}')
            bicleaner_text_fields="--scol src_text --tcol trg_text"

            if [[ -z "$bicleaner_src_lang" ]] || [[ -z "$bicleaner_trg_lang" ]]; then
                >&2 echo "ERROR: could not gather the languages from the bicleaner configuration file"

                false # Trigger exit (it will work since bash is executed in strict mode)
            else
                if [[ "$bicleaner_src_lang" == "{SRC_LANG}" ]] && [[ "$bicleaner_trg_lang" == "{TRG_LANG}" ]]; then
                    # Ok
                    true
                elif [[ "$bicleaner_src_lang" == "{TRG_LANG}" ]] && [[ "$bicleaner_trg_lang" == "{SRC_LANG}" ]]; then
                    # Change src and trg
                    bicleaner_text_fields="--scol trg_text --tcol src_text"
                else
                    >&2 echo "WARNING: the provided Bicleaner model seems to don't expect the provided languages:" \\
                             "{SRC_LANG}-{TRG_LANG} was provided, ${{bicleaner_src_lang}}-${{bicleaner_trg_lang}}" \\
                             "was expected by Bicleaner ({BICLEANER_FLAVOUR})"
                fi
            fi

            # We apply --score_only + paste because not all the data are dependant of the text fields!
            #  Check out: https://github.com/bitextor/bicleaner-ai/issues/23
            zcat {input.bifixer} \\
                | {PROFILING} cache -k $text_fields {BICLEANER_BIN} $bicleaner_text_fields \\
                    --header {params.tokenizer_l1} {params.tokenizer_l2} \\
                    --score_only -q -p {threads} {params.batch_size} - - {input.model} \\
                | paste <(zcat {input.bifixer}) - \\
                | pigz -c > {output}

            touch "{params.gpu_ofile_token}"
            """
        )


rule aggregate_bicleaner:
    """
    Helper rule to implement until=bicleaner config
    :input: the result of every biclenaer result
    :output: a file that contains the path to every biclenaer result
    """
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/07_02.bicleaner/{batch}.gz" for batch in get_postproc_batches()
        ],
    output:
        f"{TRANSIENT}/07_02.bicleaner.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


filter_input = rules.bicleaner.output if BICLEANER else rules.bicleaner.input.bifixer


rule pre_filter_sort_flags:
    input:
        filter_input,
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{{batch}}.sort_flags"
    run:
        header = None

        with open_xz_or_gzip_or_plain(str(input)) as f:
            for header in f:
                break

        header = header.strip().split('\t')
        src_text_idx = header.index('src_text') + 1 # sort counts from 1, not 0
        trg_text_idx = header.index('trg_text') + 1
        src_url_idx = header.index('src_url') + 1
        trg_url_idx = header.index('trg_url') + 1

        sort_flags = f"-k{src_text_idx},{src_text_idx} -k{trg_text_idx},{trg_text_idx} " \
                     f"-k{src_url_idx},{src_url_idx} -k{trg_url_idx},{trg_url_idx}"

        if "bifixer_hash" in header:
            i = header.index("bifixer_hash") + 1
            j = ""

            if "bifixer_score" in header:
                # We sort using bifixer score as second criteria and not bicleaner score
                #  because in the case of duplicated we prefer bifixer, which uses
                #  rules and indicates which aligned content is more "correct".
                j = header.index('bifixer_score') + 1
                j = f"-k{j},{j}nr "

            sort_flags = f"-k{i},{i} {j}" + sort_flags

        with open(output[0], 'w') as f:
            f.write(f"{sort_flags}\n")


# We sort in the 'filter' rule each batch, and later we sort in the 'sents' rule again but blocks of sorted batches,
#  so we can use '--merge' for making the sorting process more efficient (because of this, we need that the output
#  files are not compressed in the 'filter' rule)
# We could sort directly on deduped rule, but this might be very slow
rule filter:
    """
    Filter by biclenaer threshold (if applicable), compute ELRC metrics (if applicable), sort by sentence pair or bifixer hash
    :input: either the output of bicleaner (if enabled), or the output of the previous step (i.e. what would be the input ofbicleaner if it was enabled)
    :output: plain-text file, marked as temp, the senteces are sorted by duplicates
        remove sentences below biclenaer threshold (if applicable)
        add new columns for ELRC if applicable: length_ration, num_tokens_src, num_tokens_trg
        the header fields are stored in output.header instead of the file itself in order to simplify the 'sents' rule processing
    """
    input:
        f=filter_input,
        sort_flags=rules.pre_filter_sort_flags.output,
    output:
        out=temp(f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{{batch}}"),
        header=temp(f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{{batch}}.header"),
    params:
        bicleaner_header="bicleaner_score" if BICLEANER_FLAVOUR == "classic" else "bicleaner_ai_score",
    threads: THREADS["filter"]
    shell:
        """
        sort_flags="$(cat {input.sort_flags} | tr -d '\\n')"
        FILTER_BICLEANER_CMD="\\
            [[ '{BICLEANER}' == 'True' ]] \\
                && ({PROFILING} python3 {WORKFLOW}/bitextor_filter_bicleaner.py \\
                    --threshold {BICLEANER_THRESHOLD} --header-field {params.bicleaner_header}; true) \\
                || cat"
        ELRC_CMD="\\
            [[ '{ELRC}' == 'True' ]] \\
                && ({PROFILING} python3 {WORKFLOW}/bitextor_elrc_filtering.py -s; true) \\
                || cat"

        zcat {input.f} \\
            | eval "$FILTER_BICLEANER_CMD" \\
            | eval "$ELRC_CMD" \\
            > "{output.out}"

        header=$(head -1 "{output.out}" | tr -d '\\n')

        # Remove header and sort (in-place) in the output file
        sed -i '1 d' "{output.out}"
        LC_ALL=C sort -t $'\\t' $sort_flags --parallel {threads} \\
            --compress-program=gzip -T {TMPDIR} -o "{output.out}" "{output.out}"

        # Add header
        echo "$header" > "{output.header}"
        """


rule aggregate_filter:
    """
    Helper rule to implement until=filter config
    :input: the result of every filter result
    :output: a file that contains the path to every filter result
    """
    input:
        lambda wildcards: [f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{batch}" for batch in get_postproc_batches()],
    output:
        f"{TRANSIENT}/07_03.filter.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


raw_input_filename = rules.filter.input[0].split("/")[-2] # {bleualign,hunalign,vecalign}.06_02.segalign / 07_01.bifixer / 07_02.bicleaner


rule raw:
    """
    Create {lang1}-{lang2}.raw.gz file by concatenating the output chunks of the last step before filtering
        (may be segalign, bifixer or bicleaner, depending on what's enabled in the config)
    :input: the output of the last step
    :output.corpus: the concatenated inputs, columns are the same as the input
    :output:stats: the corresponding stats file in plain text
    """
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/{raw_input_filename}/{batch}.gz" for batch in get_postproc_batches()
        ],
    output:
        corpus=f"{PERMANENT}/{LANG1}-{LANG2}.raw.gz",
        stats=f"{PERMANENT}/{LANG1}-{LANG2}.stats.raw",
    shell:
        """
        header=$(head -1 <(zcat {input[0]}) | tr -d '\n')

        # Remove header from all input files and only print header once
        echo {input} \\
            | tr ' ' '\n' \\
            | xargs -I[] bash -c 'zcat "[]" | tail -n +2' \\
            | cat <(echo "$header") - \\
            | pigz -c > {output.corpus}

        # Stats
        WC1=$(zcat {output.corpus} | python3 {WORKFLOW}/utils/cut_header.py -f src_text | tail -n +2 | wc -wl | tr -s ' ')
        WC2=$(zcat {output.corpus} | python3 {WORKFLOW}/utils/cut_header.py -f trg_text | tail -n +2 | wc -w)

        echo "{LANG1}-{LANG2} raw" > {output.stats}
        echo "File size: $(du -h {output.corpus} | cut -f 1)" >> {output.stats}
        echo "Sentence pairs: $(echo $WC1 | cut -d ' ' -f 1)" >> {output.stats}
        echo "{SRC_LANG} words: $(echo $WC1 | cut -d ' ' -f 2)" >> {output.stats}
        echo "{TRG_LANG} words: $WC2" >> {output.stats}
        """


rule sents:
    """
    Create {lang1}-{lang2}.sent.gz by concatenated and merge-sorting the outputs of filters rule
    :input: the outputs of filter step
    :output: the concatenated inputs, sorted, same columns as input
    """
    input:
        filtered=lambda wildcards: [f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{batch}" for batch in get_postproc_batches()],
        header=lambda wildcards: [f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{batch}.header" for batch in get_postproc_batches()],
        sort_flags=[f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{batch}.sort_flags" for batch in get_postproc_batches()],
    output:
        corpus=f"{PERMANENT}/{LANG1}-{LANG2}.sent.gz",
        stats=f"{PERMANENT}/{LANG1}-{LANG2}.stats.sent",
    threads: THREADS["sents"]
    shell:
        """
        num_sort_flags=$(cat {input.sort_flags} | sort -u | wc -l)
        num_header=$(cat {input.header} | sort -u | wc -l)

        if [[ "$num_header" != "1" ]]; then
            >&2 echo "ERROR: expected lines in input.header is 1, but got $num_header"
            false # Trigger exit
        fi
        if [[ "$num_sort_flags" != "1" ]]; then
            >&2 echo "ERROR: expected lines in input.sort_flags is 1, but got $num_sort_flags"
            false # Trigger exit
        fi

        sort_flags=$(cat {input.sort_flags[0]} | tr -d '\\n')
        header=$(cat {input.header[0]} | tr -d '\\n')

        LC_ALL=C sort -t $'\\t' $sort_flags --parallel {threads} --compress-program=gzip \\
            -T {TMPDIR} --merge \\
            {input.filtered} \\
            | cat <(echo "$header") - \\
            | pigz -c > {output.corpus}

        # Stats
        WC1=$(zcat {output.corpus} | python3 {WORKFLOW}/utils/cut_header.py -f src_text | tail -n +2 | wc -wl | tr -s ' ')
        WC2=$(zcat {output.corpus} | python3 {WORKFLOW}/utils/cut_header.py -f trg_text | tail -n +2 | wc -w)

        echo "{LANG1}-{LANG2} filtered" > {output.stats}
        echo "File size: $(du -h {output.corpus} | cut -f 1)" >> {output.stats}
        echo "Sentence pairs: $(echo $WC1 | cut -d ' ' -f 1)" >> {output.stats}
        echo "{SRC_LANG} words: $(echo $WC1 | cut -d ' ' -f 2)" >> {output.stats}
        echo "{TRG_LANG} words: $WC2" >> {output.stats}
        """


rule documents:
    input:
        documents=lambda wildcards: [
            f"{TMPDIR}/{LANG1}_{LANG2}/shards/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}/tempDocumentsOutput.gz"
            for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)
        ],
        sents=rules.sents.output.corpus,
    output:
        f"{PERMANENT}/{LANG1}-{LANG2}.documents.gz",
    shell:
        """
	sent_file={input.sents}
	document_file={input.documents}
	header=$(head -1 <(zcat {input.documents[0]}) | tr -d '\\n')
        awk -F $'\t' 'NR == FNR {{ a[$0]; next}} $1"\t"$2 in a{{print $0}}' \\
		<(zcat $sent_file | tail -n +2 | cut -f 1,2) \\
		<(zcat $document_file | tail -n +2) | \\
		cat <(echo "$header") - | pigz -c > {output}
        """



rule tmx:
    """
    Crate {lang1}-{lang2}.not-deduped.tmx.gz output
    :input: {lang1}-{lang2}.sent.gz, i.e. corpus after filtering & sorting
    :output: corpus after filtering in TMX format
    """
    input:
        rules.sents.output.corpus,
    output:
        f"{PERMANENT}/{LANG1}-{LANG2}.not-deduped.tmx.gz",
    shell:
        """
        zcat {input} \\
            | {PROFILING} python3 {WORKFLOW}/bitextor_build_TMX.py --no-delete-seg --lang1 {SRC_LANG} --lang2 {TRG_LANG} \\
            | pigz -c > {output}
        """


rule deduped_tmx:
    """
    Create deduped corpus, TMX and txt versions
    :input: {lang1}-{lang2}.sent.gz, i.e. corpus after filtering & sorting
    :output.tmx: TMX deduplicated corpus
    :output.txt: txt dedpulicated corpus
    :output.stats: the corresponding stats file (calculated from dedup txt version)
    """
    input:
        rules.sents.output.corpus,
    output:
        tmx=f"{PERMANENT}/{LANG1}-{LANG2}.deduped.tmx.gz",
        txt=f"{PERMANENT}/{LANG1}-{LANG2}.deduped.txt.gz",
        stats=f"{PERMANENT}/{LANG1}-{LANG2}.stats.deduped",
    params:
        dedup_fields="bifixer_hash" if BIFIXER else "src_text,trg_text"
    shell:
        """
        zcat {input} \
            | {PROFILING} python3 {WORKFLOW}/bitextor_build_TMX.py --no-delete-seg --lang1 {SRC_LANG} --lang2 {TRG_LANG} \\
                --dedup {params.dedup_fields} -f {output.txt} \\
            | pigz -c > {output.tmx}

        # Stats
        WC1=$(zcat {output.txt} | python3 {WORKFLOW}/utils/cut_header.py -f src_text | tail -n +2 | wc -wl | tr -s ' ')
        WC2=$(zcat {output.txt} | python3 {WORKFLOW}/utils/cut_header.py -f trg_text | tail -n +2 | wc -w)

        echo "{LANG1}-{LANG2} deduped txt" > {output.stats}
        echo "File size: $(du -h {output.txt} | cut -f 1)" >> {output.stats}
        echo "Sentence pairs: $(echo $WC1 | cut -d ' ' -f 1)" >> {output.stats}
        echo "{SRC_LANG} words: $(echo $WC1 | cut -d ' ' -f 2)" >> {output.stats}
        echo "{TRG_LANG} words: $WC2" >> {output.stats}
        """


rule roam_tmx:
    """
    Use biroamer to created ROAMed version of the corpus
    :input.tmx: TMX file, both non-deduped and deduped if dedup=true
    :output: ROAMed TMX file, both non-deduped and deduped if dedup=true
    """
    input:
        f"{PERMANENT}/{LANG1}-{LANG2}.{{deduped}}.tmx.gz",
    output:
        f"{PERMANENT}/{LANG1}-{LANG2}.{{deduped}}.roamed.tmx.gz",
    params:
        tokenizer_l1=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, SRC_LANG), '-t "{}"'),
        tokenizer_l2=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, TRG_LANG), '-T "{}"'),
        mix_files=temp(f"{TMPDIR}/biroamer_mix_files.txt"),
    shell:
        """
        mix_files=""
        nolines=$(cat {BIROAMER_MIX_FILES} | wc -l)

        if [[ "$nolines" != "0" ]]; then
            cat {BIROAMER_MIX_FILES} > {params.mix_files}

            mix_files="-m {params.mix_files}"
        fi

        zcat {input} \\
            | {PROFILING} biroamer {params.tokenizer_l1} {params.tokenizer_l2} \\
                {BIROAMER_OMIT} {BIROAMER_ALIGNMENT_CORPUS} $mix_files {SRC_LANG} {TRG_LANG} \\
            | pigz -c > {output}
        """
