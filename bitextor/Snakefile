import os
import sys
import subprocess
import pprint

from bitextor.utils.args import validate_args
from bitextor.utils.common import (
    open_xz_or_gzip_or_plain,
    duration_to_seconds,
    return_dict_value_if_key,
    get_snakemake_execution_mark,
    is_first_snakemake_execution,
    path_exists,
)

valid, config = validate_args(config)

if not valid:
    raise Exception("provided configuration is not valid")

#################################################################
include: "rules/common.smk"


#################################################################
# BASIC PARAMETERS

WORKFLOW = workflow.basedir
DATADIR = config["dataDir"]
TRANSIENT = config["transientDir"]
PERMANENT = config["permanentDir"]
TMPDIR = config["tempDir"]

# Create directories
for f in (DATADIR, TRANSIENT, PERMANENT, TMPDIR):
    try:
        os.makedirs(f)
    except FileExistsError:
        pass

LANGS = set()

# the only scenario where config won't have LANG1 and LANG2 is when the workflow is monolingual,
# and in that case LANG1/LANG2 and SRC_LANG/TRG_LANG won't be used anywhere, so setting them to 'l1' and 'l2' is fine
# (if LANG1/LANG2 and SRC_LANG/TRG_LANG are unset or both empty string, Snakemake will give an error, even if they are not used)
LANG1 = return_dict_value_if_key(config, "lang1", "l1", only_check_key=True)
LANG2 = return_dict_value_if_key(config, "lang2", "l2", only_check_key=True)

if "langs" in config:
    LANGS = set(config["langs"])
if "lang1" in config:
    LANGS.add(LANG1)
if "lang2" in config:
    LANGS.add(LANG2)

PROFILING = return_dict_value_if_key(config, "profiling", "", pos_value="\\time -v")
VERBOSE = config["verbose"]

if VERBOSE and is_first_snakemake_execution(TMPDIR):
    # provided config
    sys.stderr.write("Config: \n")
    pprint.pprint(config, indent=2, stream=sys.stderr, width=100)
    sys.stderr.write('\n')
    # locale
    sys.stderr.write("Locale: \n")
    sys.stderr.write(subprocess.run(["locale"], stdout=subprocess.PIPE).stdout.decode("utf-8"))
    sys.stderr.write('\n')
    # free storage
    sys.stderr.write("Free storage: \n")
    sys.stderr.write(subprocess.run(["df", "-h"], stdout=subprocess.PIPE).stdout.decode("utf-8"))
    sys.stderr.write('\n')

# TODO log variables which might not be defined in the config variable if VERBOSE? (e.g. BIFIXER_AGGRESSIVE_DEDUP)

#################################################################
# CRAWLING
CRAWLTARGET = return_dict_value_if_key(config, "crawler", "")
TLD_CRAWL = return_dict_value_if_key(config, "crawlTLD", [])
CRAWLSIZELIMIT = return_dict_value_if_key(config, "crawlSizeLimit", "", apply_function=str)
CRAWLTIMELIMIT = return_dict_value_if_key(config, "crawlTimeLimit", "", apply_function=str)
CRAWLWAIT = return_dict_value_if_key(config, "crawlWait", "", apply_function=str)
CRAWLFILETYPES = return_dict_value_if_key(config, "crawlFileTypes", [])
CRAWLJOBS = return_dict_value_if_key(config, "crawlerNumThreads", 2, only_check_key=True, apply_function=str)
CRAWLTIMEOUT = return_dict_value_if_key(config, "crawlerConnectionTimeout", 10, only_check_key=True, apply_function=str)
CRAWLDUMPARGS = return_dict_value_if_key(config, "dumpCurrentCrawl", "")
CONTINUECRAWL = return_dict_value_if_key(config, "resumePreviousCrawl", False)
HERITRIXPATH = return_dict_value_if_key(config, "heritrixPath", "")
HERITRIXURL = return_dict_value_if_key(config, "heritrixUrl", "https://localhost:8443", only_check_key=True)
HERITRIXUSER = return_dict_value_if_key(config, "heritrixUser", "admin:admin", only_check_key=True)
CRAWLMAXFOLDERTREEDEPTH = return_dict_value_if_key(config, "crawlMaxFolderTreeDepth", 20, only_check_key=True, apply_function=str)
CRAWLSCOUTSTEPS = return_dict_value_if_key(config, "crawlScoutSteps", 200, only_check_key=True, apply_function=str)

if "crawler" in config:
    if CRAWLTARGET == "linguacrawl":
        # TODO should we change this default value, as we have done with wget, as well?
        CRAWLFILETYPES = ["text/html", "application/pdf"]

USERAGENT = "Mozilla/5.0 (compatible; Bitextor/8 +https://github.com/bitextor/bitextor)"
CRAWLBLACKLISTURL = ['wordpress','blogspot','facebook','google','wikipedia','youtube','perehodi','twitter','instagram']
CRAWLPREFIXFILTER = ['mailto:']

if "crawlerUserAgent" in config:
    USERAGENT = config["crawlerUserAgent"]
if "crawlBlackListURL" in config:
    CRAWLBLACKLISTURL = config["crawlBlackListURL"]
if "crawlPrefixFilter" in config:
    CRAWLPREFIXFILTER = config["crawlPrefixFilter"]

#################################################################
# PREPROCESS
PPROC = "warc2text"
PPROC_FILES = ["text.gz", "url.gz", "mime.gz"]
TEXT_FILE = "text.gz"
HTML_FILE = ""

if "writeHTML" in config and config["writeHTML"]:
    HTML_FILE = "html.gz"
    PPROC_FILES.append("html.gz")
if "preprocessor" in config:
    if config["preprocessor"] == "warc2preprocess":
        PPROC = "w2p"
        PPROC_FILES = ["plain_text.gz", "url.gz", "mime.gz", "normalized_html.gz", "deboilerplate_html.gz"]
        TEXT_FILE = "plain_text.gz"
        HTML_FILE = "deboilerplate_html.gz"
    else:
        PPROC = config["preprocessor"]

SHARDS = config["shards"]
BATCHES = config["batches"]

BOILERPLATE_CLEANING = config["boilerplateCleaning"]
PARAGRAPH_IDENTIFICATION = config["paragraphIdentification"]

CLEANHTML = return_dict_value_if_key(config, "cleanHTML", "", pos_value="--cleanhtml")
FTFY = return_dict_value_if_key(config, "ftfy", "", pos_value="--ftfy")
LANGID = return_dict_value_if_key(config, "langID", "cld2", only_check_key=True)
HTML5LIB = return_dict_value_if_key(config, "html5lib", "", pos_value="--html5lib")
BOILERPIPE_MAX_HEAP_SIZE = return_dict_value_if_key(config, "boilerpipeMaxHeapSize", -1, only_check_key=True)

PARSER = ""
PDFEXTRACT = ""
if "parser" in config:
    PARSER = f"--parser {config['parser']}"
if "PDFextract" in config and config["PDFextract"]:
    PDFEXTRACT_CF = ""
    PDFEXTRACT_SJ = ""
    PDFEXTRACT_KL = ""

    if "PDFextract_configfile" in config and config["PDFextract_configfile"]:
        PDFEXTRACT_CF = f" --pe_configfile {config['PDFextract_configfile']}"
    if "PDFextract_sentence_join_path" in config and config["PDFextract_sentence_join_path"]:
        PDFEXTRACT_SJ = f" --sentence_join_path {config['PDFextract_sentence_join_path']}"
    if "PDFextract_kenlm_path" in config and config["PDFextract_kenlm_path"]:
        PDFEXTRACT_KL = f" --kenlm_path {config['PDFextract_kenlm_path']}"

    PDFEXTRACT = f"--pdfextract {PDFEXTRACT_CF} {PDFEXTRACT_SJ} {PDFEXTRACT_KL}"

# sentence splitting and tokenisation
SENTTOKS = return_dict_value_if_key(config, "sentenceSplitters", {})
CUSTOMNBPS = return_dict_value_if_key(config, "customNBPs", {})
WORDTOKS = return_dict_value_if_key(config, "wordTokenizers", {})
MORPHTOKS = return_dict_value_if_key(config, "morphologicalAnalysers", {})

WORDTOK1 = f"{WORKFLOW}/data/moses/tokenizer/tokenizer.perl -q -b -a -l {LANG1}"
WORDTOK2 = f"{WORKFLOW}/data/moses/tokenizer/tokenizer.perl -q -b -a -l {LANG2}"

if WORDTOKS:
    WORDTOK1 = get_lang_or_default(WORDTOKS, LANG1)
    WORDTOK2 = get_lang_or_default(WORDTOKS, LANG2)

PRUNE_THRESHOLD = f"--prune {config['pruneThreshold']}"
PRUNE_TYPE = f"--prune-type {config['pruneType']}"

# Sharding
EMPTY_SHARD_CHECK = f"{TMPDIR}/empty_shard"
EMPTY_SHARD_BATCH_DIR = "EMPTY/1"

#################################################################
# EMBEDDINGS
EMBEDDINGS_BATCH_SIZE_NEURAL = config["embeddingsBatchSize"]
EMBEDDINGS_MODEL = return_dict_value_if_key(config, "embeddingsModel", "", only_check_key=True)

#################################################################
# DOCALIGN
DOCALIGN = config["documentAligner"]
DOC_THRESHOLD = return_dict_value_if_key(config, "documentAlignerThreshold", 0.1, only_check_key=True)

# mt
MT_COMMAND = config["alignerCmd"] if DOCALIGN == "externalMT" else None
SRC_LANG = LANG1
TRG_LANG = LANG2

if "translationDirection" in config and config["translationDirection"] == f"{LANG2}2{LANG1}":
    # Swap necessary variables
    SRC_LANG, TRG_LANG = LANG2, LANG1
    WORDTOK1, WORDTOK2 = WORDTOK2, WORDTOK1

# dic-based docalign
# snakemake/include/dic-docsegalign
#################################################################
# dic
# snakemake/include/dic-generation

DIC = return_dict_value_if_key(config, "dic", None)
DIC_GENERATE = return_dict_value_if_key(config, "generateDic", False)

#################################################################
# SEGALIGN
SEGALIGN = config["sentenceAligner"]
# bleualign
SEGALIGN_THRESHOLD = return_dict_value_if_key(config, "sentenceAlignerThreshold", 0.0)

# hunalign
# snakemake/include/dic-docsegalign
#################################################################
# CLEANING
DEFERRED = return_dict_value_if_key(config, "deferred", False, pos_value=True)
DEFERRED_CMD = "mmhsum" if DEFERRED else '' # Do NOT use double quote in the command

BIFIXER = return_dict_value_if_key(config, "bifixer", False, pos_value=True)
BIFIXER_DEFERRED_COLS = "--sdeferredcol src_deferred_hash --tdeferredcol trg_deferred_hash" if DEFERRED else ""
BIFIXER_PARAGRAPHS_COLS = "--sparagraphid src_paragraph_id --tparagraphid trg_paragraph_id" if PARAGRAPH_IDENTIFICATION else ""
# Enabled by default if not provided
BIFIXER_AGGRESSIVE_DEDUP = "--aggressive_dedup" if "bifixerAggressiveDedup" not in config else \
    return_dict_value_if_key(config, "bifixerAggressiveDedup", "", pos_value="--aggressive_dedup")
# Enabled by default if not provided
BIFIXER_IGNORE_SEGMENTATION = "--ignore_segmentation" if "bifixerIgnoreSegmentation" not in config else \
    return_dict_value_if_key(config, "bifixerIgnoreSegmentation", "", pos_value="--ignore_segmentation")

BICLEANER = return_dict_value_if_key(config, "bicleaner", False, pos_value=True)
BICLEANER_FLAVOUR = return_dict_value_if_key(config, "bicleanerFlavour", "classic", only_check_key=True)
BICLEANER_MODEL = config["bicleanerModel"] if BICLEANER else ""
BICLEANER_THRESHOLD = return_dict_value_if_key(config, "bicleanerThreshold", 0.0)
BICLEANER_GENERATE_MODEL = return_dict_value_if_key(config, "bicleanerGenerateModel", False)
BICLEANER_TRAINING_CORPUS_PREFIX = return_dict_value_if_key(config, "bicleanerParallelCorpusTrainingPrefix", [])
BICLEANER_AI_MONO_CORPUS_PREFIX = return_dict_value_if_key(config, "bicleanerMonoCorpusPrefix", [])
BICLEANER_AI_DEV_CORPUS_PREFIX = return_dict_value_if_key(config, "bicleanerParallelCorpusDevPrefix", [])
BICLEANER_BIN = "bicleaner-classify" if BICLEANER_FLAVOUR == "classic" else "bicleaner-ai-classify"

if BICLEANER:
    is_flavour_ai = BICLEANER_FLAVOUR == "ai"
    path_exists_f=os.path.isdir if is_flavour_ai else os.path.isfile

    # Create soft links based on the flavour if necessary
    if is_flavour_ai:
        # Get directory path from bicleaner model path
        BICLEANER_MODEL = '/'.join(BICLEANER_MODEL.split('/')[:-1])

    # BICLEANER_MODEL will be a directory if is_flavour_ai else will be a file

ELRC = return_dict_value_if_key(config, "elrc", False, pos_value=True)
TMX = return_dict_value_if_key(config, "tmx", False, pos_value=True)
DEDUPED = return_dict_value_if_key(config, "deduped", False, pos_value=True)

BIROAMER = return_dict_value_if_key(config, "biroamer", False, pos_value=True)
BIROAMER_MIX_FILES = "/dev/null"
BIROAMER_ALIGNMENT_CORPUS = ""
BIROAMER_OMIT = return_dict_value_if_key(config, "biroamerOmitRandomSentences", "", pos_value="-o")

OUTPUT_FILES = ["sent", "raw"]
STATS_FILES = ["sent", "raw"]

if TMX:
    OUTPUT_FILES.append("not-deduped.tmx")
if DEDUPED:
    OUTPUT_FILES.append("deduped.tmx")
    OUTPUT_FILES.append("deduped.txt")
    STATS_FILES.append("deduped")
if BIROAMER:
    if TMX:
        OUTPUT_FILES.append("not-deduped.roamed.tmx")

    if DEDUPED:
        OUTPUT_FILES.append("deduped.roamed.tmx")

    if "biroamerMixFiles" in config and len(config["biroamerMixFiles"]) > 0:
        BIROAMER_MIX_FILES = " ".join(config["biroamerMixFiles"])

    if "biroamerImproveAlignmentCorpus" in config:
        BIROAMER_ALIGNMENT_CORPUS = f"-a {config['biroamerImproveAlignmentCorpus']}"
if not BICLEANER_GENERATE_MODEL:
    BICLEANER_TRAINING_CORPUS_PREFIX = []
    BICLEANER_AI_MONO_CORPUS_PREFIX = []
    BICLEANER_AI_DEV_CORPUS_PREFIX = []


#################################################################
# DATASOURCES
HOSTS = set()
WARCS = set()
PREVERTICALS = set()

if "warcs" in config:
    WARCS = WARCS.union(config["warcs"])
if "hosts" in config:
    HOSTS = HOSTS.union(config["hosts"])
if "preverticals" in config:
    PREVERTICALS = PREVERTICALS.union(config["preverticals"])
if "hostsFile" in config:
    with open_xz_or_gzip_or_plain(config["hostsFile"]) as f:
        for line in f:
            HOSTS.add(line.strip())
if "warcsFile" in config:
    with open_xz_or_gzip_or_plain(config["warcsFile"]) as f:
        for line in f:
            WARCS.add(line.strip())
if "preverticalsFile" in config:
    with open_xz_or_gzip_or_plain(config["preverticalsFile"]) as f:
        for line in f:
            PREVERTICALS.add(line.strip())

# sort in order to avoid different results across executions
HOSTS = sorted(list(HOSTS))
WARCS = sorted(list(WARCS))
PREVERTICALS = sorted(list(PREVERTICALS))

# group hosts by domain and check their validity
DOMAIN_2_HOSTS = create_domain_key_2_host_map(HOSTS)

# assign an ID to each WARC and check that all WARCs exist
TARGET_2_PROVIDED_WARCS = create_id_key_2_file_map(WARCS, file_desc="WARCs")

# assign an ID to each prevertical and check that all preverticals exist
TARGET_2_PROVIDED_PREVERTICALS = create_id_key_2_file_map(PREVERTICALS, id_offset=len(TARGET_2_PROVIDED_WARCS), file_desc="preverticals")

# group crawled WARCs by domains
TARGET_2_CRAWLED_WARCS = dict([
    (domain, [f"{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz" for host in hosts])
    for (domain, hosts) in DOMAIN_2_HOSTS.items()
])

# group all files (e.g. WARCs, prevertical) by target (either domain if crawled, or ID if provided by user)
TARGET_2_WARCS = {**TARGET_2_CRAWLED_WARCS, **TARGET_2_PROVIDED_WARCS}
TARGET_2_PREVERTICALS = {**TARGET_2_PROVIDED_PREVERTICALS}

#################################################################
### WORKFLOW EXECUTION ##########################################
THREADS = {
    "split": 1,
    "translate": 1,
    "tokenise": 1,
    "docalign": 1,
    "segalign": 1,
    "bifixer": 1,
    "bicleaner": 1,
    "filter": 1,
    "sents": 1,
    "mgiza": 1,
}
JOB_THREADS = {
    "split": 0,
    "translate": 0,
    "tokenise": 0,
    "docalign": 0,
    "segalign": 0,
    "bifixer": 0,
    "bicleaner": 0,
}

if "parallelWorkers" in config:
    for k in config["parallelWorkers"]:
        THREADS[k] = config["parallelWorkers"][k]

if "parallelJobs" in config:
    for k in config["parallelJobs"]:
        JOB_THREADS[k] = config["parallelJobs"][k]

        # Modify the snakemake resources if they were not provided
        job_resource_name = f"job_{k}"

        if job_resource_name not in workflow.global_resources:
            # Since we want to run as parallel jobs as configured, the resources
            #  are squared in order to leave available the exact number of resources
            #  for each job.
            # E.g., if parallelJobs["bicleaner"] = 4, we want 4 jobs at max. to run
            #  in parallel, so we need 4 "job_bicleaner" resources to be free for each
            #  "bicleaner" job, so 4 * 4 = 16 necessary "job_bicleaner" resources
            workflow.global_resources[job_resource_name] = JOB_THREADS[k] ** 2

OUTPUT = []
UNTIL = return_dict_value_if_key(config, "until", "")
UNTIL_RULES_ALL_LANGS = ("crawl", "preprocess", "shard", "split", "tokenise") # All languages from LANGS

if UNTIL not in UNTIL_RULES_ALL_LANGS:
    # At some point, not all languages from LANGS will be taken into account but only LANG1 and LANG2

    if len(LANGS) > 2:
        # There are different languages to process, not only LANG1 and LANG2

        # Warn about the fact that not all languages will be processed at some point
        if not UNTIL:
            snakemake.logger.logger.warning("Not all languages will be processed until the end of the pipeline")
        else:
            snakemake.logger.logger.warning(f"Not all languages will be processed until '{UNTIL}'")

if not UNTIL: # config["until"] not provided -> run the whole pipeline
    OUTPUT = expand(
        "{permanent}/{lang1}-{lang2}.{output_file}.gz",
        permanent=PERMANENT,
        lang1=LANG1,
        lang2=LANG2,
        output_file=OUTPUT_FILES,
    )
    OUTPUT.extend(
        expand(
            "{permanent}/{lang1}-{lang2}.stats.{stats_file}",
            permanent=PERMANENT,
            lang1=LANG1,
            lang2=LANG2,
            stats_file=STATS_FILES,
        )
    )
    # this has to be added, because tokenisation rules are the last rules that don't need both shards
    # otherwise snakemake waites for both shards be completed to continue
    OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{TRG_LANG}")
    if DOCALIGN == "externalMT":
        OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}")
    elif DOCALIGN in ("DIC", "NDA"):
        OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{SRC_LANG}")
elif UNTIL == "crawl":
    if CRAWLTARGET == "linguacrawl":
        OUTPUT.append(f"{DATADIR}/warc/linguacrawl.finished")
    else:
        for domain, hosts in DOMAIN_2_HOSTS.items():
            for host in hosts:
                OUTPUT.append(f"{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz")
elif UNTIL == "preprocess":
    OUTPUT = expand(
        "{datadir}/preprocess/{target}/{pproc}/{lang}/{pproc_file}",
        datadir=DATADIR,
        target=TARGET_2_WARCS,
        pproc=PPROC,
        lang=LANGS,
        pproc_file=PPROC_FILES,
    )
    OUTPUT.extend(expand(
        "{datadir}/preprocess/{target}/prevertical2text/{lang}/{pproc_file}",
        datadir=DATADIR,
        target=TARGET_2_PREVERTICALS,
        lang=LANGS,
        pproc_file=PPROC_FILES,
    ))
elif UNTIL == "shard":
    OUTPUT = expand("{datadir}/shards/02.batches.{lang}", datadir=DATADIR, lang=LANGS)
elif UNTIL == "split":
    OUTPUT = expand("{datadir}/shards/03.split.{lang}", datadir=DATADIR, lang=LANGS)
elif UNTIL == "tokenise":
    OUTPUT = expand("{datadir}/shards/05.tokenise.{lang}", datadir=DATADIR, lang=LANGS)

# TODO should we implement all languages provided in LANGS?
# SRC_LANG and TRG_LANG dependent
elif UNTIL == "translate":
    OUTPUT = f"{DATADIR}/shards/04.translate.{SRC_LANG}2{TRG_LANG}"
elif UNTIL == "tokenise_trg":
    OUTPUT = f"{DATADIR}/shards/05.tokenise.{TRG_LANG}"
elif UNTIL == "tokenise_src":
    if DOCALIGN == "externalMT":
        OUTPUT = f"{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}"
    elif DOCALIGN in ("DIC", "NDA"):
        OUTPUT = f"{DATADIR}/shards/05.tokenise.{SRC_LANG}"
else:
    OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{TRG_LANG}")
    OUTPUT.append(f"{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}")
    if UNTIL == "docalign":
        OUTPUT.append(f"{TRANSIENT}/06_01.docalign.{LANG1}_{LANG2}")
    elif UNTIL == "segalign":
        OUTPUT.append(f"{TRANSIENT}/06_02.segalign.{LANG1}_{LANG2}")
    elif UNTIL == "bifixer":
        OUTPUT.append(f"{TRANSIENT}/07_01.bifixer.{LANG1}_{LANG2}")
    elif UNTIL == "bicleaner":
        OUTPUT.append(f"{TRANSIENT}/07_02.bicleaner.{LANG1}_{LANG2}")
    elif UNTIL == "filter":
        OUTPUT.append(f"{TRANSIENT}/07_03.filter.{LANG1}_{LANG2}")

#################################################################
# OTHER
BASE64_FILTER_CMD = "cat"

if PARAGRAPH_IDENTIFICATION:
    BASE64_FILTER_CMD = f"python3 {WORKFLOW}/utils/apply_command_b64_doc.py 'cut -f 1'"

# Create mark
with open(get_snakemake_execution_mark(TMPDIR), "w"):
    pass

shell.prefix("set -euo pipefail;")


#################################################################
### FINAL OUTPUTS ###############################################
rule all:
    input:
        OUTPUT,


#################################################################
### INCLUDE #####################################################
include: "rules/dic_generation.smk"
include: "rules/dic_doc_seg_align.smk"


#################################################################
### CRAWLING ####################################################

rule wget_download:
    """
    Download {target} with wget
    """
    params:
        url="http://{target}",
        folder=lambda wildcards, output: os.path.dirname(output[0]),
        time_limit=apply_format(CRAWLTIMELIMIT, "-t {}s"),
        user_agent=apply_format(USERAGENT, "-a '{}'"),
        wait=apply_format(CRAWLWAIT, "--wait {}"),
        file_types=apply_format(",".join(CRAWLFILETYPES), "-f {}")
    output:
        f"{DATADIR}/warc/{{target}}/wget.warc.gz",
    shell:
        """
        mkdir -p {params.folder} {TMPDIR}
        DIRNAME=$(mktemp -d "{TMPDIR}/downloaded.{wildcards.target}.XXXXXX")
        {PROFILING} python3 {WORKFLOW}/bitextor_wget.py --url {params.url} --output-path $DIRNAME {params.time_limit} {params.user_agent} {params.file_types} {params.wait} --warc {output}
        rm -rf $DIRNAME
        """


rule heritrix_download:
    """
    Download {target} with heritrix
    """
    params:
        url="http://{target}",
        folder=lambda wildcards, output: os.path.dirname(output[0]),
        time_limit=CRAWLTIMELIMIT
    output:
        f"{DATADIR}/warc/{{target}}/heritrix.warc.gz",
    shell:
        """
        URL=$(python3 -c "from bitextor.utils.common import check_connection; \
            e, url = check_connection('{params.url}'); \
            print(url) ; \
            exit(e)")
        if [ $? -ne 0 ]; then
            touch {DATADIR}/warc/{wildcards.target}/heritrix.warc
            gzip {DATADIR}/warc/{wildcards.target}/heritrix.warc
        else
            mkdir -p {params.folder} {TMPDIR}
            if [ "$(ps aux | grep -i Heritrix | grep -v grep)" == "" ]
                then {HERITRIXPATH}/bin/heritrix -a {HERITRIXUSER}
            fi
            curl -v -d "action=teardown" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
            curl -v -d "createpath={wildcards.target}&action=create" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine
            DIRNAME=$(mktemp -d "{TMPDIR}/downloaded.{wildcards.target}.XXXXXX")
            cat {WORKFLOW}/data/crawler-beans.cxml | sed "s@http://example.example/example@${{URL}}@g" > $DIRNAME/my-crawler-beans.cxml
            curl -v -T $DIRNAME/my-crawler-beans.cxml -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}/jobdir/crawler-beans.cxml
            curl -v -d "action=build" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
            curl -v -d "action=launch&checkpoint=latest" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
            sleep 2
            curl -v -d "action=unpause" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
            RUNTIME=0
            sleep 15
            while [ -f {HERITRIXPATH}/jobs/{wildcards.target}/latest/warcs/*warc.gz.open ]
            do
                sleep 5
                RUNTIME=$((RUNTIME+5))
                if [ "{params.time_limit}" != "" ]
                then
                    if [ $RUNTIME -gt "{params.time_limit}" ]
                    then
                        echo "Crawling time limit reached"
                        curl -v -d "action=pause" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                        curl -v -d "action=checkpoint" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                        curl -v -d "action=terminate" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                    fi
                fi
            done
            echo "Job {wildcards.target} finished!"
            cat {HERITRIXPATH}/jobs/{wildcards.target}/*/warcs/*warc.gz > {output}
        fi
        """


rule linguacrawl_config:
    """
    Create a linguacrawl yaml config file from Bitextor's parameters
    """
    output:
        yaml_file=temp(f"{TMPDIR}/linguacrawl.yaml"),
    params:
        user_agent=apply_format(USERAGENT, "user_agent: '{}'\n"),
        output_dir=apply_format(f"{DATADIR}/warc/linguacrawl", "output_dir: '{}'\n"),
        wait=apply_format(CRAWLWAIT, "crawl_delay: {}\n"),
        time_limit=apply_format(CRAWLTIMELIMIT, "max_time_per_site: {}\n"),
        size_limit=apply_format(CRAWLSIZELIMIT, "max_size_per_site: {}\n"),
        timeout=apply_format(CRAWLTIMEOUT, "connection_timeout: {}\n"),
        num_threads=apply_format(CRAWLJOBS, "max_jobs: {}\n"),
        prefix_filter=apply_format(str(CRAWLPREFIXFILTER), "prefix_filter: {}\n"),
        resume_crawling=apply_format(str(CONTINUECRAWL), "resume_crawling: {}\n"),
        dump_args=apply_format(str(CRAWLDUMPARGS), "verbose: {}\n"),
        file_types=apply_format(f"({'|'.join(CRAWLFILETYPES)})", "accepted_content: '{}'\n"),
        max_folder_depth=apply_format(CRAWLMAXFOLDERTREEDEPTH, "max_folder_tree_depth: {}\n"),
        scout_steps=apply_format(CRAWLSCOUTSTEPS, "scout_steps: {}\n"),
        blacklist=apply_format(str(CRAWLBLACKLISTURL), "url_blacklist: {}\n"),
    run:
        yaml_content = ""

        langs = "','".join(LANGS)
        hosts = "','".join(HOSTS)

        # Mandatory
        yaml_content += params.user_agent
        yaml_content += f"langs_of_interest: ['{langs}']\n"
        yaml_content += params.output_dir
        # Optional (the argument parser will not complain, but the crawler will crash in some cases)
        yaml_content += params.wait
        yaml_content += params.time_limit
        yaml_content += params.size_limit
        yaml_content += params.timeout
        yaml_content += params.num_threads
        yaml_content += params.resume_crawling
        yaml_content += f"seed_urls: ['{hosts}']\n"
        # yaml_content += f"seed_urls_from_file: \n" # HOSTS contain all hosts if hosts defined either with or without file
        yaml_content += params.prefix_filter
        yaml_content += params.dump_args
        yaml_content += params.file_types
        yaml_content += params.max_folder_depth
        yaml_content += f"max_attempts: 3\n"
        yaml_content += params.scout_steps
        yaml_content += f"min_langs_in_site: 2\n"
        yaml_content += params.blacklist

        # TODO use specific argument? If not, use SRC_LANG/TRG_LANG?
        if LANG1 or LANG2:
            yaml_content += f"mandatory_lang: '{LANG1}'\n" if LANG1 else f"mandatory_lang: '{LANG2}'\n"
            yaml_content += f"min_percent_mandatory_lang: 10\n"

        # tld = LANGS.union([params.url.split('.')[-1]]).union(TLD_CRAWL)
        tld = LANGS.union([url.split(".")[-1] for url in HOSTS]).union(TLD_CRAWL)
        tld = "','".join(tld)

        yaml_content += f"accepted_tlds: ['{tld}']\n"

        fdescriptor = open(output.yaml_file, "w")
        fdescriptor.write(yaml_content)
        fdescriptor.flush()
        fdescriptor.close()


# This must be a checkpoint because we do not know the resulted warcs since we let the user decide if they want to
#  either join all the warcs (it loses important information like the source of the warcs) or not
# Manual stopping: kill -s sigint `ps aux | grep bin/linguacrawl | grep -v grep | awk '{print $2}'`
# TODO move code to a single python script and call it
checkpoint linguacrawl_download:
    """
    Download HOSTS with linguacrawl
    """
    input:
        f"{TMPDIR}/linguacrawl.yaml"
    params:
        folder=f"{DATADIR}/warc/linguacrawl",
        crawl_cat=config["crawlCat"] if "crawlCat" in config else False,
        crawl_cat_max=config["crawlCatMaxSize"]*1024*1024 if "crawlCatMaxSize" in config else 0
    output:
        f"{DATADIR}/warc/linguacrawl.finished",
    run:
        shell(
            """
            mkdir -p {params.folder} {TMPDIR}
            {PROFILING} linguacrawl {input}
            """
        )

        all_files = subprocess.Popen(("ls", params.folder), stdout=subprocess.PIPE)
        try:
            # Process the resulted warcs
            warcs = subprocess.check_output(
                ("grep", "[.]warc[.]gz$"), stdin=all_files.stdout
            )  # It will throw an exception if no warcs were downloaded
            all_files.wait()

            warcs = warcs.decode("utf-8").split("\n")
            warcs = list(filter(lambda warc: len(warc) != 0, warcs))
            warcs = list(map(lambda warc: f"{params.folder}/{warc}", warcs))

            if not params.crawl_cat:
                warcs = "\n".join(warcs)
                shell(f'echo -e "{warcs}" > {{output[0]}}')
            else:
                if params.crawl_cat_max <= 0:
                    warcs = " ".join(warcs)
                    shell(
                        f"""
                        cat {warcs} > {params.folder}/linguacrawl.warc.gz
                        echo "{params.folder}/linguacrawl.warc.gz" > {{output[0]}}
                        """
                    )
                else:
                    # Improve the number of preprocess rules that are executed
                    current = 0
                    blocks = 0
                    files = [f"{params.folder}/linguacrawl{blocks}.warc.gz"]

                    for warc in warcs:
                        if current > {params.crawl_cat_max}:
                            current = 0
                            blocks += 1
                            files.append(f"{params.folder}/linguacrawl{blocks}.warc.gz")

                        shell(f"cat {warc} >> {files[-1]}")

                        du_command = subprocess.Popen(("du", "-b", warc), stdout=subprocess.PIPE)

                        try:
                            du_warc = subprocess.check_output(("awk", "{print $1}"), stdin=du_command.stdout)
                            du_command.wait()

                            current += int(du_warc)
                        except:
                            sys.stderr.write(f"WARNING: could not retrieve the size of {warc}\n")

                    files = "\n".join(files)
                    shell(f'echo -e "{files}" > {{output[0]}}')
        except:
            # No warcs were downloaded: error or warning
            if len(WARCS) == 0:
                sys.stderr.write("ERROR: could not find any file after crawling\n")
                sys.exit(1)
            sys.stderr.write("WARNING: could not find any file after crawling\n")
            shell(f"touch {output}")


#################################################################
### PREPROCESS ##################################################
rule warc2preprocess:
    """
    Process a list of WARCs (or a single WARC)
        and produce {plain_text,mime,url,normalized_html,deboilerplate_html}.gz
    """
    input:
        get_pproc_input,
    output:
        expand("{data}/preprocess/{{target}}/w2p/{lang}/{pproc_file}", data=DATADIR, lang=LANGS, pproc_file=PPROC_FILES),
    threads: 2
    params:
        folder=lambda wildcards, output: os.path.dirname(os.path.dirname(output[0])),  # remove "{lang}/{pproc_file}"
        pproclangs=','.join(LANGS),
        boilerplate="--boilerpipe" if BOILERPLATE_CLEANING else '',
        heap_size=apply_format(str(BOILERPIPE_MAX_HEAP_SIZE) if BOILERPIPE_MAX_HEAP_SIZE >= 0 else '', "--boilerpipe-max-heap-size {}"),
        paragraphs="--paragraph-identification" if PARAGRAPH_IDENTIFICATION else '',
    shell:
        """
        mkdir -p {params.folder}
        cat {input} \
            | {PROFILING} python3 {WORKFLOW}/bitextor_warc2htmlwarc.py {CLEANHTML} {FTFY} {PDFEXTRACT} --disable-output-gzip \
            | {PROFILING} python3 {WORKFLOW}/bitextor_warc2preprocess.py --input - --langs {params.pproclangs} \
                --compression gz --langid {LANGID} {params.boilerplate} {params.heap_size} {HTML5LIB} {PARSER} \
                --output-dir {params.folder} {params.paragraphs}
        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/plain_text.gz ]; then
                >&2 echo "WARNING: no \'$lang\' data found in {wildcards.target}: creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{plain_text,mime,url,normalized_html,deboilerplate_html}}
                gzip {params.folder}/$lang/{{plain_text,mime,url,normalized_html,deboilerplate_html}}

                # Generate empty file in order to be able to check if is needed to generate empty shards later
                touch "{EMPTY_SHARD_CHECK}_$lang"
            fi
        done
        """


rule warc2text:
    """
    Process a list of WARCs (or a single WARC)
        and produce {text,mime,url}.gz and optionally html.gz
    """
    input:
        get_pproc_input,
    output:
        expand(
            "{data}/preprocess/{{target}}/warc2text/{lang}/{pproc_file}",
            data=DATADIR,
            lang=LANGS,
            pproc_file=PPROC_FILES,
        ),
    params:
        folder=lambda wildcards, output: os.path.dirname(os.path.dirname(output[0])),  # remove "{lang}/{pproc_file}"
        f=",".join([f.strip(".gz") for f in PPROC_FILES]),
        paragraphs='--paragraph-identification' if PARAGRAPH_IDENTIFICATION else '',
    shell:
        """
        mkdir -p {params.folder}
        {PROFILING} warc2text -o {params.folder} -s -f {params.f} {params.paragraphs} {input}
        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/text.gz ]; then
                >&2 echo "WARNING: no \'$lang\' data found in {wildcards.target}: creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{{params.f}}}
                gzip {params.folder}/$lang/{{{params.f}}}

                # Generate empty file in order to be able to check if is needed to generate empty shards later
                touch "{EMPTY_SHARD_CHECK}_$lang"
            fi
        done
        """


rule prevertical2text:
    """
    Process a list of prevertical format files (or a single prevertical format file)
        and produce {text,mime,url}.gz and optionally html.gz
    """
    input:
        lambda wildcards: TARGET_2_PREVERTICALS[wildcards.target],
    output:
        expand(
            "{data}/preprocess/{{target}}/prevertical2text/{lang}/{pproc_file}",
            data=DATADIR,
            lang=LANGS,
            pproc_file=PPROC_FILES,
        ),
    params:
        folder=lambda wildcards, output: os.path.dirname(os.path.dirname(output[0])),  # remove "{lang}/{pproc_file}"
        f=",".join([f.strip(".gz") for f in PPROC_FILES]),
        boilerplate='-b' if BOILERPLATE_CLEANING else '',
        paragraphs='-p' if PARAGRAPH_IDENTIFICATION else '',
    shell:
        """
        CAT=$([[ {input} == *.gz ]] && echo "zcat" || echo "cat")

        mkdir -p {params.folder}
        $CAT {input} | \
            python3 {WORKFLOW}/bitextor_prevertical_lang_iso639_1.py | \
            {PROFILING} prevertical2text -o {params.folder} -s -f {params.f} {params.boilerplate} {params.paragraphs} -

        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/text.gz ]; then
                >&2 echo "WARNING: no \'$lang\' data found in {wildcards.target}: creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{{params.f}}}
                gzip {params.folder}/$lang/{{{params.f}}}

                # Generate empty file in order to be able to check if is needed to generate empty shards later
                touch "{EMPTY_SHARD_CHECK}_$lang"
            fi
        done
        """


# DAG will be re-evaluated after completing shard rule (because number of batches is dynamic and unknown)
checkpoint shard:
    """
    Use giashard to shard the output of warc2text/warc2preprocess to balance jobs
    :input: output of the preprocessing rules
    :output: a plain text files that contains the list of every batch generated
        (i.e. each line has a path to a batch folder)
    """
    input:
        # this is separated in two functions to make sure that
        # the preprocessing of the provided files and the crawling
        # may be executed in parallel
        # (otherwise the linguacrawl checkpoint might prevent that)
        get_shard_input_files,
        get_shard_input_crawled,
    output:
        f"{DATADIR}/shards/02.batches.{{lang}}",  # list of batches created for lang
    params:
        n=SHARDS,
        b=BATCHES,
        o_no_lang=lambda wildcards, output: os.path.dirname(output[0]),
        o=f"{DATADIR}/shards/{{lang}}",
        f=",".join([f.strip(".gz") for f in PPROC_FILES]),
        l=temp(f"{TMPDIR}/shard_list_files.{{lang}}"),
    shell:
        """
        ulimit -n 2048
        mkdir -p {params.o}
        rm -rf {params.o}/* # remove anything that might be left after a previous run
        rm -f {params.l}
        find {DATADIR}/preprocess/*/prevertical2text/{wildcards.lang} -type d >> {params.l} 2> /dev/null || true
        find {DATADIR}/preprocess/*/{PPROC}/{wildcards.lang} -type d >> {params.l} 2> /dev/null || true

        giashard_bin=$([[ "$(command -v giashard)" == "" ]] && echo "$HOME/go/bin/giashard" || echo "giashard")

        {PROFILING} $giashard_bin -n {params.n} -b {params.b} -o {params.o} -f {params.f} -l {params.l}

        nofiles=$(ls {params.o} | wc -l)

        if [[ "$nofiles" == "0" ]] && [[ -f "{EMPTY_SHARD_CHECK}_{wildcards.lang}" ]]; then
            # No files generated
            >&2 echo "WARNING: no files generated after running giashard for lang '{wildcards.lang}': creating empty files instead"

            # Generate empty shards if needed in order to avoid the pipeline to break
            mkdir -p {params.o}/{EMPTY_SHARD_BATCH_DIR}

            touch {params.o}/{EMPTY_SHARD_BATCH_DIR}/empty
            touch {params.o}/{EMPTY_SHARD_BATCH_DIR}/{{{params.f}}}
            gzip {params.o}/{EMPTY_SHARD_BATCH_DIR}/{{{params.f}}}
        fi

        ls -d {params.o}/*/* > {output}
        """


rule split:
    """
    Use sentence splitter to obtain sentences from the plain text file
    :input: gz-compressed file with a base64-encoded document per line
        document is the plain text extracted by the preprocess
    :output: gz-compressed file with a base64-encoded document per line
        output must have the same number of lines as the input (i.e. same number of docs)
    """
    input:
        f"{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/{TEXT_FILE}",
    output:
        f"{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/sentences.gz",
    params:
        splitter=lambda wildcards: apply_format(get_lang_or_default(SENTTOKS, wildcards.lang), '--sentence-splitter "{}"'),
        customnbp=lambda wildcards: apply_format(get_customnbp(CUSTOMNBPS, wildcards.lang), '--customnbp "{}"'),
        paragraphs='--process-paragraphs' if PARAGRAPH_IDENTIFICATION else '',
    threads: THREADS["split"]
    resources:
        job_split=JOB_THREADS["split"],
    shell:
        """
        parallel_cmd=$([[ {threads} -gt 1 ]] && echo "parallel --gnu --halt 2 --pipe --j {threads} -k" || echo "")

        zcat {input} \
            | {PROFILING} ${{parallel_cmd}} python3 {WORKFLOW}/bitextor_split.py \
                {params.splitter} {params.customnbp} \
                --langcode {wildcards.lang} \
                {PRUNE_THRESHOLD} {PRUNE_TYPE} {params.paragraphs} \
            | pigz -c > {output}
        """


rule aggregate_split:
    """
    Helper rule to implement until=split config
    :input: the result of every split rule
    :output: a file that contains the path to the output of every split rule per lang
    """
    input:
        lambda wildcards: [f"{batch}/sentences.gz" for batch in get_batches(wildcards.lang)],
    output:
        f"{DATADIR}/shards/03.split.{{lang}}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


#################################################################
### DOCALIGN ####################################################
rule aggregate_matches:
    """
    Helper rule to implement until=docalign config
    :input: the result of every docalign result
    :output: a file that contains the path to the output of every matches file
    """
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.{DOCALIGN}.06_01.matches"
            for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)
        ],
    output:
        f"{TRANSIENT}/06_01.docalign.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


# MT ############################################################
rule custom_translate:
    """
    Translate source documents into target language (according to translationDirection)
    :input: gz-compressed file with a base64-encoded document per line
        the documents are the output of sentence splitting
    :output: gz-compressed file with a base64-encoded translated document per line
        output must have the same number of lines as the input (i.e. same number of docs)
        each translated document must have the same number of lines as the source
    """
    input:
        source=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz",
    output:
        f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences_{TRG_LANG}.gz",
    params:
        paragraphs_file=f"{TMPDIR}/custom_translate_{{shard}}_{{src_batch}}.paragraphs",
        tmp_translations_file=f"{TMPDIR}/custom_translate_{{shard}}_{{src_batch}}.translations",
    threads: THREADS["translate"]
    resources:
        job_translate=JOB_THREADS["translate"],
    shell:
        """
        mkdir -p {TMPDIR}
        initial_nolines=$(zcat {input.source} | base64 -d | wc -l)
        output="{output}"
        filter_command="cat"

        if [[ "{PARAGRAPH_IDENTIFICATION}" == "True" ]]; then
            zcat {input.source} \
                | python3 {WORKFLOW}/utils/apply_command_b64_doc.py --empty-docs-value "" "cut -f 2" > "{params.paragraphs_file}"

            para_nolines=$(cat "{params.paragraphs_file}" | base64 -d \
                | grep -E "^p[0-9]+s[0-9]+/[0-9]+$|^p-1s-1$" | sed "/^\s*$/d" | wc -l)

            if [[ "$initial_nolines" -ne "$para_nolines" ]]; then
                >&2 echo "Lines count differs: source $initial_nolines, paragraph identification $para_nolines"
                exit 1
            fi

            output="{params.tmp_translations_file}"
            filter_command="python3 {WORKFLOW}/utils/apply_command_b64_doc.py --empty-docs-value '' 'cut -f 1'"
        fi

        parallel_cmd=$([[ {threads} -gt 1 ]] && echo "parallel --gnu --halt 2 --pipe --j {threads} -k" || echo "")

        zcat {input.source} \
            | eval "$filter_command" \
            | {PROFILING} b64filter cache ${{parallel_cmd}} {MT_COMMAND} \
            | pigz -c > "$output"

        n_after=$(zcat "$output" | base64 -d | wc -l)

        if [ $initial_nolines -ne $n_after ]; then
            >&2 echo "Lines count differs: source $initial_nolines, target $n_after"
            exit 1
        fi
        if [[ "{PARAGRAPH_IDENTIFICATION}" == "True" ]]; then
            paste <(zcat "$output") <(cat "{params.paragraphs_file}") \
                | python3 {WORKFLOW}/utils/join_b64_docs.py \
                | pigz -c > "{output}"
        fi

        if [[ "{PARAGRAPH_IDENTIFICATION}" == "True" ]]; then
            rm -f "{params.paragraphs_file}"
            rm -f "{params.tmp_translations_file}"
        fi
        """


rule aggregate_translate:
    """
    Helper rule to implement until=translate config
    :input: the result of every translate rule
    :output: a file that contains the path to the output of every translate rule per lang
    """
    input:
        lambda wildcards: [f"{batch}/sentences_{TRG_LANG}.gz" for batch in get_batches(SRC_LANG)],
    output:
        f"{DATADIR}/shards/04.translate.{SRC_LANG}2{TRG_LANG}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


rule tokenise_translated:
    """
    Tokenise the output of translation rule to feed into mt-docalign
    :input: gz-compressed file with a base64-encoded document per line
        the documents are the output of translation
    :output: gz-compressed file with a base64-encoded translated and tokenised document per line
        output must have the same number of lines as the input (i.e. same number of docs)
        each tokenised document must have the same number of lines as the source
    """
    input:
        rules.custom_translate.output,
    output:
        f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/tokenised_{TRG_LANG}.gz",
    params:
        tokeniser=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, TRG_LANG), '--word-tokenizer "{}"'),
        lemmatizer=lambda wildcards: apply_format(get_lang_or_default(MORPHTOKS, TRG_LANG), '--morph-analyser "{}"'),
    threads: THREADS["tokenise"]
    resources:
        job_tokenise=JOB_THREADS["tokenise"],
    shell:
        """
        parallel_cmd=$([[ {threads} -gt 1 ]] && echo "parallel --gnu --halt 2 --pipe --j {threads} -k" || echo "")

        zcat {input} \
            | eval "{BASE64_FILTER_CMD}" \
            | {PROFILING} ${{parallel_cmd}} python3 {WORKFLOW}/bitextor_tokenize.py \
                {params.tokeniser} {params.lemmatizer} \
                --langcode {TRG_LANG} \
            | pigz -c > {output}
        """


rule tokenise:
    """
    Tokenise the target side documents to feed into mt-docalign or dic-docalign
    :input: gz-compressed file with a base64-encoded document per line
        the documents are the output of sentence splitting
    :output: gz-compressed file with a base64-encoded tokenised document per line
        output must have the same number of lines as the input (i.e. same number of docs)
        each tokenised document must have the same number of lines as the source
    """
    input:
        f"{DATADIR}/shards/{{lang}}/{{shard}}/{{trg_batch}}/sentences.gz",
    output:
        f"{DATADIR}/shards/{{lang}}/{{shard}}/{{trg_batch}}/tokenised.gz",
    params:
        tokeniser=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, wildcards.lang), '--word-tokenizer "{}"'),
        lemmatizer=lambda wildcards: apply_format(get_lang_or_default(MORPHTOKS, wildcards.lang), '--morph-analyser "{}"'),
    threads: THREADS["tokenise"]
    resources:
        job_tokenise=JOB_THREADS["tokenise"],
    shell:
        """
        parallel_cmd=$([[ {threads} -gt 1 ]] && echo "parallel --gnu --halt 2 --pipe --j {threads} -k" || echo "")

        zcat {input} \
            | eval "{BASE64_FILTER_CMD}" \
            | {PROFILING} ${{parallel_cmd}} python3 {WORKFLOW}/bitextor_tokenize.py \
                {params.tokeniser} {params.lemmatizer} \
                --langcode {wildcards.lang} \
            | pigz -c > {output}
        """

rule aggregate_tokenise:
    """
    Helper rule to implement until=tokenise_trg config
    :input: the result of every tokenise rule
    :output: a file that contains the path to the output of every tokenise rule
    """
    input:
        lambda wildcards: [f"{batch}/tokenised.gz" for batch in get_batches(wildcards.lang)],
    output:
        f"{DATADIR}/shards/05.tokenise.{{lang}}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


rule aggregate_translate_tokenise_source:
    """
    Helper rule to implement until=tokenise_src config
    :input: the result of every tokenise_translated rule
    :output: a file that contains the path to the output of every tokenise_translated rule
    """
    input:
        lambda wildcards: [f"{batch}/tokenised_{TRG_LANG}.gz" for batch in get_batches(SRC_LANG)],
    output:
        f"{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


rule mt_matches:
    """
    Align documents using mt document aligner
    :input.l1: gz-compressed file with a base64-encoded document per line
        source language document translated into target language document, and tokenised
    :input.l2: gz-compressed file with a base64-encoded document per line
        tokenised target language documents
    :output: indices file
        plain text file with 3 tab separated columns: mt_doc_aligner_score, idx_translated, idx_trg
            idx_translated is the number of the aligned document in source language
            idx_trg is the number of the aligned document in target language
    """
    input:
        l1=rules.tokenise_translated.output,
        l2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/tokenised.gz",
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.externalMT.06_01.matches",
    params:
        folder=lambda wildcards, output: os.path.dirname(output[0]),
    threads: THREADS["docalign"]
    resources:
        job_docalign=JOB_THREADS["docalign"],
    shell:
        """
        mkdir -p {params.folder}

        {PROFILING} docalign {input.l1} {input.l2} --threshold {DOC_THRESHOLD} -j {threads} > {output}
        """


rule doc_emb_matches:
    """
    Align documents using neural-document-aligner
    :input.plain1: gz-compressed file with a base64-encoded document per line
        sentence-split source documents
    :input.plain2: gz-compressed file with a base64-encoded document per line
        sentence-split target documents
    :output: indices file
        plain text file with 3 tab separated columns: src_idx, trg_idx, nda_score
            src_idx is the number of the aligned document in source language
            trg_idx is the number of the aligned document in target language
    """
    input:
        plain1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz",
        plain2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/sentences.gz",
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.NDA.06_01.matches",
    params:
        folder=f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{{shard}}/nda/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}",
        src_emb_name="emb.src",
        trg_emb_name="emb.trg",
        model=apply_format(EMBEDDINGS_MODEL, "--model {}"),
    threads: THREADS["docalign"]
    shell:
        """
        mkdir -p {params.folder}
        cat <(zcat {input.plain1} | eval "{BASE64_FILTER_CMD}" | sed "s:$:	-	src:") \
            <(zcat {input.plain2} | eval "{BASE64_FILTER_CMD}" | sed "s:$:	-	trg:") \
            | {PROFILING} neural-document-aligner - {params.folder}/{params.src_emb_name} \
                {params.folder}/{params.trg_emb_name} {params.model} \
                --docalign-strategy 'faiss' --weights-strategy 0 \
                --merging-strategy 3 --results-strategy 0 \
                --emb-optimization-strategy 2 --gen-emb-optimization-strategy 2 \
                --output-with-idxs --paths-to-docs-are-base64-values \
                --threshold {DOC_THRESHOLD} --logging-level 30 \
                --embeddings-batch-size {EMBEDDINGS_BATCH_SIZE_NEURAL} > {output}
        """


### SEGALIGN ####################################################
rule aggregate_segalign:
    """
    Helper rule to implement until=segalign config
    :input: the result of every segalin result
    :output: a file that contains the path to every segalign result
    """
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.{SEGALIGN}.06_02.segalign.gz"
            for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)
        ],
    output:
        f"{TRANSIENT}/06_02.segalign.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


# BLEUALIGN #####################################################
rule bleualign:
    """
    Use bleualign to align sentences withing the matched documents
    :input.indices: output of mt docalign (columns are "mt_doc_aligner_score idx_translated idx_trg")
    :input.plain1: gz-compressed file with a base64-encoded document per line
        sentence-split source documents
    :input.plain2: gz-compressed file with a base64-encoded document per line
        sentence-split target documents
    :input.url1: gz-compressed file with a URL per line for source (source of the corresponding documents)
    :input.url2: gz-compressed file with a URL per line for target (source of the corresponding documents)
    :input.translated1: gz-compressed file with a base64-encoded document per line
        source documents translated into target language
    :output: aligned sentences
        gz-compressed file with 5 tab-separated columns: url1,url2,sentence1,sentence2,bleualign_score
    """
    input:
        indices=rules.mt_matches.output,
        plain1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz",
        plain2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/sentences.gz",
        url1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/url.gz",
        url2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/url.gz",
        translated1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences_{TRG_LANG}.gz",
    params:
        folder=lambda wildcards, output: os.path.dirname(output[0]),
        paragraphs="--paragraph-identification" if PARAGRAPH_IDENTIFICATION else '',
        deferred="--print-sent-hash" if DEFERRED else '',
    # in segalign rule output columns are reordered (or not) in accordance with translationDirection
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.bleualign.06_02.segalign.gz",
    threads: THREADS["segalign"]
    resources:
        job_segalign=JOB_THREADS["segalign"],
    shell:
        """
        mkdir -p {params.folder}

        parallel_cmd=$([[ {threads} -gt 1 ]] && echo "parallel --gnu --halt 2 --pipe --j {threads} -l 1 --group --header 1" || echo "")
        header="src_url\ttrg_url\tsrc_text\ttrg_text\tsrc_translated"

        python3 {WORKFLOW}/utils/cut_header.py -f idx_translated,idx_trg --input {input.indices} \
            | tail -n +2 \
            | docjoin \
                -l {input.url1} -r {input.url2} \
                -l {input.plain1} -r {input.plain2} \
                -l {input.translated1} \
            | cat <(echo "$header") - \
            | {PROFILING} ${{parallel_cmd}} bleualign_cpp {params.deferred} {params.paragraphs} \
                --bleu-threshold {SEGALIGN_THRESHOLD} \
            | pigz -c > {output}
        """


# VECALIGN #####################################################
rule vecalign:
    """
    Use vecalign to align sentences withing the matched documents
    :input.indices: output of embeddings docalign (columns are "emb_doc_aligner_score src_index trg_index")
    :input.plain1: gz-compressed file with a base64-encoded document per line
        sentence-split source documents
    :input.plain2: gz-compressed file with a base64-encoded document per line
        sentence-split target documents
    :input.url1: gz-compressed file with a URL per line for source (source of the corresponding documents)
    :input.url2: gz-compressed file with a URL per line for target (source of the corresponding documents)
    :output: aligned sentences
        gz-compressed file with 5 tab-separated columns: url1,url2,sentence1,sentence2,vecalign_score
    """
    input:
        indices=rules.doc_emb_matches.output,
        plain1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz",
        plain2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/sentences.gz",
        url1=f"{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/url.gz",
        url2=f"{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/url.gz",
    params:
        folder=f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{{shard}}/vecalign/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}",
        src_emb_storage=f"{rules.doc_emb_matches.params.folder}/{rules.doc_emb_matches.params.src_emb_name}",
        trg_emb_storage=f"{rules.doc_emb_matches.params.folder}/{rules.doc_emb_matches.params.trg_emb_name}",
        paragraphs="--paragraph-identification" if PARAGRAPH_IDENTIFICATION else '',
        deferred=f"--print-sent-hash \"{DEFERRED_CMD}\"" if DEFERRED else '',
        model=apply_format(EMBEDDINGS_MODEL, "--model {}"),
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.vecalign.06_02.segalign.gz",
    threads: THREADS["segalign"]
    resources:
        job_segalign=JOB_THREADS["segalign"],
    shell:
        """
        mkdir -p "{params.folder}"

        cat <(zcat "{input.plain1}") <(zcat "{input.plain2}") \
            <(zcat "{input.url1}" | sed "s:$:\tsrc:") <(zcat "{input.url2}" | sed "s:$:\ttrg:") \
                | python3 "{WORKFLOW}/utils/print_alternatively_lines_cli.py" 2 | paste - - \
                | {PROFILING} python3 "{WORKFLOW}/bitextor_nda_vecalign.py" - "{input.indices}" \
                    --threshold {SEGALIGN_THRESHOLD} {params.paragraphs} {params.deferred} {params.model} \
                    --tmp-dir "{params.folder}" --nda-input-is-base64 \
                    --embeddings-batch-size {EMBEDDINGS_BATCH_SIZE_NEURAL} --first-match-offset 1 \
                    --embedding-src-storage-input "{input.plain1}" --embedding-src-storage-input-base64 \
                    --embedding-src-storage-path "{params.src_emb_storage}" --embedding-trg-storage-input "{input.plain2}" \
                    --embedding-trg-storage-input-base64 --embedding-trg-storage-path "{params.trg_emb_storage}" \
                    --src-embeddings-optimization-strategy 2 --trg-embeddings-optimization-strategy 2 \
                    --src-storage-embeddings-optimization-strategy 2 --trg-storage-embeddings-optimization-strategy 2 \
                    --embedding-src-storage-not-uniq --embedding-trg-storage-not-uniq \
                    --vecalign-num-overlaps 2 --vecalign-alignment-max-size 2 \
                | pigz -c > "{output}"
        """


### FILTERING AND CLEANING ######################################

split_input_filename = "bleualign.06_02.segalign"

if SEGALIGN == "hunalign":
    split_input_filename = "hunalign.06_02.segalign"
elif SEGALIGN == "vecalign":
    split_input_filename = "vecalign.06_02.segalign"

# split segalign results into balanced chunks
checkpoint split_segalign:
    """
    Join all of the output of segalign and split them again into even chunks this time
        each batch is a gz-compressed file that has the same format as segalign output
        if translatationDirection is different from lang1->lang2, in this rule the columns are switched
    :input: the result of every segalin result
    :output: a plain text files that contains the list of every postprocessing batch generated
        (i.e. each line has a path to a postprocessing batch file without the extension)
    """
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/shards/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.{split_input_filename}.gz"
            for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)
        ],
    output:
        batches=f"{TRANSIENT}/{LANG1}_{LANG2}/{LANG1}_{LANG2}.postprocessing_batches",
    params:
        size=BATCHES,  # use same parameter as for shards
        folder=f"{TRANSIENT}/{LANG1}_{LANG2}/{split_input_filename}",
    run:
        # We need to make this check in order to avoid the pipeline to break in the case that no input was received (e.g. no common shards)
        # It is needed to run this piece of code directly in Python and not in bash because Snakemake attempts to replace {input[0]} before
        #  running the script, so it fails even before we are able to check if the input is empty in bash script
        if len(input) == 0:
            raise Exception("Didn't get any input file")
        else:
            shell(
                """
                mkdir -p {params.folder}
                rm -f {params.folder}/* # remove anything that might be left after a previous run
                header=$(head -1 <(zcat {input[0]}) | tr -d '\n')

                echo {input} \
                    | tr ' ' '\n' \
                    | xargs -I[] bash -c 'zcat "[]" | tail -n +2' \
                    | cat <(echo "$header") - \
                    | python3 {WORKFLOW}/bitextor_split_segalign.py -f src_text,trg_text -s {params.size} --gzip -o "{params.folder}/"
                if [ -z "$(ls -A {params.folder})" ]; then
                    cat < /dev/null > {output.batches}
                else
                    ls {params.folder}/* | sed 's/[.]gz$//g' > {output.batches}
                fi
                """
            )


rule bifixer:
    """
    Apply bifixer to the segalign output
    :input: a single chunk of segalign output
        gz-compressed, columns are: url1 url2 sent1 sent2 score deferred1 deferred2
        (deferred is optional)
    :output: plain text, marked as temp, same columns as input with two new columns: hash and score
    """
    input:
        segalign=f"{TRANSIENT}/{LANG1}_{LANG2}/{split_input_filename}/{{batch}}.gz",
    output:
        temp(f"{TRANSIENT}/{LANG1}_{LANG2}/07_01.bifixer/{{batch}}.gz"),
    threads: THREADS["bifixer"]
    resources:
        job_bifixer=JOB_THREADS["bifixer"],
    shell:
        """
        parallel_cmd=$([[ {threads} -gt 1 ]] && echo "parallel --gnu --halt 2 --pipe --j {threads} -k --header 1" || echo "")

        zcat {input.segalign} \
            | {PROFILING} ${{parallel_cmd}} bifixer -q {BIFIXER_AGGRESSIVE_DEDUP} {BIFIXER_IGNORE_SEGMENTATION} \
                {BIFIXER_DEFERRED_COLS} {BIFIXER_PARAGRAPHS_COLS} --header - - {SRC_LANG} {TRG_LANG} \
            | pigz -c > {output}
        """


rule aggregate_bifixer:
    """
    Helper rule to implement until=bifixer config
    :input: the result of every bifixer result
    :output: a file that contains the path to every bifixer result
    """
    input:
        lambda wildcards: [f"{TRANSIENT}/{LANG1}_{LANG2}/07_01.bifixer/{batch}.gz" for batch in get_postproc_batches()],
    output:
        f"{TRANSIENT}/07_01.bifixer.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


if BICLEANER_GENERATE_MODEL and BICLEANER_FLAVOUR == "classic":
    rule bicleaner_train_model:
        """
        Train bicleaner model (flavour: classic)
        """
        input:
            corpus_l1=expand("{dataset}.{lang}.gz", dataset=BICLEANER_TRAINING_CORPUS_PREFIX, lang=SRC_LANG),
            corpus_l2=expand("{dataset}.{lang}.gz", dataset=BICLEANER_TRAINING_CORPUS_PREFIX, lang=TRG_LANG),
            # 'optional_str_input' is needed because if 'config["generateDic"] == True and not BICLEANER_GENERATE_MODEL' then
            #   will trigger this rule ("reason: Input files updated by another job") even if the output already exists
            e2f=optional_str_input(f"{DIC}.lex.e2f.gz", BICLEANER_GENERATE_MODEL),
            f2e=optional_str_input(f"{DIC}.lex.f2e.gz", BICLEANER_GENERATE_MODEL),
            vcb1=optional_str_input(f"{mgizaModelDir}/corpus.{SRC_LANG}.filtered.vcb.gz", BICLEANER_GENERATE_MODEL),
            vcb2=optional_str_input(f"{mgizaModelDir}/corpus.{TRG_LANG}.filtered.vcb.gz", BICLEANER_GENERATE_MODEL),
        output:
            model=BICLEANER_MODEL,
        params:
            training_corpus=temp(f"{TMPDIR}/bicleaner_train/training_parallel_corpus.{SRC_LANG}_{TRG_LANG}"),
            classifier=f"{'/'.join(BICLEANER_MODEL.split('/')[:-1])}/{SRC_LANG}-{TRG_LANG}.classifier",
        shell:
            """
            echo "Training corpus: {params.training_corpus}"
            mkdir -p "{TMPDIR}/bicleaner_train"
            paste <(zcat {input.corpus_l1}) <(zcat {input.corpus_l2}) > {params.training_corpus}

            {PROFILING} bicleaner-train {params.training_corpus} -S "{WORDTOK1}" -T "{WORDTOK2}" --treat_oovs \
                --normalize_by_length -s {SRC_LANG} -t {TRG_LANG} -d {input.e2f} -D {input.f2e} -f {input.vcb1} \
                -F {input.vcb2} -c {params.classifier} -m {output.model} --classifier_type random_forest \
                --seed 71213
            """

if BICLEANER_GENERATE_MODEL and BICLEANER_FLAVOUR == "ai":
    rule bicleaner_train_model:
        """
        Train bicleaner model (flavour: ai)
        """
        input:
            corpus_l1=expand("{dataset}.{lang}.gz", dataset=BICLEANER_TRAINING_CORPUS_PREFIX, lang=SRC_LANG),
            corpus_l2=expand("{dataset}.{lang}.gz", dataset=BICLEANER_TRAINING_CORPUS_PREFIX, lang=TRG_LANG),
            mono_l1=' '.join(expand("{dataset}.{lang}.gz", dataset=BICLEANER_AI_MONO_CORPUS_PREFIX, lang=SRC_LANG)),
            mono_l2=' '.join(expand("{dataset}.{lang}.gz", dataset=BICLEANER_AI_MONO_CORPUS_PREFIX, lang=TRG_LANG)),
            dev_corpus_l1=' '.join(expand("{dataset}.{lang}.gz", dataset=BICLEANER_AI_DEV_CORPUS_PREFIX, lang=SRC_LANG)),
            dev_corpus_l2=' '.join(expand("{dataset}.{lang}.gz", dataset=BICLEANER_AI_DEV_CORPUS_PREFIX, lang=TRG_LANG)),
        output:
            model=BICLEANER_MODEL,
        params:
            training_corpus=temp(f"{TMPDIR}/bicleaner_train/training_parallel_corpus.{SRC_LANG}_{TRG_LANG}"),
            tfreq=temp(f"{TMPDIR}/bicleaner_train/bicleaner-ai/wordfreq_{TRG_LANG}.gz"),
            dev_corpus=temp(f"{TMPDIR}/bicleaner_train/bicleaner-ai/dev_parallel_corpus.{SRC_LANG}_{TRG_LANG}"),
            mono_corpus=temp(f"{TMPDIR}/bicleaner_train/bicleaner-ai/mono_corpus.{SRC_LANG}_{TRG_LANG}"),
            metadata_dir=BICLEANER_MODEL,
            ai_metadata=f"{BICLEANER_MODEL}/metadata.yaml",
            model=BICLEANER_MODEL,
        shell:
            """
            echo "Training corpus: {params.training_corpus}"
            mkdir -p "{TMPDIR}/bicleaner_train/bicleaner-ai"
            paste <(zcat {input.corpus_l1}) <(zcat {input.corpus_l2}) > {params.training_corpus}

            # Target lang frequency
            echo "Frequency file (lang={TRG_LANG}): {params.tfreq}"
            zcat {input.mono_l2} \
                | {WORDTOK2} \
                | awk '{{print tolower($0)}}' \
                | tr ' ' '\n' \
                | LC_ALL=C sort | uniq -c \
                | LC_ALL=C sort -nr \
                | grep -v "[[:space:]]*1" \
                | pigz -c > {params.tfreq}

            # Dev corpus
            echo "Dev corpus: {params.dev_corpus}"
            paste <(zcat {input.dev_corpus_l1}) <(zcat {input.dev_corpus_l2}) > {params.dev_corpus}

            # Mono
            echo "Mono corpus: {params.mono_corpus}"

            mono_l1_nolines=$(zcat {input.mono_l1} | wc -l)
            mono_l2_nolines=$(zcat {input.mono_l2} | wc -l)
            mono_min_nolines=$([[ $mono_l1_nolines -gt $mono_l2_nolines ]] && echo "$mono_l2_nolines" || echo "$mono_l1_nolines")

            head -n $mono_min_nolines <(zcat {input.mono_l1}) > {params.mono_corpus}
            head -n $mono_min_nolines <(zcat {input.mono_l2}) >> {params.mono_corpus}

            # Train
            {PROFILING} bicleaner-ai-train -S "{WORDTOK1}" -T "{WORDTOK2}" -s {SRC_LANG} -t {TRG_LANG} -F {params.tfreq} \
                -m {params.metadata_dir} --classifier_type dec_attention --parallel_train {params.training_corpus} \
                --parallel_valid {params.dev_corpus} --mono_train {params.mono_corpus} --seed 71213 && \
                    mv {params.metadata_dir} {output.model}
            """


rule bicleaner:
    """
    Compute bicleaner scores of the aligned sentence pairs
    :input.bifixer: either the output of bifixer rule, or a single chunk of segalign output if bifixer is disabled
    :input.model: bicleaner model, either provided by the user or generated by train_bicleaer
    :output: gz-compressed, same columns as input with one new column: score
    """
    input:
        bifixer=rules.bifixer.output if BIFIXER else rules.bifixer.input.segalign,
        model=BICLEANER_MODEL,
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/07_02.bicleaner/{{batch}}.gz",
    params:
        metadata_file=f"{BICLEANER_MODEL}/metadata.yaml" if BICLEANER_FLAVOUR == "ai" else BICLEANER_MODEL,
    threads: THREADS["bicleaner"]
    resources:
        job_bicleaner=JOB_THREADS["bicleaner"],
    shell:
        """
        parallel_cmd=$([[ {threads} -gt 1 ]] && echo "parallel --gnu --halt 2 --pipe --j {threads} -k --header 1" || echo "")
        slang=$(egrep "source_lang" {params.metadata_file} | cut -d " " -f 2)
        text_fields=$(head -1 <(zcat {input.bifixer}) \
            | python3 {WORKFLOW}/utils/cut_header.py -f src_text,trg_text --only-print-idxs --respect-original-sorting \
            | tr -d '\n')

        bicleaner_src_lang=$(cat "{params.metadata_file}" | grep "^source_lang: " | awk '{{print $2}}')
        bicleaner_trg_lang=$(cat "{params.metadata_file}" | grep "^target_lang: " | awk '{{print $2}}')
        bicleaner_text_fields="--scol src_text --tcol trg_text"

        if [[ -z "$bicleaner_src_lang" ]] || [[ -z "$bicleaner_trg_lang" ]]; then
            >&2 echo "ERROR: could not gather the languages from the bicleaner configuration file"

            false # Trigger exit (it will work since bash is executed in strict mode)
        else
            if [[ "$bicleaner_src_lang" == "{SRC_LANG}" ]] && [[ "$bicleaner_trg_lang" == "{TRG_LANG}" ]]; then
                # Ok
                true
            elif [[ "$bicleaner_src_lang" == "{TRG_LANG}" ]] && [[ "$bicleaner_trg_lang" == "{SRC_LANG}" ]]; then
                # Change src and trg
                bicleaner_text_fields="--scol trg_text --tcol src_text"
            else
                >&2 echo "WARNING: the provided Bicleaner model seems to don't expect the provided languages: {SRC_LANG}-{TRG_LANG} was provided, ${{bicleaner_src_lang}}-${{bicleaner_trg_lang}} was expected by Bicleaner ({BICLEANER_FLAVOUR})"
            fi
        fi

        # Bicleaner doesn't need word tokenizer since it'll be defined in the model metadata if any was used
        zcat {input.bifixer} \
            | {PROFILING} cache -k $text_fields ${{parallel_cmd}} {BICLEANER_BIN} $bicleaner_text_fields \
                --header -q - - {input.model} \
            | pigz -c > {output}
        """


rule aggregate_bicleaner:
    """
    Helper rule to implement until=bicleaner config
    :input: the result of every biclenaer result
    :output: a file that contains the path to every biclenaer result
    """
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/07_02.bicleaner/{batch}.gz" for batch in get_postproc_batches()
        ],
    output:
        f"{TRANSIENT}/07_02.bicleaner.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


filter_input = rules.bicleaner.output if BICLEANER else rules.bicleaner.input.bifixer


rule pre_filter:
    input:
        filter_input,
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{{batch}}.sort_flags"
    run:
        header = None

        with open_xz_or_gzip_or_plain(str(input)) as f:
            for header in f:
                break

        header = header.strip().split('\t')
        src_text_idx = header.index('src_text') + 1 # sort counts from 1, not 0
        trg_text_idx = header.index('trg_text') + 1
        src_url_idx = header.index('src_url') + 1
        trg_url_idx = header.index('trg_url') + 1

        sort_flags = f"-k{src_text_idx},{src_text_idx} -k{trg_text_idx},{trg_text_idx} " \
                     f"-k{src_url_idx},{src_url_idx} -k{trg_url_idx},{trg_url_idx}"

        if "bifixer_hash" in header:
            i = header.index("bifixer_hash") + 1
            j = ""

            if "bifixer_score" in header:
                j = header.index('bifixer_score') + 1
                j = f"-k{j},{j}nr "

            sort_flags = f"-k{i},{i} {j}" + sort_flags

        with open(output[0], 'w') as f:
            f.write(f"{sort_flags}\n")


rule pre_filter_sort_flags:
    input:
        lambda wildcards: [f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{batch}.sort_flags" for batch in get_postproc_batches()],
    output:
        f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/sort_flags"
    shell:
        """
        value="/dev/null"
        first="0"

        for f in $(echo {input} | tr ' ' '\n'); do
            if [[ "$first" == "0" ]]; then
                value="$f"
                first="1"
            else
                current_value="$f"
                d=$(diff $value $current_value | wc -l)

                if [[ "$d" != "0" ]]; then
                    >&2 echo "ERROR: all sort flags have to be the same"

                    false # Trigger exit (it will work since bash is executed in strict mode)
                fi
            fi
        done

        cat $value > {output}
        """


rule filter:
    """
    Filter by biclenaer threshold (if applicable), compute ELRC metrics (if applicable), sort by sentence pair or bifixer hash
    :input: either the output of bicleaner (if enabled), or the output of the previous step (i.e. what would be the input of bicleaner if it was enabled)
    :output: plain-text file, marked as temp, the senteces are sorted by duplicates
        remove sentences below biclenaer threshold (if applicable)
        add new columns for ELRC if applicable: length_ration, num_tokens_src, num_tokens_trg
    """
    input:
        f=filter_input,
        sort_flags=rules.pre_filter_sort_flags.output,
    output:
        temp(f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{{batch}}"),
    params:
        bicleaner_header="bicleaner_score" if BICLEANER_FLAVOUR == "classic" else "bicleaner_ai_score",
    threads: THREADS["filter"]
    shell:
        """
        sort_flags="$(cat {input.sort_flags} | tr -d '\n')"
        FILTER_BICLEANER_CMD="\
            [[ '{BICLEANER}' == 'True' ]] \
                && ({PROFILING} python3 {WORKFLOW}/bitextor_filter_bicleaner.py \
                    --threshold {BICLEANER_THRESHOLD} --header-field {params.bicleaner_header}; true) \
                || cat"
        ELRC_CMD="\
            [[ '{ELRC}' == 'True' ]] \
                && ({PROFILING} python3 {WORKFLOW}/bitextor_elrc_filtering.py -s; true) \
                || cat"

        zcat {input.f} \
            | eval "$FILTER_BICLEANER_CMD" \
            | eval "$ELRC_CMD" \
            > {output}

        header=$(head -1 <(cat {output}) | tr -d '\n')

        # Remove header, sort and add header (in-place) from output file
        sed -i '1 d' {output} \
            && LC_ALL=C sort -t $'\t' $sort_flags --parallel {threads} \
                --compress-program=gzip -T {TMPDIR} -o {output} {output} \
            && sed -i '1 s/^/'"$header"'\\n/' {output}

        if [[ ! -s "{output}" ]]; then
            echo "$header" > {output}
        fi
        """


rule aggregate_filter:
    """
    Helper rule to implement until=filter config
    :input: the result of every filter result
    :output: a file that contains the path to every filter result
    """
    input:
        lambda wildcards: [f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{batch}" for batch in get_postproc_batches()],
    output:
        f"{TRANSIENT}/07_03.filter.{LANG1}_{LANG2}",
    shell:
        """ echo {input} | tr ' ' '\n' > {output} """


raw_input_filename = rules.filter.input[0].split("/")[-2] # {bleualign,hunalign,vecalign}.06_02.segalign / 07_01.bifixer / 07_02.bicleaner


rule raw:
    """
    Create {lang1}-{lang2}.raw.gz file by concatenating the output chunks of the last step before filtering
        may be segalign, bifixer or biclenaer, depending on what's enabled in the config
    :input: the output of the last step
    :output.corpus: the concatenated inputs, columns are the same as the input
    :output:stats: the corresponding stats file in plain text
    """
    input:
        lambda wildcards: [
            f"{TRANSIENT}/{LANG1}_{LANG2}/{raw_input_filename}/{batch}.gz" for batch in get_postproc_batches()
        ],
    output:
        corpus=f"{PERMANENT}/{LANG1}-{LANG2}.raw.gz",
        stats=f"{PERMANENT}/{LANG1}-{LANG2}.stats.raw",
    shell:
        """
        header=$(head -1 <(zcat {input[0]}) | tr -d '\n')

        # Remove header from all input files and only print header once
        echo {input} \
            | tr ' ' '\n' \
            | xargs -I[] bash -c 'zcat "[]" | tail -n +2' \
            | cat <(echo "$header") - \
            | pigz -c > {output.corpus}

        # Stats
        WC1=$(zcat {output.corpus} | python3 {WORKFLOW}/utils/cut_header.py -f src_text | tail -n +2 | wc -wl | tr -s ' ')
        WC2=$(zcat {output.corpus} | python3 {WORKFLOW}/utils/cut_header.py -f trg_text | tail -n +2 | wc -w)

        echo "{LANG1}-{LANG2} raw" > {output.stats}
        echo "File size: $(du -h {output.corpus} | cut -f 1)" >> {output.stats}
        echo "Sentence pairs: $(echo $WC1 | cut -d ' ' -f 1)" >> {output.stats}
        echo "{SRC_LANG} words: $(echo $WC1 | cut -d ' ' -f 2)" >> {output.stats}
        echo "{TRG_LANG} words: $WC2" >> {output.stats}
        """


rule sents:
    """
    Create {lang1}-{lang2}.sent.gz by concatenated and merge-sorting the outputs of filters rule
    :input: the outputs of filter step
    :output: the concatenated inputs, sorted, same columns as input
    """
    input:
        lambda wildcards: [f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{batch}" for batch in get_postproc_batches()],
    output:
        corpus=f"{PERMANENT}/{LANG1}-{LANG2}.sent.gz",
        stats=f"{PERMANENT}/{LANG1}-{LANG2}.stats.sent",
    params:
        sort_flags=f"{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/0.sort_flags",
    threads: THREADS["sents"]
    run:
        cmd = \
        f"""
        sort_flags="$(cat {params.sort_flags} | tr -d '\n')"
        header=$(head -1 <(cat {input[0]}) | tr -d '\\n')

        LC_ALL=C sort -t $'\\t' $sort_flags --parallel {threads} --compress-program=gzip \\
            -T {TMPDIR} --merge \\
        """

        for filename in input:
            # Use tmp named pipes since --merge does need the file descriptor and we need to get rid of the header
            cmd += f" <(tail -n +2 {filename})"

        cmd += \
        f""" \\
        | cat <(echo "$header") - \\
        | pigz -c > {output.corpus}

        # Stats
        WC1=$(zcat {output.corpus} | python3 {WORKFLOW}/utils/cut_header.py -f src_text | tail -n +2 | wc -wl | tr -s ' ')
        WC2=$(zcat {output.corpus} | python3 {WORKFLOW}/utils/cut_header.py -f trg_text | tail -n +2 | wc -w)

        echo "{LANG1}-{LANG2} filtered" > {output.stats}
        echo "File size: $(du -h {output.corpus} | cut -f 1)" >> {output.stats}
        echo "Sentence pairs: $(echo $WC1 | cut -d ' ' -f 1)" >> {output.stats}
        echo "{SRC_LANG} words: $(echo $WC1 | cut -d ' ' -f 2)" >> {output.stats}
        echo "{TRG_LANG} words: $WC2" >> {output.stats}
        """

        shell(cmd)


rule tmx:
    """
    Crate {lang1}-{lang2}.not-deduped.tmx.gz output
    :input: {lang1}-{lang2}.sent.gz, i.e. corpus after filtering & sorting
    :output: corpus after filtering in TMX format
    """
    input:
        rules.sents.output.corpus,
    output:
        f"{PERMANENT}/{LANG1}-{LANG2}.not-deduped.tmx.gz",
    shell:
        """
        zcat {input} \
            | {PROFILING} python3 {WORKFLOW}/bitextor_build_TMX.py --no-delete-seg --lang1 {SRC_LANG} --lang2 {TRG_LANG} \
            | pigz -c > {output}
        """


rule deduped_tmx:
    """
    Create deduped corpus, TMX and txt versions
    :input: {lang1}-{lang2}.sent.gz, i.e. corpus after filtering & sorting
    :output.tmx: TMX deduplicated corpus
    :output.txt: txt dedpulicated corpus
    :output.stats: the corresponding stats file (calculated from dedup txt version)
    """
    input:
        rules.sents.output.corpus,
    output:
        tmx=f"{PERMANENT}/{LANG1}-{LANG2}.deduped.tmx.gz",
        txt=f"{PERMANENT}/{LANG1}-{LANG2}.deduped.txt.gz",
        stats=f"{PERMANENT}/{LANG1}-{LANG2}.stats.deduped",
    params:
        dedup_fields="bifixer_hash" if BIFIXER else "src_text,trg_text"
    shell:
        """
        zcat {input} \
            | {PROFILING} python3 {WORKFLOW}/bitextor_build_TMX.py --no-delete-seg --lang1 {SRC_LANG} --lang2 {TRG_LANG} \
                --dedup {params.dedup_fields} -f {output.txt} \
            | pigz -c > {output.tmx}

        # Stats
        WC1=$(zcat {output.txt} | python3 {WORKFLOW}/utils/cut_header.py -f src_text | tail -n +2 | wc -wl | tr -s ' ')
        WC2=$(zcat {output.txt} | python3 {WORKFLOW}/utils/cut_header.py -f trg_text | tail -n +2 | wc -w)

        echo "{LANG1}-{LANG2} deduped txt" > {output.stats}
        echo "File size: $(du -h {output.txt} | cut -f 1)" >> {output.stats}
        echo "Sentence pairs: $(echo $WC1 | cut -d ' ' -f 1)" >> {output.stats}
        echo "{SRC_LANG} words: $(echo $WC1 | cut -d ' ' -f 2)" >> {output.stats}
        echo "{TRG_LANG} words: $WC2" >> {output.stats}
        """


rule roam_tmx:
    """
    Use biroamer to created ROAMed version of the corpus
    :input.tmx: TMX file, both non-deduped and deduped if dedup=true
    :output: ROAMed TMX file, both non-deduped and deduped if dedup=true
    """
    input:
        f"{PERMANENT}/{LANG1}-{LANG2}.{{deduped}}.tmx.gz",
    output:
        f"{PERMANENT}/{LANG1}-{LANG2}.{{deduped}}.roamed.tmx.gz",
    params:
        tokenizer_l1=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, SRC_LANG), '-t "{}"'),
        tokenizer_l2=lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, TRG_LANG), '-T "{}"'),
        mix_files=temp(f"{TMPDIR}/biroamer_mix_files.txt"),
    shell:
        """
        mix_files=""
        nolines=$(cat {BIROAMER_MIX_FILES} | wc -l)

        if [[ "$nolines" != "0" ]]; then
            cat {BIROAMER_MIX_FILES} > {params.mix_files}

            mix_files="-m {params.mix_files}"
        fi

        zcat {input} \
            | {PROFILING} biroamer {params.tokenizer_l1} {params.tokenizer_l2} \
                {BIROAMER_OMIT} {BIROAMER_ALIGNMENT_CORPUS} $mix_files {SRC_LANG} {TRG_LANG} \
            | pigz -c > {output}
        """
