import os
import sys
import requests
import subprocess

sys.path.append(os.path.dirname(os.path.abspath(f"{config['bitextor']}/utils")))
from utils.args import validate_args
from utils.common import open_xz_or_gzip_or_plain, get_all_ppids, \
    snake_no_more_race_get_pgid, snake_no_more_race_get, snake_no_more_race_set
from utils.non_generic import *

config = validate_args(config)

#################################################################
# BASIC PARAMETERS
BITEXTOR = config["bitextor"]
DATADIR = config["dataDir"]
TRANSIENT = config["transientDir"]
PERMANENT = config["permanentDir"]
TMPDIR = config["tempDir"]
#WORKDIR = os.getcwd()
#SNAKEMAKEDIR = os.path.dirname(workflow.snakefile)

LANGS = set()
LANG1 = ""
LANG2 = ""

if "langs" in config:
    LANGS = set(config["langs"])
if "lang1" in config:
    LANG1 = config["lang1"]
    LANGS.add(LANG1)
if "lang2" in config:
    LANG2 = config["lang2"]
    LANGS.add(LANG2)

PROFILING = ""
if "profiling" in config and config["profiling"]:
    PROFILINE = "/usr/bin/time -v"

#################################################################
# CRAWLING
CRAWLTARGET = ""
TLD_CRAWL = ""
USERAGENT = ""
CRAWLSIZELIMIT = ""
CRAWLTIMELIMIT = ""
CRAWLWAIT = ""
CRAWLPAGELIMIT = ""
CRAWLFILETYPES = "-f html,pdf"
CRAWLJOBS = "-j 2"
CRAWLTIMEOUT = ""
CRAWLDUMPARGS = ""
CONTINUECRAWL = ""
HERITRIXPATH = ""
HERITRIXURL = "https://localhost:8443"
HERITRIXUSER = "admin:admin"
CRAWLMAXFOLDERTREEDEPTH = ""
CRAWLSCOUTSTEPS = ""
CRAWLBLACKLISTURL = "['wordpress','blogspot','facebook','google','wikipedia','youtube','perehodi','twitter','instagram']"
CRAWLPREFIXFILTER = "['mailto:']"

if "crawler" in config:
    CRAWLTARGET = config["crawler"]

    if CRAWLTARGET == "linguacrawl":
        CRAWLFILETYPES = "(text/html|application/pdf)"
        CRAWLJOBS = "2"
if "crawlTLD" in config and config["crawlTLD"]:
    if CRAWLTARGET == "linguacrawl":
        TLD_CRAWL = config["crawlTLD"]
    else:
        TLD_CRAWL = "-D"
if "crawlerUserAgent" in config:
    if CRAWLTARGET == "linguacrawl":
        USERAGENT = config["crawlerUserAgent"]
    else:
        USERAGENT = f'-a "{config["crawlerUserAgent"]}"'
if "crawlSizeLimit" in config:
    if CRAWLTARGET == "linguacrawl":
        CRAWLSIZELIMIT = config["crawlSizeLimit"]
    else:
        CRAWLSIZELIMIT = f'-s {config["crawlSizeLimit"]}'
if "crawlTimeLimit" in config:
    if CRAWLTARGET in ("heritrix", "linguacrawl"):
        CRAWLTIMELIMIT = config["crawlTimeLimit"]
    else:
        CRAWLTIMELIMIT = f'-t {config["crawlTimeLimit"]}'
if "crawlWait" in config:
    if CRAWLTARGET == "linguacrawl":
        CRAWLWAIT = config["crawlWait"]
    else:
        CRAWLWAIT = f'--wait {config["crawlWait"]}'
if "crawlFileTypes" in config:
    if CRAWLTARGET == "linguacrawl":
        CRAWLFILETYPES = f'({CRAWLFILETYPES.replace(",", "|")})'
    else:
        CRAWLFILETYPES = f'-f {config["crawlFileTypes"]}'
if "crawlerNumThreads" in config:
    if CRAWLTARGET == "linguacrawl":
        CRAWLJOBS = config["crawlerNumThreads"]
    else:
        CRAWLJOBS = f'-j {config["crawlerNumThreads"]}'
if "crawlerConnectionTimeout" in config:
    if CRAWLTARGET == "linguacrawl":
        CRAWLTIMEOUT = config["crawlerConnectionTimeout"]
    else:
        CRAWLTIMEOUT = f'-o {config["crawlerConnectionTimeout"]}'
if "dumpCurrentCrawl" in config:
    if CRAWLTARGET == "linguacrawl":
        CRAWLDUMPARGS = config["dumpCurrentCrawl"]
    else:
        CRAWLDUMPARGS = f'-d {config["dumpCurrentCrawl"]}'
if "resumePreviousCrawl" in config:
    CONTINUECRAWL = f'-l {config["resumePreviousCrawl"]}'
if "heritrixPath" in config:
    HERITRIXPATH = config["heritrixPath"]
if "heritrixUrl" in config:
    HERITRIXURL = config["heritrixUrl"]
if "heritrixUser" in config:
    HERITRIXUSER = config["heritrixUser"]
if "crawlMaxFolderTreeDepth" in config:
    CRAWLMAXFOLDERTREEDEPTH = config["crawlMaxFolderTreeDepth"]
if "crawlScoutSteps" in config:
    CRAWLSCOUTSTEPS = config["crawlScoutSteps"]
if "crawlBlackListURL" in config:
    CRAWLBLACKLISTURL = str(config["crawlBlackListURL"])
if "crawlPrefixFilter" in config:
    CRAWLPREFIXFILTER = str(config["crawlPrefixFilter"])

#################################################################
# PREPROCESS
PPROC = "warc2text"
PPROC_FILES = ["text.gz", "url.gz", "mime.gz"]
TEXT_FILE = "text.gz"
HTML_FILE = ""
if "writeHTML" in config and config["writeHTML"]:
    HTML_FILE = "html.gz"
    PPROC_FILES.append("html.gz")

if "preprocessor" in config and config["preprocessor"] == "warc2preprocess":
    PPROC = "w2p"
    PPROC_FILES = ["plain_text.gz", "url.gz", "mime.gz", "normalized_html.gz", "deboilerplate_html.gz"]
    TEXT_FILE = "plain_text.gz"
    HTML_FILE = "deboilerplate_html.gz"

SHARDS = config["shards"]
BATCHES = config["batches"]

CLEANHTML = ""
FTFY = ""
LANGID = "cld2"
PARSER = ""
BOILERPIPE = ""
PDFEXTRACT = ""
HTML5LIB = ""

if "cleanHTML" in config and config["cleanHTML"]:
    CLEANHTML = "--cleanhtml"
if "ftfy" in config and config["ftfy"]:
    FTFY = "--ftfy"
if "langID" in config:
    LANGID = config["langID"]
if "parser" in config:
    PARSER = f"--parser {config['parser']}"
if "boilerpipeCleaning" in config and config["boilerpipeCleaning"]==True:
    BOILERPIPE = "--boilerpipe"
if "PDFextract" in config and config["PDFextract"]:
    PDFEXTRACT = "--pdfextract "
    if "PDFextract_configfile" in config and config["PDFextract_configfile"]:
        PDFEXTRACT += " --pe_configfile " + config["PDFextract_configfile"]
    if "PDFextract_sentence_join_path" in config and config["PDFextract_sentence_join_path"]:
        PDFEXTRACT += " --sentence_join_path " + config["PDFextract_sentence_join_path"]
    if "PDFextract_kenlm_path" in config and config["PDFextract_kenlm_path"]:
        PDFEXTRACT += " --kenlm_path " + config["PDFextract_kenlm_path"]
if "html5lib" in config and config["html5lib"]:
    HTML5LIB = "--html5lib"

# sentence splitting and tokenisation
SENTTOKS = {} if not "sentenceSplitters" in config else config["sentenceSplitters"]
CUSTOMNBPS = {} if not "customNBPs" in config else config["customNBPs"]
WORDTOKS = {} if not "wordTokenizers" in config else config["wordTokenizers"]
MORPHTOKS = {} if not "morphologicalAnalysers" in config else config["morphologicalAnalysers"]

# Concrete tokenizers and morph. tokenizers
if WORDTOKS:
    WORDTOK1 = get_lang_or_default(WORDTOKS, LANG1)
    WORDTOK2 = get_lang_or_default(WORDTOKS, LANG2)
else:
    WORDTOK1, WORDTOK2 = f"{BITEXTOR}/preprocess/moses/tokenizer/tokenizer.perl -q -b -a -l {LANG1}", f"{BITEXTOR}/preprocess/moses/tokenizer/tokenizer.perl -q -b -a -l {LANG2}"
MORPHTOK1 = get_lang_or_default(MORPHTOKS, LANG1)
MORPHTOK2 = get_lang_or_default(MORPHTOKS, LANG2)

PRUNE_THRESHOLD = f"--prune {config['pruneThreshold']}"
PRUNE_TYPE = f"--prune-type {config['pruneType']}"

#################################################################
# DOCALIGN
DOCALIGN = config["documentAligner"]
# mt
if DOCALIGN == "externalMT":
    MT_COMMAND = config['alignerCmd']
else:
    MT_COMMAND = None
SRC_LANG = LANG1
TRG_LANG = LANG2
if "translationDirection" in config and config["translationDirection"] == f'{LANG2}2{LANG1}':
    SRC_LANG = LANG2
    TRG_LANG = LANG1

DOC_THRESHOLD = 0.1
if "documentAlignerThreshold" in config:
    DOC_THRESHOLD = config["documentAlignerThreshold"]

# dic-based docalign
# snakemake/include/dic-docsegalign
#################################################################
# dic
# snakemake/include/dic-generation

try:
    os.makedirs(TMPDIR)
except:
    pass

DIC=None
DIC_TRAINING=None

if "dic" in config:
    DIC = config["dic"]
    DIC_TRAINING = DIC
    dic_training_file_path = f"{TMPDIR}/dic_training.value"
    dic_training_file = False

    value = snake_no_more_race_get(dic_training_file_path)

    if value is not None:
        DIC_TRAINING = value
        dic_training_file = True

    if (not dic_training_file and os.path.isfile(DIC) and ('bicleanerCorpusTrainingPrefix' in config and 'initCorpusTrainingPrefix' in config)):
        DIC_TRAINING += ".generated"

    # Create file with DIC_TRAINING value in order to avoid that, when DAG is reevaluated, changes the value of the variable
    snake_no_more_race_set(dic_training_file_path, DIC_TRAINING)

#################################################################
# SEGALIGN
SEGALIGN = config["sentenceAligner"]
# bleualign
SEGALIGN_THRESHOLD = 0.0
if "sentenceAlignerThreshold" in config:
    SEGALIGN_THRESHOLD=config["sentenceAlignerThreshold"]
# hunalign
# snakemake/include/dic-docsegalign
#################################################################
# CLEANING
FIELDS = ['url1','url2','seg1','seg2','aligner']
DEFERRED = ""
MMHSUM_PATH = ""
DEFERRED_FIELDS = []
BIFIXER = False
BIFIXER_FIELDS = []
BIFIXER_DEFERRED_COLS = ""
AGGRESSIVE_DEDUP = "--aggressive_dedup"
BICLEANER = False
BICLEANER_MODEL = ""
BICLEANER_FIELDS = []
BICLEANER_THRESHOLD = 0.0
ELRC = False
ELRC_FIELDS = []
TMX = False
DEDUPED = False
BIROAMER = False
# TODO: add rawCorpus option to generate lang1-lang2.raw.gz
OUTPUT_FILES = ["sent", "raw"]

if 'deferred' in config and config['deferred']:
    DEFERRED = "--print-sent-hash"
    MMHSUM_PATH = f"{BITEXTOR}/preprocess/bin/mmhsum"
    DEFERRED_FIELDS = ['checksum1','checksum2']
    BIFIXER_DEFERRED_COLS = "--sdeferredcol 6 --tdeferredcol 7"
if 'bifixer' in config and config['bifixer']:
    BIFIXER = True
    BIFIXER_FIELDS = ['bifixerhash','bifixerscore']
if 'aggressiveDedup' in config and not config['aggressiveDedup']:
    AGGRESSIVE_DEDUP = ''
if 'bicleaner' in config:
    BICLEANER = True
    BICLEANER_MODEL = config['bicleaner']
    BICLEANER_FIELDS = ['bicleaner']
if 'bicleanerThreshold' in config:
    BICLEANER_THRESHOLD = config['bicleanerThreshold']
if 'elrc' in config and config['elrc']:
    ELRC = True
    ELRC_FIELDS = ['lengthratio','numTokensSL','numTokensTL']
if 'tmx' in config and config['tmx']:
    TMX = True
    OUTPUT_FILES.append('not-deduped.tmx')
if 'deduped' in config and config['deduped']:
    DEDUPED = True
    OUTPUT_FILES.append('deduped.tmx')
    OUTPUT_FILES.append('deduped.txt')
if 'biroamer' in config and config['biroamer']:
    BIROAMER = True

    if 'deduped.tmx' in OUTPUT_FILES:
        OUTPUT_FILES.append('deduped-roamed.tmx')
    else:
        OUTPUT_FILES.append('not-deduped-roamed.tmx')

BEFORE_ELRC_FIELDS = FIELDS + DEFERRED_FIELDS + BIFIXER_FIELDS + BICLEANER_FIELDS
TMX_FIELDS = BEFORE_ELRC_FIELDS + ELRC_FIELDS

FILTER_SORT_FIELDS="-k3,4"
TMX_DEDUP_FIELDS = 'seg1,seg2'
if 'bifixerhash' in BEFORE_ELRC_FIELDS:
    i = BEFORE_ELRC_FIELDS.index('bifixerhash')
    i = i + 1 # sort counts from 1, not 0
    FILTER_SORT_FIELDS = f'-k{i},{i} -k{i+1},{i+1}nr'
    TMX_DEDUP_FIELDS = 'bifixerhash'

BEFORE_ELRC_FIELDS = ','.join(BEFORE_ELRC_FIELDS)
TMX_FIELDS = ','.join(TMX_FIELDS)
#################################################################
# DATASOURCES
HOSTS = set()
WARCS = set()

if "warcs" in config:
    WARCS = WARCS.union(config["warcs"])

if "hosts" in config:
    HOSTS = HOSTS.union(config["hosts"])

if "hostsFile" in config:
    with open_xz_or_gzip_or_plain(config["hostsFile"]) as f:
        for line in f:
            HOSTS.add(line.strip())

if "warcsFile" in config:
    with open_xz_or_gzip_or_plain(config["warcsFile"]) as f:
        for line in f:
            WARCS.add(line.strip())

DOMAIN_2_HOSTS = create_domain_key_2_host_map(HOSTS)
# process WARCs individually
TARGET_2_WARCS = {f'{k}': v for k,v in enumerate(WARCS)}
# group crawled hosts by domains
TARGET_2_WARCS.update(dict([(domain, [f'{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz' for host in hosts]) for (domain, hosts) in DOMAIN_2_HOSTS.items()]))

TARGETS = TARGET_2_WARCS.keys()

#################################################################
### WORKFLOW EXECUTION ##########################################
THREADS = {'split': 1, 'translate': 1, 'tokenise_src': 1, 'tokenise_trg': 1, 'docalign': 1, 'segalign': 1, 'bifixer': 1, 'bicleaner': 1, 'sents': 1}
if 'parallelWorkers' in config:
    for k in config['parallelWorkers']:
        THREADS[k] = config['parallelWorkers'][k]

OUTPUT = []
UNTIL = config['until'] if 'until' in config else ''
if 'until' not in config:
    OUTPUT = expand('{permanent}/{lang1}-{lang2}.{output_file}.gz', permanent=PERMANENT, target=TARGETS, lang1=LANG1, lang2=LANG2, output_file=OUTPUT_FILES)
    # this has to be added, because tokenisation rules are the last rules that don't need both shards
    # otherwise snakemake waites for both shards be completed to continue
    OUTPUT.append(f'{DATADIR}/shards/05.tokenise.{TRG_LANG}')
    OUTPUT.append(f'{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}')
elif UNTIL == 'crawl':
    if CRAWLTARGET == "linguacrawl":
        OUTPUT.append(f'{DATADIR}/warc/linguacrawl.finished')
    else:
        for domain, hosts in DOMAIN_2_HOSTS.items():
            for host in hosts:
                OUTPUT.append(f'{DATADIR}/warc/{host}/{CRAWLTARGET}.warc.gz')
elif UNTIL == 'preprocess':
    OUTPUT = expand('{datadir}/preprocess/{target}/{pproc}/{lang}/{pproc_file}', datadir=DATADIR, target=TARGETS, pproc=PPROC, langs=LANGS, pproc_file=PPROC_FILES)
elif UNTIL == 'shard':
    OUTPUT = expand('{datadir}/shards/02.batches.{lang}', datadir=DATADIR, lang=LANGS)
elif UNTIL == 'split':
    OUTPUT = expand('{datadir}/shards/03.split.{lang}', datadir=DATADIR, lang=LANGS)
elif UNTIL == 'translate':
    OUTPUT = f'{DATADIR}/shards/04.translate.{SRC_LANG}2{TRG_LANG}'
elif UNTIL == 'tokenise_trg':
    # TODO: decide where to put '0X_step' files (output of aggregate rules)
    OUTPUT = f'{DATADIR}/shards/05.tokenise.{TRG_LANG}'
elif UNTIL == 'tokenise_src':
    OUTPUT = f'{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}'
else:
    OUTPUT.append(f'{DATADIR}/shards/05.tokenise.{TRG_LANG}')
    OUTPUT.append(f'{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}')
    if UNTIL == 'docalign':
        OUTPUT.append(f'{TRANSIENT}/06_01.docalign.{SRC_LANG}_{TRG_LANG}')
    elif UNTIL == 'segalign':
        OUTPUT.append(f'{TRANSIENT}/06_02.segalign.{LANG1}_{LANG2}')
    elif UNTIL == 'bifixer':
        OUTPUT.append(f'{TRANSIENT}/07_01.bifixer.{LANG1}_{LANG2}')
    elif UNTIL == 'bicleaner':
        OUTPUT.append(f'{TRANSIENT}/07_02.bicleaner.{LANG1}_{LANG2}')
    elif UNTIL == 'filter':
        OUTPUT.append(f'{TRANSIENT}/07_03.filter.{LANG1}_{LANG2}')

shell.prefix("set -euo pipefail;")

#################################################################
### FINAL OUTPUTS ###############################################
rule all:
    input: OUTPUT

#################################################################
### INCLUDE #####################################################
include: "rules/dicgeneration.smk"
include: "rules/dictdocsegalign.smk"

#################################################################
### CRAWLING ####################################################
rule creepy_download:
    params: url="http://{target}", folder=f"{DATADIR}/warc/{{target}}"
    output: f'{DATADIR}/warc/{{target}}/creepy.warc.gz'
    shell: '''
        mkdir -p {params.folder} {TMPDIR}
        {PROFILING} python3 {BITEXTOR}/bitextor-creepy.py {TLD_CRAWL} {CRAWLSIZELIMIT} {CRAWLTIMELIMIT} {CRAWLWAIT} {CRAWLJOBS} {CRAWLTIMEOUT} {CRAWLDUMPARGS} {CONTINUECRAWL} {USERAGENT} {params.url} > {output}
        '''

rule httrack_download:
    params: url="http://{target}", folder=f"{DATADIR}/warc/{{target}}"
    output: f'{DATADIR}/warc/{{target}}/httrack.warc.gz'
    shell: '''
        mkdir -p {params.folder} {TMPDIR}
        echo hostname=$HOSTNAME
        DIRNAME=$(mktemp -d {TMPDIR}/downloaded.{wildcards.target}.XXXXXX)
        {PROFILING} {BITEXTOR}/bitextor-httrack.py --url {params.url} --output-path $DIRNAME {CRAWLTIMELIMIT} {CRAWLPAGELIMIT} {USERAGENT} {CRAWLWAIT}
        {BITEXTOR}/bitextor-webdir2warc.sh $DIRNAME > {output}
        rm -rf $DIRNAME
        '''

rule wget_download:
    params: url="http://{target}", folder=f"{DATADIR}/warc/{{target}}"
    output: f'{DATADIR}/warc/{{target}}/wget.warc.gz'
    shell: '''
        mkdir -p {params.folder} {TMPDIR}
        echo hostname=$HOSTNAME
        DIRNAME=$(mktemp -d "{TMPDIR}/downloaded.{wildcards.target}.XXXXXX")
        {PROFILING} {BITEXTOR}/bitextor-wget.py --url {params.url} --output-path $DIRNAME {CRAWLTIMELIMIT} {USERAGENT} {CRAWLFILETYPES} {CRAWLWAIT} --warc {output}
        rm -rf $DIRNAME
        '''

rule heritrix_download:
    params: url="http://{target}", folder=f"{DATADIR}/warc/{{target}}"
    output: f'{DATADIR}/warc/{{target}}/heritrix.warc.gz'
    run:
        # Check the connection
        url = params.url
        connection_error = False
        connection = None

        for check in range(2):
            try:
                connection = requests.get(url, timeout=15)
            except requests.exceptions.ConnectTimeout:
                if check:
                    connection_error = True
                else:
                    url = f"https{url[4:]}"
            except:
                if check:
                    connection_error = True
                    sys.stderr.write("WARNING: error connecting: ")
                    sys.stderr.write(str(sys.exc_info()[0]) + "\n")

        if not connection_error:
            params.url = url

            # Execute heritrix
            shell('''
                    mkdir -p {params.folder} {TMPDIR}
                    echo hostname=$HOSTNAME
                    if [ "$(ps aux | grep -i Heritrix | grep -v grep)" == "" ]
                        then {HERITRIXPATH}/bin/heritrix -a {HERITRIXUSER}
                    fi
                    curl -v -d "action=teardown" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                    curl -v -d "createpath={wildcards.target}&action=create" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine
                    DIRNAME=$(mktemp -d "{TMPDIR}/downloaded.{wildcards.target}.XXXXXX")
                    cat {BITEXTOR}/crawler-beans.cxml | sed "s@http://example.example/example@{params.url}@g" > $DIRNAME/my-crawler-beans.cxml
                    curl -v -T $DIRNAME/my-crawler-beans.cxml -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}/jobdir/crawler-beans.cxml
                    curl -v -d "action=build" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                    curl -v -d "action=launch&checkpoint=latest" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                    sleep 2
                    curl -v -d "action=unpause" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                    RUNTIME=0
                    sleep 15
                    while [ -f {HERITRIXPATH}/jobs/{wildcards.target}/latest/warcs/*warc.gz.open ]
                    do
                        sleep 5
                        RUNTIME=$((RUNTIME+5))
                        if [ "{CRAWLTIMELIMIT}" != "" ]
                        then
                            if [ $RUNTIME -gt "{CRAWLTIMELIMIT}" ]
                            then
                                echo "Crawling time limit reached"
                                curl -v -d "action=pause" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                                curl -v -d "action=checkpoint" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                                curl -v -d "action=terminate" -k -u {HERITRIXUSER} --anyauth --location {HERITRIXURL}/engine/job/{wildcards.target}
                            fi
                        fi
                    done
                    echo "Job {wildcards.target} finished!"
                    cat {HERITRIXPATH}/jobs/{wildcards.target}/*/warcs/*warc.gz > {output}
                ''')
        else:
            # Create empty warc file
            with open(output[0], 'wb') as f_out:
                f_out.close()

# Is needed a checkpoint because we do not know the resulted warcs since we let the user decide if he wants to
#  either join all the warcs (it loses important information like the source of the warcs) or not
# Manual stopping: kill -s sigint `ps aux | grep bin/linguacrawl | grep -v grep | awk '{print $2}'`
checkpoint linguacrawl_download:
    params:
        folder=f"{DATADIR}/warc/linguacrawl",
        yaml_file=f"{TMPDIR}/linguacrawl.yaml",
    output: f'{DATADIR}/warc/linguacrawl.finished'
    run:
        global HOSTS
        shell('''
                mkdir -p {params.folder} {TMPDIR}
                echo hostname=$HOSTNAME
                touch {params.yaml_file}
                cat < /dev/null > {params.yaml_file}
            ''')

        yaml_content = ""
        useragent = USERAGENT
        langs = "','".join(LANGS)
        hosts = "','".join(HOSTS)

        if not useragent:
            useragent = "Mozilla/5.0 (compatible; Bitextor/8 +https://github.com/bitextor/bitextor)"

        # Mandatory
        yaml_content += f"user_agent: '{useragent}'\n"
        yaml_content += f"langs_of_interest: ['{langs}']\n"
        yaml_content += f"output_dir: '{params.folder}'\n"
        # Optional (the argument parser will not complain, but the crawler will crash in some cases)
        yaml_content += f"crawl_delay: {CRAWLWAIT}\n" if CRAWLWAIT else ""
        yaml_content += f"max_time_per_site: {CRAWLTIMELIMIT}\n" if CRAWLTIMELIMIT else ""
        yaml_content += f"max_size_per_site: {CRAWLSIZELIMIT}\n" if CRAWLSIZELIMIT else ""
        yaml_content += f"connection_timeout: {CRAWLTIMEOUT}\n" if CRAWLTIMEOUT else "connection_timeout: 10\n"
        yaml_content += f"max_jobs: {CRAWLJOBS}\n" if CRAWLJOBS else ""
        yaml_content += f"resume_crawling: True\n" if CONTINUECRAWL else ""
        yaml_content += f"seed_urls: ['{hosts}']\n"
        #yaml_content += f"seed_urls_from_file: \n" # HOSTS contain all hosts if hosts defined either with or without file
        yaml_content += f"prefix_filter: {CRAWLPREFIXFILTER}\n"
        yaml_content += f"verbose: True\n" if CRAWLDUMPARGS else "verbose: False\n"
        yaml_content += f"accepted_content: '{CRAWLFILETYPES}'\n"
        yaml_content += f"max_folder_tree_depth: {CRAWLMAXFOLDERTREEDEPTH}\n" if CRAWLMAXFOLDERTREEDEPTH else "max_folder_tree_depth: 20\n"
        yaml_content += f"max_attempts: 3\n"
        yaml_content += f"scout_steps: {CRAWLSCOUTSTEPS}\n" if CRAWLSCOUTSTEPS else "scout_steps: 200\n"
        yaml_content += f"min_langs_in_site: 2\n"
        yaml_content += f"url_blacklist: {CRAWLBLACKLISTURL}\n"

        if LANG1 or LANG2:
            yaml_content += f"mandatory_lang: '{LANG1}'\n" if LANG1 else f"mandatory_lang: '{LANG2}'\n"
            yaml_content += f"min_percent_mandatory_lang: 10\n"
        
        #tld = LANGS.union([params.url.split('.')[-1]]).union(TLD_CRAWL)
        tld = LANGS.union([url.split('.')[-1] for url in HOSTS]).union(TLD_CRAWL)
        tld = "','".join(tld)

        yaml_content += f"accepted_tlds: ['{tld}']\n"

        fdescriptor = open(params.yaml_file, "w")
        fdescriptor.write(yaml_content)
        fdescriptor.flush()
        fdescriptor.close()

        sys.stderr.write(f"Configuration file stored: {params.yaml_file}\n")

        shell("{PROFILING} linguacrawl {params.yaml_file}")

        all_files = subprocess.Popen(('ls', params.folder), stdout=subprocess.PIPE)
        try:
            # Process the resulted warcs
            warcs = subprocess.check_output(('grep', '[.]warc[.]gz$'), stdin=all_files.stdout)  # It will throw an exception if no warcs were downloaded
            all_files.wait()

            warcs = warcs.decode("utf-8").split("\n")
            warcs = list(filter(lambda warc: len(warc) != 0, warcs))
            warcs = list(map(lambda warc: f"{params.folder}/{warc}", warcs))

            if ('crawlCat' not in config or not config['crawlCat']):
                warcs = "\n".join(warcs)
                shell(f'echo -e "{warcs}" > {{output[0]}}')
            else:
                if 'crawlCatMaxSize' not in config or config['crawlCatMaxSize'] <= 0:
                    warcs = " ".join(warcs)
                    shell(f'''
                        cat {warcs} > {params.folder}/linguacrawl.warc.gz
                        echo "{params.folder}/linguacrawl.warc.gz" > {{output[0]}}
                        ''')
                else:
                    # Improve the number of preprocess rules that are executed
                    max_limit = config['crawlCatMaxSize']
                    current = 0
                    blocks = 0
                    files = [f"{params.folder}/linguacrawl{blocks}.warc.gz"]

                    for warc in warcs:
                        if current > max_limit:
                            current = 0
                            blocks += 1
                            files.append(f"{params.folder}/linguacrawl{blocks}.warc.gz")

                        shell(f"cat {warc} >> {files[-1]}")

                        du_command = subprocess.Popen(('du', '-b', warc), stdout=subprocess.PIPE)

                        try:
                            du_warc = subprocess.check_output(('awk', '{print $1}'), stdin=du_command.stdout)
                            du_command.wait()

                            current += int(du_warc)
                        except:
                            sys.stderr.write(f"WARNING: could not retrieve the size of {warc}")

                    files = "\n".join(files)
                    shell(f'echo -e "{files}" > {{output[0]}}')
        except:
            # No warcs were downloaded: error or warning
            if len(WARCS) == 0:
                sys.stderr.write("ERROR: could not find any file after crawling\n")
                sys.exit(1)
            sys.stderr.write("WARNING: could not find any file after crawling\n")
            shell(f"touch {output}")

#################################################################
### PREPROCESS ##################################################

# pproc_output = {}
# for pproc_file in PPROC_FILES:
#     name = pproc_file.split('.')[0]
#     for lang in LANGS:
#         pproc_output[f"{lang}_{name}"] = f"{DATADIR}/preprocess/{{target}}/{PPROC}/{lang}/{pproc_file}"

def get_linguacrawl_warcs():
    warcs = []
    with checkpoints.linguacrawl_download.get().output[0].open() as f:
        for line in f:
            warcs.append(line.strip())
    return warcs

def get_warcs_names(wildcards):
    if CRAWLTARGET == "linguacrawl":
        warcs = get_linguacrawl_warcs()
        return warcs, list(map(lambda warc: warc.split("/")[-1], warcs))

    return TARGET_2_WARCS[wildcards.target].split("/")[-1]

def get_pproc_input(wildcards):
    if (CRAWLTARGET == "linguacrawl" and len(HOSTS) != 0):
        try:
            # Retrieve warcs names, that in first instance will fail because linguacrawl has not been executed yet
            warcs, warcs_targets = get_warcs_names(wildcards)
        except snakemake.exceptions.IncompleteCheckpointException:
            # We have received a provided warc (likely)
            try:
                # Check if the requested pproc input is a provided warc
                return TARGET_2_WARCS[wildcards.target]
            except:
                # Should not happen, but force exception again in case something went wrong (it might continue if linguacrawl finished)
                warcs, warcs_targets = get_warcs_names(wildcards)

        for warc in warcs:
            if not os.path.exists(warc):
                warcs_targets.remove(warc.split("/")[-1])
                sys.stderr.write(f"WARNING: non-existent WARC ({warc}) detected and fixed\n")

        try:
            # Check if the requested pproc input is a provided warc
            return TARGET_2_WARCS[wildcards.target]
        except:
            pass

        return warcs[warcs_targets.index(wildcards.target)]

    return TARGET_2_WARCS[wildcards.target]

rule warc2preprocess:
    #input: lambda wildcards: TARGET_2_WARCS[wildcards.target]
    input: get_pproc_input
    output:expand("{data}/preprocess/{{target}}/w2p/{lang}/{pproc_file}", data=DATADIR, lang=LANGS, pproc_file=PPROC_FILES)
    threads: 2
    params: folder=f'{DATADIR}/preprocess/{{target}}/w2p', pproclangs=",".join(LANGS)
    shell: '''
        mkdir -p {params.folder}
        cat {input} | \
            {PROFILING} {BITEXTOR}/bitextor-warc2htmlwarc.py {CLEANHTML} {FTFY} {PDFEXTRACT} --disable-output-gzip | \
            {PROFILING} {BITEXTOR}/bitextor-warc2preprocess.py --input - --langs {params.pproclangs} \
                --compression gz --langid {LANGID} {BOILERPIPE} {HTML5LIB} {PARSER} --output-dir {params.folder}
        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/plain_text.gz ]; then
                >&2 echo "WARNING: no \'$lang\' data found in {wildcards.target}. Creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{plain_text,mime,url,normalized_html,deboilerplate_html}}
                gzip {params.folder}/$lang/{{plain_text,mime,url,normalized_html,deboilerplate_html}}
            fi
        done
    '''

rule warc2text:
    input: get_pproc_input
    output: expand("{data}/preprocess/{{target}}/warc2text/{lang}/{pproc_file}", data=DATADIR, lang=LANGS, pproc_file=PPROC_FILES)
    params:
        folder = f'{DATADIR}/preprocess/{{target}}/warc2text',
        f = ','.join([f.strip(".gz") for f in PPROC_FILES])
    shell: '''
        mkdir -p {params.folder}
        {PROFILING} {BITEXTOR}/warc2text/bin/warc2text -o {params.folder} -s -f {params.f} {input}
        for lang in {LANGS}; do
            if [ ! -f {params.folder}/$lang/text.gz ]; then
                >&2 echo "WARNING: no \'$lang\' data found in {wildcards.target}. Creating empty files instead"
                mkdir -p {params.folder}/$lang
                touch {params.folder}/$lang/{{text,mime,url}}
                gzip {params.folder}/$lang/{{text,mime,url}}
            fi
        done
        '''

def get_shard_input(lang):
    if (CRAWLTARGET == "linguacrawl" and len(HOSTS) != 0):
        warcs_path, warcs = get_warcs_names(None)
        
        for warc in warcs_path:
            if not os.path.exists(warc):
                warcs.remove(warc.split("/")[-1])
                sys.stderr.write(f"WARNING: non-existent WARC ({warc}) detected and fixed\n")

        return expand("{datadir}/preprocess/{target}/{pproc}/{{lang}}/url.gz", datadir=DATADIR, target=warcs, pproc=PPROC)

    # crawler != linguacrawl
    return expand("{datadir}/preprocess/{target}/{pproc}/{{lang}}/url.gz", datadir=DATADIR, target=TARGETS, pproc=PPROC)

# We split the behaviour in 2 functions in order to make work the linguacrawl checkpoint
#  because when the exception is thrown with get_warcs_names() due to linguacrawl checkpoint,
#  we cannot retrieve the defined warcs at the same time (snakemake does not allow to use yield),
#  so using a separate function allows us to execute in parallel linguacrawl and warcs preprocess
def get_shard_input_linguacrawl_prov_warcs(lang):
    if (CRAWLTARGET == "linguacrawl" and len(HOSTS) != 0):
        return expand("{datadir}/preprocess/{target}/{pproc}/{{lang}}/url.gz", datadir=DATADIR, target=list(range(len(WARCS))), pproc=PPROC)
    return []

# DAG will be re-evaluated after completing shard rule (because number of batches is dynamic and unknown)
checkpoint shard:
    # use url.gz as input to avoid having directories as input
    input: 
        get_shard_input_linguacrawl_prov_warcs,
        get_shard_input
    output: f'{DATADIR}/shards/02.batches.{{lang}}' # list of batches created for lang
    params:
        n = SHARDS,
        b = BATCHES,
        o_no_lang = f'{DATADIR}/shards',
        o = f'{DATADIR}/shards/{{lang}}',
        f = ','.join([f.strip(".gz") for f in PPROC_FILES]),
        empty_shard_folder = "EMPTY",
        empty_shard_batch_folder = "EMPTY/1"
    shell: '''
        ulimit -n 2048
        mkdir -p {params.o}
        rm -rf {params.o}/* # remove anything that might be left after a previous run

        binary=giashard
        if [[ "$(command -v $binary)" == "" ]]; then
            binary=~/go/bin/giashard
        fi

        {PROFILING} $binary -n {params.n} -b {params.b} -o {params.o} -f {params.f} {DATADIR}/preprocess/*/{PPROC}/{wildcards.lang}

        nofiles=$(ls -l {params.o} | wc -l)
        if [ "$nofiles" == "1" ]; then
            # No files generated
            >&2 echo "WARNING: no files generated after running giashard. Creating empty files instead"

            # Fake shards
            mkdir -p {params.o}/{params.empty_shard_batch_folder}

            touch {params.o}/{params.empty_shard_batch_folder}/empty
            touch {params.o}/{params.empty_shard_batch_folder}/{{{params.f}}}
            gzip {params.o}/{params.empty_shard_batch_folder}/{{{params.f}}}

            mylang="{SRC_LANG}"
            notmylang="{TRG_LANG}"

            if [ "{wildcards.lang}" == "{TRG_LANG}" ]; then
                mylang="{TRG_LANG}"
                notmylang="{SRC_LANG}"
            fi

            create_all=1

            while [ ! -f "{DATADIR}/shards/02.batches.$notmylang" ]; do
                if [ -f "{params.o_no_lang}/$notmylang/{params.empty_shard_batch_folder}/empty" ]; then
                    # Avoid deadlock (if both langs do not generate shards and reach this point at the same time)
                    create_all=0
                    break
                else
                    sleep 1
                fi
            done

            if [ -f "{params.o_no_lang}/$notmylang/{params.empty_shard_batch_folder}/empty" ]; then
                # In case that did not enter in the while loop, this condition might still be true
                create_all=0
            fi

            if [ "$create_all" == "1" ]; then
                rm -rf {params.o}/{params.empty_shard_folder}

                # Create the same shards that have been created in the other lang (only 1 batch)
                ls -d {params.o_no_lang}/$notmylang/* | xargs -I[] basename [] | xargs -I[] mkdir -p {params.o_no_lang}/$mylang/[]/1
                    ls -d {params.o}/*/* | xargs -I[] touch []/{{{params.f}}}
                    ls -d {params.o}/*/* | xargs -I[] gzip []/{{{params.f}}}
            fi
        fi

        ls -d {params.o}/*/* > {output}
        '''

# obtain list of batches for lang
def get_batches(lang):
    batches = []
    with checkpoints.shard.get(lang=lang).output[0].open() as f:
        for line in f:
            batches.append(line.strip())
    return batches

def apply_format(string, replace_format, replace_token="{}", replace_only_if_true=True):
    if replace_only_if_true and string or not replace_only_if_true:
        return replace_format.replace(replace_token, string)

    return string

rule split:
    input: f'{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/{TEXT_FILE}'
    params:
        splitter = lambda wildcards: apply_format(get_lang_or_default(SENTTOKS, wildcards.lang), "--sentence-splitter \"{}\""),
        customnbp = lambda wildcards: apply_format(get_customnbp(CUSTOMNBPS, wildcards.lang), "--customnbp \"{}\"")
    output: f'{DATADIR}/shards/{{lang}}/{{shard}}/{{batch}}/sentences.gz'
    threads: THREADS['split']
    shell: '''
        parallel_cmd=""
        if [ {threads} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {threads} -k"
        fi
        zcat {input} \
            | {PROFILING} ${{parallel_cmd}} {BITEXTOR}/bitextor-split.py \
                {params.splitter} {params.customnbp} \
                --langcode "{wildcards.lang}" \
                {PRUNE_THRESHOLD} {PRUNE_TYPE} \
            | pigz -c > {output}
        '''

rule aggregate_split:
    input: lambda wildcards: [f'{batch}/sentences.gz' for batch in get_batches(wildcards.lang)]
    output: f'{DATADIR}/shards/03.split.{{lang}}'
    shell: ''' echo "{input}" | tr ' ' '\n' > {output} '''

#################################################################
### DOCALIGN ####################################################
def get_align_inputs(src_lang, trg_lang):
    src_batches = get_batches(src_lang)
    trg_batches = get_batches(trg_lang)
    # each input -> (shard, (src_batch, trg_batch))
    inputs = get_mt_docalign_inputs(src_batches, trg_batches)
    return inputs

rule aggregate_matches:
    input: lambda wildcards: [f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.06_01.matches' for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)]
    output: f'{TRANSIENT}/06_01.docalign.{SRC_LANG}_{TRG_LANG}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''
# MT ############################################################

rule custom_translate:
    input:
        source=f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz'
    output: f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences_{TRG_LANG}.gz'
    threads: max(2, THREADS['translate'])
    params: THREADS['translate']
    shell: '''
        parallel_cmd=""
        if [ {params} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {params} -k"
        fi
        zcat {input.source} \
            | {PROFILING} {BITEXTOR}/preprocess/bin/b64filter {BITEXTOR}/preprocess/bin/cache ${{parallel_cmd}} {MT_COMMAND} \
            | pigz -c > {output}
        n_before=$(zcat {input.source} | base64 -d | wc -l)
        n_after=$(zcat {output} | base64 -d | wc -l)
        # echo "Check lines count: $n_before -> $n_after for {SRC_LANG}/{wildcards.shard}/{wildcards.src_batch}"
        if [ $n_before -ne $n_after ]
        then
            >&2 echo "Lines count differ: source $n_before, target $n_after"
            exit 1
        fi
        '''
        # TODO: add early stopping

if MT_COMMAND is None:
    translation_output = f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz'
else:
    translation_output = rules.custom_translate.output

rule aggregate_translate:
    input: lambda wildcards: [f'{batch}/sentences_{TRG_LANG}.gz' for batch in get_batches(SRC_LANG)]
    output: f'{DATADIR}/shards/04.translate.{SRC_LANG}2{TRG_LANG}'
    shell: ''' echo "{input}" | tr ' ' '\n' > {output} '''

rule tokenise_translated:
    input: translation_output
    output: f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/tokenised_{TRG_LANG}.gz'
    params:
        tokeniser = lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, TRG_LANG), "--word-tokenizer \"{}\""),
        lemmatizer = lambda wildcards: apply_format(get_lang_or_default(MORPHTOKS, TRG_LANG), "--morph-analyser \"{}\"")
    threads: THREADS['tokenise_src']
    shell: '''
        parallel_cmd=""
        if [ {threads} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {threads} -k"
        fi
        zcat {input} \
            | {PROFILING} ${{parallel_cmd}} {BITEXTOR}/bitextor-tokenize.py \
                {params.tokeniser} {params.lemmatizer} \
                --langcode {TRG_LANG} \
            | pigz -c > {output}
        '''

rule tokenise_target:
    input: f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/sentences.gz'
    output: f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/tokenised.gz'
    params:
        tokeniser = lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, TRG_LANG), "--word-tokenizer \"{}\""),
        lemmatizer = lambda wildcards: apply_format(get_lang_or_default(MORPHTOKS, TRG_LANG), "--morph-analyser \"{}\"")
    threads: THREADS['tokenise_trg']
    shell: '''
        parallel_cmd=""
        if [ {threads} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {threads} -k"
        fi
        zcat {input} \
            | {PROFILING} ${{parallel_cmd}} {BITEXTOR}/bitextor-tokenize.py \
                {params.tokeniser} {params.lemmatizer} \
                --langcode {TRG_LANG} \
            | pigz -c > {output}
        '''

rule tokenise_source:
    input: f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz'
    output: f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/tokenised.gz'
    params:
        tokeniser = lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, TRG_LANG), "--word-tokenizer \"{}\""),
        lemmatizer = lambda wildcards: apply_format(get_lang_or_default(MORPHTOKS, TRG_LANG), "--morph-analyser \"{}\"")
    threads: THREADS['tokenise_src']
    shell: '''
        parallel_cmd=""
        if [ {threads} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {threads} -k"
        fi
        zcat {input} \
            | {PROFILING} ${{parallel_cmd}} {BITEXTOR}/bitextor-tokenize.py \
                {params.tokeniser} {params.lemmatizer} \
                --langcode {SRC_LANG} \
            | pigz -c > {output}
        '''

rule aggregate_tokenise_target:
    input: lambda wildcards: [f'{batch}/tokenised.gz' for batch in get_batches(TRG_LANG)]
    output: f'{DATADIR}/shards/05.tokenise.{TRG_LANG}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''

rule aggregate_tokenise_source:
    input: lambda wildcards: [f'{batch}/tokenised_{TRG_LANG}.gz' for batch in get_batches(SRC_LANG)]
    output: f'{DATADIR}/shards/05.tokenise.{SRC_LANG}2{TRG_LANG}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''

rule mt_matches:
    input:
        l1=rules.tokenise_translated.output,
        l2=rules.tokenise_target.output
    output: f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.06_01.matches'
    params: folder=f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{{shard}}'
    threads: THREADS['docalign']
    shell: '''
        mkdir -p {params.folder}
        {PROFILING} {BITEXTOR}/document-aligner/bin/docalign {input.l1} {input.l2} --threshold {DOC_THRESHOLD} -j {threads} > {output}
        '''

### SEGALIGN ####################################################
rule aggregate_segalign:
    input: lambda wildcards: [f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.06_02.segalign.gz' for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)]
    output: f'{TRANSIENT}/06_02.segalign.{LANG1}_{LANG2}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''

# BLEUALIGN #####################################################
rule bleualign:
    input:
        indices=rules.mt_matches.output,
        plain1=f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/sentences.gz',
        plain2=f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/sentences.gz',
        url1=f'{DATADIR}/shards/{SRC_LANG}/{{shard}}/{{src_batch}}/url.gz',
        url2=f'{DATADIR}/shards/{TRG_LANG}/{{shard}}/{{trg_batch}}/url.gz',
        translated1=translation_output
    params:
        folder=f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{{shard}}',
        workers=THREADS['segalign']
    # in segalign rule output columns are reordered (or not) in accordance with translationDirection
    output: temp(f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{{shard}}/{SRC_LANG}{{src_batch}}_{TRG_LANG}{{trg_batch}}.06_02.segalign.gz')
    threads: max(THREADS['segalign'], 2)
    shell: '''
        mkdir -p {params.folder}
        parallel_cmd=""
        if [ {params.workers} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {params.workers} -l 1 --group"
        fi
        cat {input.indices} \
            | {BITEXTOR}/document-aligner/bin/docjoin \
                -l {input.url1} -r {input.url2} \
                -l {input.plain1} -r {input.plain2} \
                -l {input.translated1} \
            | {PROFILING} ${{parallel_cmd}} {BITEXTOR}/bleualign-cpp/bleualign_cpp {DEFERRED} --bleu-threshold {SEGALIGN_THRESHOLD} \
            | pigz -c > {output}
        '''

### FILTERING AND CLEANING ######################################

split_input_filename = '06_02.segalign'
split_input_extension = '.gz'

if SEGALIGN == "hunalign":
    split_input_filename = 'hunalign.06_02.segalign'
    split_input_extension = '.xz'

# split segalign results into balanced chunks
checkpoint split_segalign:
    input: lambda wildcards: [f'{TRANSIENT}/{SRC_LANG}_{TRG_LANG}/{shard}/{SRC_LANG}{src_batch}_{TRG_LANG}{trg_batch}.{split_input_filename}{split_input_extension}' for (shard, (src_batch, trg_batch)) in get_align_inputs(SRC_LANG, TRG_LANG)]
    output: batches=f'{TRANSIENT}/{LANG1}_{LANG2}/{LANG1}_{LANG2}.postprocessing_batches'
    params:
        size=BATCHES, # use same parameter as for shards
        folder=f'{TRANSIENT}/{LANG1}_{LANG2}/{split_input_filename}'
    run:
        # We need to make this check in order to avoid the pipeline to break in the case that no input was received (e.g. no common shards)
        # It is needed to run this piece of code directly in Python and not in bash because Snakemake attempts to replace {input[0]} before
        #  running the script, so it fails even before we are able to check if the input is empty in bash script
        if len(input) == 0:
            shell(f'''
                >&2 echo "INFO: no data to work with. Stopping execution"
                touch {output}
                # Kill only the current Snakemake, not all of them (might be running multiple instances, and we only want to stop this one)
                # https://snakemake.readthedocs.io/en/stable/project_info/faq.html#how-do-i-exit-a-running-snakemake-workflow
                pid_to_kill="{snake_no_more_race_get_pgid()}"
                if [[ "$pid_to_kill" == "" ]]; then
                    echo "ERROR: could not stop the execution with a normal status. Forcing snakemake to stop"
                    exit 1
                else
                    kill -TERM "$pid_to_kill" || exit 1
                fi
                ''')
        else:
            shell('''
                mkdir -p {params.folder}
                rm -f {params.folder}/* # remove anything that might be left after a previous run
                CAT=cat
                if [[ {input[0]} == *.gz ]]; then
                    CAT=zcat
                elif [[ {input[0]} == *.xz ]]; then
                    CAT=xzcat
                fi
                $CAT {input} \
                    | ( [ "{SRC_LANG}" = "{LANG1}" ] && cat || awk -F '\t' '{{ print $2,$1,$4,$3,$5 }}' OFS='\t' )\
                    | python3 {BITEXTOR}/utils/split.py -f 3,4 -s {params.size} --gzip -o "{params.folder}/"
                if [ -z "$(ls -A {params.folder})" ]; then
                    cat < /dev/null > {output.batches}
                else
                    ls {params.folder}/* | sed 's/.gz$//g' > {output.batches}
                fi
                ''')

def get_postproc_batches():
    batches = []
    with checkpoints.split_segalign.get().output.batches.open() as f:
        for line in f:
            batches.append(line.strip().split('/')[-1]) # obtain just the number of the chunks
    return batches

rule bifixer:
    input: segalign=f'{TRANSIENT}/{LANG1}_{LANG2}/{split_input_filename}/{{batch}}.gz',
    output: temp(f'{TRANSIENT}/{LANG1}_{LANG2}/07_01.bifixer/{{batch}}')
    threads: THREADS['bifixer']
    shell: '''
        CAT=cat; if [[ {input.segalign} == *.gz ]]; then CAT=zcat; fi
        parallel_cmd=""
        if [ {threads} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {threads} -k"
        fi
        $CAT {input.segalign} \
            | {PROFILING} ${{parallel_cmd}} python3 {BITEXTOR}/bifixer/bifixer/bifixer.py -q - - {LANG1} {LANG2} {AGGRESSIVE_DEDUP} {BIFIXER_DEFERRED_COLS} \
            > {output}
        '''

rule aggregate_bifixer:
    input: lambda wildcards: [f'{TRANSIENT}/{LANG1}_{LANG2}/07_01.bifixer/{batch}' for batch in get_postproc_batches()]
    output: f'{TRANSIENT}/07_01.bifixer.{LANG1}_{LANG2}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''

bicleaner_input = rules.bifixer.output
if not BIFIXER:
    bicleaner_input = rules.bifixer.input.segalign

if "bicleanerCorpusTrainingPrefix" in config:
    bicleanerTrainPrefixes=config["bicleanerCorpusTrainingPrefix"]
else:
    bicleanerTrainPrefixes=None

rule bicleaner_train_model:
    input:
        corpusl1=expand("{dataset}.{lang}.xz", dataset=bicleanerTrainPrefixes, lang=LANG1),
        corpusl2=expand("{dataset}.{lang}.xz", dataset=bicleanerTrainPrefixes, lang=LANG2),
        e2f=f"{DIC_TRAINING}.lex.e2f.gz",
        f2e=f"{DIC_TRAINING}.lex.f2e.gz",
        vcb1=f"{mgizaModelDir}/corpus.{LANG1}.filtered.vcb.gz",
        vcb2=f"{mgizaModelDir}/corpus.{LANG2}.filtered.vcb.gz",
        dic=f"{DIC_TRAINING}",  # If DIC != DIC_TRAINING, DIC_TRAINING will be generated but not used because of this input
                                # This input makes the dictionary to be generated (without it, statistical info would be generated but not the dic)
    output:
        model=BICLEANER_MODEL

    shell: '''
        training=$(mktemp {TMPDIR}/train.XXXXXXXX)
        paste <(xzcat -f {input.corpusl1}) <(xzcat -f {input.corpusl2}) > $training
        DIR=$(dirname {BICLEANER_MODEL})
        echo $DIR
        lines=$(cat $training | wc -l)
        trainlines=$(echo \"$lines*4/10\" | bc)
        testlines=$(echo \"($lines-2*$trainlines)/2\" | bc)
        # Parameters for good/bad examples have been removed -> now the whole file is being used (https://github.com/bitextor/bicleaner/commit/cdd1bc03928701884cd42ffac4b602220ec3e732)
        {PROFILING} python3  {BITEXTOR}/bicleaner/bicleaner/bicleaner_train.py $training -S "{WORDTOK1}" -T "{WORDTOK2}" --treat_oovs --normalize_by_length -s {LANG1} -t {LANG2} -d {input.e2f} -D {input.f2e} -f {input.vcb1} -F {input.vcb2} -c $DIR/{LANG1}-{LANG2}.classifier -m {BICLEANER_MODEL} --classifier_type random_forest
        rm $training
        '''

rule bicleaner:
    input: bifixer=bicleaner_input, model=BICLEANER_MODEL
    output: f'{TRANSIENT}/{LANG1}_{LANG2}/07_02.bicleaner/{{batch}}.gz'
    params: THREADS['bicleaner']
    threads: max(2, THREADS['bicleaner'])
    shell: '''
        parallel_cmd=""
        if [ {params} -gt 1 ]; then
            parallel_cmd="parallel --gnu --halt 2 --pipe --j {params} -k"
        fi
        CAT=cat; if [[ {input.bifixer} == *.gz ]]; then CAT=zcat; fi
        slang=$(egrep "source_lang" {input.model} | cut -d " " -f 2)
        if [ "$slang" == "{LANG1}" ]; then
            $CAT {input.bifixer} \
                | {PROFILING} {BITEXTOR}/preprocess/bin/cache -k 3,4 ${{parallel_cmd}} python3 {BITEXTOR}/bicleaner/bicleaner/bicleaner_classifier_lite.py --score_only -q - - {input.model} \
                | paste <($CAT {input.bifixer}) - \
                | pigz -c > {output}
        else
            $CAT {input.bifixer} \
                | awk ' BEGIN {{FS="\t"; OFS="\t"}} {{ t = $3; $3 = $4; $4 = t; print;}} ' \
                | {PROFILING} {BITEXTOR}/preprocess/bin/cache -k 3,4 ${{parallel_cmd}} python3 {BITEXTOR}/bicleaner/bicleaner/bicleaner_classifier_lite.py --score_only -q - - {input.model} \
                | paste <($CAT {input.bifixer}) - \
                | pigz -c > {output}
        fi
        '''

rule aggregate_bicleaner:
    input: lambda wildcards: [f'{TRANSIENT}/{LANG1}_{LANG2}/07_02.bicleaner/{batch}.gz' for batch in get_postproc_batches()]
    output: f'{TRANSIENT}/07_02.bicleaner.{LANG1}_{LANG2}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''

filter_input = rules.bicleaner.output
if not BICLEANER:
    filter_input = rules.bicleaner.input.bifixer

rule filter:
    input: filter_input
    output: temp(f'{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{{batch}}')
    threads: lambda wildcards: 2 if BICLEANER and ELRC else 1
    run:
        cat_cmd = "cat"
        if input[0][-3:] == ".gz":
            cat_cmd = "zcat"
        cmd = f''' {cat_cmd} {input} '''
        if BICLEANER:
            cmd += f''' | {PROFILING} python3 {BITEXTOR}/bitextor-filterbicleaner.py --threshold {BICLEANER_THRESHOLD} '''
        if ELRC:
            cmd += f''' | {PROFILING} python3 {BITEXTOR}/bitextor-elrc-filtering.py -c "{BEFORE_ELRC_FIELDS}" -s '''
        cmd += f''' | LC_ALL=C sort -t $'\t' {FILTER_SORT_FIELDS} ''' # sorted by either sentences or bifixer
        cmd += f''' > {output} '''
        shell(cmd)

rule aggregate_filter:
    input: lambda wildcards: [f'{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{{batch}}' for batch in get_postproc_batches()]
    output: f'{TRANSIENT}/07_03.filter.{LANG1}_{LANG2}'
    shell: ''' echo {input} | tr ' ' '\n' > {output} '''

raw_input_filename = rules.filter.input[0].split('/')[-2] # 06_02.segalign / 07_01.bifixer / 07_02.bicleaner
extension = ""
if raw_input_filename in ["07_02.bicleaner", "06_02.segalign", "hunalign.06_02.segalign"]:
    extension = ".gz"

rule raw:
    input: lambda wildcards: [f'{TRANSIENT}/{LANG1}_{LANG2}/{raw_input_filename}/{batch}{extension}' for batch in get_postproc_batches()]
    output:
        corpus=f'{PERMANENT}/{LANG1}-{LANG2}.raw.gz',
        stats=f'{PERMANENT}/{LANG1}-{LANG2}.stats.raw'
    shell: '''
        if [[ {input[0]} == *.gz ]]; then
            cat {input} > {output.corpus}
        else
            cat {input} | pigz -c > {output.corpus}
        fi
        echo "{LANG1}-{LANG2} raw" > {output.stats}
        echo "File size: $(du -h {output.corpus} | cut -f 1)" >> {output.stats}
        WC1=$(zcat {output.corpus} | cut -f 3 | wc -wl | tr -s ' ')
        WC2=$(zcat {output.corpus} | cut -f 4 | wc -w)
        echo "Sentence pairs: $(echo $WC1 | cut -d ' ' -f 1)" >> {output.stats}
        echo "{LANG1} words: $(echo $WC1 | cut -d ' ' -f 2)" >> {output.stats}
        echo "{LANG2} words: $WC2" >> {output.stats}
        '''

rule sents:
    input: lambda wildcards: [f'{TRANSIENT}/{LANG1}_{LANG2}/07_03.filtered/{batch}' for batch in get_postproc_batches()]
    output: f'{PERMANENT}/{LANG1}-{LANG2}.sent.gz'
    threads: THREADS['sents']
    shell: '''
        LC_ALL=C sort -t $'\t' {FILTER_SORT_FIELDS} --parallel {threads} --compress-program=gzip -T {TMPDIR} --merge {input} \
            | pigz -c > {output}
        '''

rule tmx:
    input: rules.sents.output
    output: f'{PERMANENT}/{LANG1}-{LANG2}.not-deduped.tmx.gz'
    shell: '''
        zcat {input} \
            | {PROFILING} python3 {BITEXTOR}/bitextor-buildTMX.py --lang1 {LANG1} --lang2 {LANG2} -c "{TMX_FIELDS}" \
            | pigz -c > {output}
        '''

rule deduped_tmx:
    input: rules.sents.output
    output:
        tmx=f'{PERMANENT}/{LANG1}-{LANG2}.deduped.tmx.gz',
        txt=f'{PERMANENT}/{LANG1}-{LANG2}.deduped.txt.gz',
        stats=f'{PERMANENT}/{LANG1}-{LANG2}.stats.deduped'
    shell: '''
        zcat {input} \
            | {PROFILING} {BITEXTOR}/bitextor-buildTMX.py --lang1 {LANG1} --lang2 {LANG2} -c "{TMX_FIELDS}" --dedup "{TMX_DEDUP_FIELDS}" -f {output.txt} \
            | pigz -c > {output.tmx}
        echo "{LANG1}-{LANG2} deduped txt" > {output.stats}
        echo "File size: $(du -h {output.txt} | cut -f 1)" >> {output.stats}
        WC1=$(zcat {output.txt} | cut -f 3 | wc -wl | tr -s ' ')
        WC2=$(zcat {output.txt} | cut -f 4 | wc -w)
        echo "Sentence pairs: $(echo $WC1 | cut -d ' ' -f 1)" >> {output.stats}
        echo "{LANG1} words: $(echo $WC1 | cut -d ' ' -f 2)" >> {output.stats}
        echo "{LANG2} words: $WC2" >> {output.stats}
        '''

rule roam_tmx:
    input:
        tmx=rules.tmx.output if not DEDUPED else rules.deduped_tmx.output.tmx,
        txt=rules.sents.output if not DEDUPED else rules.deduped_tmx.output.txt
    output: f'{PERMANENT}/{LANG1}-{LANG2}.not-deduped-roamed.tmx.gz' if not DEDUPED else f'{PERMANENT}/{LANG1}-{LANG2}.deduped-roamed.tmx.gz'
    params:
        tokenizer_l1 = lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, LANG1), "-t \"{}\""),
        tokenizer_l2 = lambda wildcards: apply_format(get_lang_or_default(WORDTOKS, LANG2), "-T \"{}\""),
        mix_files = '/dev/null' if not 'biroamerMixFiles' in config or len(config['biroamerMixFiles']) == 0 else ' '.join(config['biroamerMixFiles']),
        alignment_corpus = "" if "biroamerImproveAlignmentCorpus" not in config else f"-a {config['biroamerImproveAlignmentCorpus']}",
        omit = "" if "biroamerOmitRandomSentences" not in config or not config['biroamerOmitRandomSentences'] else "-o",
    shell: '''
        mix_files="$(mktemp {TMPDIR}/roam_tmx_mix.{LANG1}-{LANG2}.XXXXXX)"
        cat {params.mix_files} > $mix_files
        nolines=$(cat $mix_files | wc -l)

        if [[ "$nolines" != "0" ]]; then
            mix_files="-m $mix_files"
        else
            mix_files=""
        fi

        CAT=cat; if [[ {input.txt} == *.gz ]]; then CAT=zcat; fi
        nolines_txt=$($CAT {input.txt} | wc -l)
        total_nolines=$(echo $nolines + $nolines_txt | bc)

        if [[ "$total_nolines" -lt "100000" ]] && [[ "{params.alignment_corpus}" == "" ]]; then
            >&2 echo "WARNING: biroamer suggests, at least, 100k lines in order to have a good alignment (current nolines: $total_nolines)."\
                     "Check 'biroamerImproveAlignmentCorpus' config option if you want to improve the resulted roamed tmx"
        fi

        zcat {input.tmx} \
            | {PROFILING} {BITEXTOR}/biroamer/biroamer.sh {params.tokenizer_l1} {params.tokenizer_l2} \
                {params.omit} {params.alignment_corpus} $mix_files {LANG1} {LANG2} \
            | pigz -c > {output}
        '''
